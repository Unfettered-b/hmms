{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 144,
   "id": "99fd6d19",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "\n",
    "import logging\n",
    "import sys\n",
    "import os\n",
    "import pandas as pd\n",
    "from datetime import datetime\n",
    "from pathlib import Path\n",
    "\n",
    "\n",
    "rightnow = datetime.now().strftime(\"%Y-%m-%d-%H-%M-%S\")\n",
    "LINEAGE_LEVEL = \"family\"\n",
    "HMM_FILE = \"ascogs_escrt_system_20260116_121403.tsv\"\n",
    "FLANK_K = 0               # number of 'other' tokens allowed at each flank\n",
    "\n",
    "NOTEBOOK_PATH = Path.cwd() / \"ESCRT_system_synteny_2.ipynb\"\n",
    "\n",
    "WINDOW = 5\n",
    "CORE_TARGETS = (\"vps\", \"escrt\", \"katanin\", \"eap\", \"flad\")\n",
    "\n",
    "\n",
    "MAIN_OUTDIR = os.path.join(os.getcwd(), f\"ESCRT_synteny_pipeline_m3v2_{LINEAGE_LEVEL}_{rightnow}\")\n",
    "os.makedirs(MAIN_OUTDIR, exist_ok=True)\n",
    "\n",
    "def setup_logging(log_file=None):\n",
    "    \"\"\"\n",
    "    Configure logging for the pipeline.\n",
    "    Logs to stdout and optionally to a file.\n",
    "    \"\"\"\n",
    "    handlers = [logging.StreamHandler(sys.stdout)]\n",
    "    if log_file:\n",
    "        handlers.append(logging.FileHandler(log_file))\n",
    "\n",
    "    logging.basicConfig(\n",
    "        level=logging.INFO,\n",
    "        format=\"%(asctime)s [%(levelname)s] %(message)s\",\n",
    "        handlers=handlers,\n",
    "        force=True  # Force reconfiguration if already set\n",
    "    )\n",
    "\n",
    "\n",
    "STEP = 0\n",
    "\n",
    "setup_logging(os.path.join(MAIN_OUTDIR, f\"ESCRT_synteny_pipeline{rightnow}.log\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "id": "74c28612",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-16 20:11:34,290 [INFO] ================================================================================\n",
      "2026-01-16 20:11:34,291 [INFO] NOTEBOOK JSON SNAPSHOT (Start of Run)\n",
      "2026-01-16 20:11:34,291 [INFO] ================================================================================\n",
      "2026-01-16 20:11:34,292 [INFO] {\n",
      "2026-01-16 20:11:34,293 [INFO]   \"cells\": [\n",
      "2026-01-16 20:11:34,293 [INFO]     {\n",
      "2026-01-16 20:11:34,294 [INFO]       \"cell_type\": \"code\",\n",
      "2026-01-16 20:11:34,294 [INFO]       \"execution_count\": null,\n",
      "2026-01-16 20:11:34,295 [INFO]       \"id\": \"99fd6d19\",\n",
      "2026-01-16 20:11:34,295 [INFO]       \"metadata\": {},\n",
      "2026-01-16 20:11:34,295 [INFO]       \"outputs\": [],\n",
      "2026-01-16 20:11:34,296 [INFO]       \"source\": [\n",
      "2026-01-16 20:11:34,296 [INFO]         \"# Configuration\\n\",\n",
      "2026-01-16 20:11:34,297 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,297 [INFO]         \"import logging\\n\",\n",
      "2026-01-16 20:11:34,297 [INFO]         \"import sys\\n\",\n",
      "2026-01-16 20:11:34,298 [INFO]         \"import os\\n\",\n",
      "2026-01-16 20:11:34,298 [INFO]         \"import pandas as pd\\n\",\n",
      "2026-01-16 20:11:34,298 [INFO]         \"from datetime import datetime\\n\",\n",
      "2026-01-16 20:11:34,299 [INFO]         \"from pathlib import Path\\n\",\n",
      "2026-01-16 20:11:34,299 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,300 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,300 [INFO]         \"rightnow = datetime.now().strftime(\\\"%Y-%m-%d-%H-%M-%S\\\")\\n\",\n",
      "2026-01-16 20:11:34,301 [INFO]         \"LINEAGE_LEVEL = \\\"family\\\"\\n\",\n",
      "2026-01-16 20:11:34,301 [INFO]         \"HMM_FILE = \\\"ascogs_escrt_system_20260116_121403.tsv\\\"\\n\",\n",
      "2026-01-16 20:11:34,301 [INFO]         \"FLANK_K = 0               # number of 'other' tokens allowed at each flank\\n\",\n",
      "2026-01-16 20:11:34,302 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,303 [INFO]         \"NOTEBOOK_PATH = Path.cwd() / \\\"ESCRT_system_synteny_2.ipynb\\\"\\n\",\n",
      "2026-01-16 20:11:34,303 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,303 [INFO]         \"WINDOW = 5\\n\",\n",
      "2026-01-16 20:11:34,304 [INFO]         \"CORE_TARGETS = (\\\"vps\\\", \\\"escrt\\\", \\\"katanin\\\", \\\"eap\\\", \\\"flad\\\")\\n\",\n",
      "2026-01-16 20:11:34,304 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,305 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,305 [INFO]         \"MAIN_OUTDIR = os.path.join(os.getcwd(), f\\\"ESCRT_synteny_pipeline_m3v2_{LINEAGE_LEVEL}_{rightnow}\\\")\\n\",\n",
      "2026-01-16 20:11:34,306 [INFO]         \"os.makedirs(MAIN_OUTDIR, exist_ok=True)\\n\",\n",
      "2026-01-16 20:11:34,306 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,306 [INFO]         \"def setup_logging(log_file=None):\\n\",\n",
      "2026-01-16 20:11:34,307 [INFO]         \"    \\\"\\\"\\\"\\n\",\n",
      "2026-01-16 20:11:34,307 [INFO]         \"    Configure logging for the pipeline.\\n\",\n",
      "2026-01-16 20:11:34,308 [INFO]         \"    Logs to stdout and optionally to a file.\\n\",\n",
      "2026-01-16 20:11:34,308 [INFO]         \"    \\\"\\\"\\\"\\n\",\n",
      "2026-01-16 20:11:34,309 [INFO]         \"    handlers = [logging.StreamHandler(sys.stdout)]\\n\",\n",
      "2026-01-16 20:11:34,309 [INFO]         \"    if log_file:\\n\",\n",
      "2026-01-16 20:11:34,310 [INFO]         \"        handlers.append(logging.FileHandler(log_file))\\n\",\n",
      "2026-01-16 20:11:34,310 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,310 [INFO]         \"    logging.basicConfig(\\n\",\n",
      "2026-01-16 20:11:34,311 [INFO]         \"        level=logging.INFO,\\n\",\n",
      "2026-01-16 20:11:34,311 [INFO]         \"        format=\\\"%(asctime)s [%(levelname)s] %(message)s\\\",\\n\",\n",
      "2026-01-16 20:11:34,312 [INFO]         \"        handlers=handlers,\\n\",\n",
      "2026-01-16 20:11:34,312 [INFO]         \"        force=True  # Force reconfiguration if already set\\n\",\n",
      "2026-01-16 20:11:34,313 [INFO]         \"    )\\n\",\n",
      "2026-01-16 20:11:34,313 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,313 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,314 [INFO]         \"STEP = 0\\n\",\n",
      "2026-01-16 20:11:34,314 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,315 [INFO]         \"setup_logging(os.path.join(MAIN_OUTDIR, f\\\"ESCRT_synteny_pipeline{rightnow}.log\\\"))\\n\"\n",
      "2026-01-16 20:11:34,315 [INFO]       ]\n",
      "2026-01-16 20:11:34,316 [INFO]     },\n",
      "2026-01-16 20:11:34,316 [INFO]     {\n",
      "2026-01-16 20:11:34,316 [INFO]       \"cell_type\": \"code\",\n",
      "2026-01-16 20:11:34,317 [INFO]       \"execution_count\": null,\n",
      "2026-01-16 20:11:34,317 [INFO]       \"id\": \"74c28612\",\n",
      "2026-01-16 20:11:34,318 [INFO]       \"metadata\": {},\n",
      "2026-01-16 20:11:34,318 [INFO]       \"outputs\": [],\n",
      "2026-01-16 20:11:34,318 [INFO]       \"source\": [\n",
      "2026-01-16 20:11:34,319 [INFO]         \"import json\\n\",\n",
      "2026-01-16 20:11:34,319 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,320 [INFO]         \"# ------------------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,321 [INFO]         \"# Load the current notebook file (Jupyter notebooks are JSON)\\n\",\n",
      "2026-01-16 20:11:34,321 [INFO]         \"# ------------------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,321 [INFO]         \"with open(NOTEBOOK_PATH, \\\"r\\\", encoding=\\\"utf-8\\\") as nb:\\n\",\n",
      "2026-01-16 20:11:34,322 [INFO]         \"    notebook_json = json.load(nb)\\n\",\n",
      "2026-01-16 20:11:34,322 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,322 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,323 [INFO]         \"# ------------------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,323 [INFO]         \"# Remove execution outputs from code cells\\n\",\n",
      "2026-01-16 20:11:34,324 [INFO]         \"#   - prevents massive logs\\n\",\n",
      "2026-01-16 20:11:34,324 [INFO]         \"#   - avoids embedding binary blobs (plots, images)\\n\",\n",
      "2026-01-16 20:11:34,325 [INFO]         \"#   - keeps only the executable logic + structure\\n\",\n",
      "2026-01-16 20:11:34,325 [INFO]         \"# ------------------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,326 [INFO]         \"def strip_outputs(nb):\\n\",\n",
      "2026-01-16 20:11:34,326 [INFO]         \"    for cell in nb.get(\\\"cells\\\", []):\\n\",\n",
      "2026-01-16 20:11:34,326 [INFO]         \"        if cell.get(\\\"cell_type\\\") == \\\"code\\\":\\n\",\n",
      "2026-01-16 20:11:34,327 [INFO]         \"            cell[\\\"outputs\\\"] = []\\n\",\n",
      "2026-01-16 20:11:34,327 [INFO]         \"            cell[\\\"execution_count\\\"] = None\\n\",\n",
      "2026-01-16 20:11:34,328 [INFO]         \"    return nb\\n\",\n",
      "2026-01-16 20:11:34,328 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,328 [INFO]         \"notebook_json = strip_outputs(notebook_json)\\n\",\n",
      "2026-01-16 20:11:34,329 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,329 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,330 [INFO]         \"# ------------------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,330 [INFO]         \"# Serialize notebook JSON to a formatted string\\n\",\n",
      "2026-01-16 20:11:34,330 [INFO]         \"#   - indentation preserves readability in logs\\n\",\n",
      "2026-01-16 20:11:34,331 [INFO]         \"#   - done once to avoid repeated JSON encoding\\n\",\n",
      "2026-01-16 20:11:34,331 [INFO]         \"# ------------------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,332 [INFO]         \"notebook_str = json.dumps(notebook_json, indent=2)\\n\",\n",
      "2026-01-16 20:11:34,332 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,332 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,333 [INFO]         \"# ------------------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,333 [INFO]         \"# Append notebook snapshot to the log file\\n\",\n",
      "2026-01-16 20:11:34,334 [INFO]         \"#   - logged line-by-line to preserve formatting\\n\",\n",
      "2026-01-16 20:11:34,334 [INFO]         \"#   - ensures compatibility with logging handlers\\n\",\n",
      "2026-01-16 20:11:34,334 [INFO]         \"# ------------------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,335 [INFO]         \"logging.info(\\\"=\\\" * 80)\\n\",\n",
      "2026-01-16 20:11:34,335 [INFO]         \"logging.info(\\\"NOTEBOOK JSON SNAPSHOT (Start of Run)\\\")\\n\",\n",
      "2026-01-16 20:11:34,336 [INFO]         \"logging.info(\\\"=\\\" * 80)\\n\",\n",
      "2026-01-16 20:11:34,336 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,337 [INFO]         \"for line in notebook_str.splitlines():\\n\",\n",
      "2026-01-16 20:11:34,337 [INFO]         \"    logging.info(line)\\n\"\n",
      "2026-01-16 20:11:34,337 [INFO]       ]\n",
      "2026-01-16 20:11:34,338 [INFO]     },\n",
      "2026-01-16 20:11:34,338 [INFO]     {\n",
      "2026-01-16 20:11:34,338 [INFO]       \"cell_type\": \"code\",\n",
      "2026-01-16 20:11:34,339 [INFO]       \"execution_count\": null,\n",
      "2026-01-16 20:11:34,339 [INFO]       \"id\": \"25546ebc\",\n",
      "2026-01-16 20:11:34,339 [INFO]       \"metadata\": {},\n",
      "2026-01-16 20:11:34,340 [INFO]       \"outputs\": [],\n",
      "2026-01-16 20:11:34,340 [INFO]       \"source\": [\n",
      "2026-01-16 20:11:34,341 [INFO]         \"# Merge HMM hits with protein metadata\\n\",\n",
      "2026-01-16 20:11:34,341 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,341 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,342 [INFO]         \"# ----------------------------\\n\",\n",
      "2026-01-16 20:11:34,342 [INFO]         \"# Input files\\n\",\n",
      "2026-01-16 20:11:34,342 [INFO]         \"# ----------------------------\\n\",\n",
      "2026-01-16 20:11:34,343 [INFO]         \"hits_file = \\\"/home/anirudh/synteny/hmms/ESCRT_results/hits/ESCRT_hits_final.csv\\\"\\n\",\n",
      "2026-01-16 20:11:34,343 [INFO]         \"proteins_file = \\\"/home/anirudh/synteny/proteins_genomes_cp90_con5.csv\\\"\\n\",\n",
      "2026-01-16 20:11:34,344 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,344 [INFO]         \"# ----------------------------\\n\",\n",
      "2026-01-16 20:11:34,344 [INFO]         \"# Output file naming\\n\",\n",
      "2026-01-16 20:11:34,345 [INFO]         \"# ----------------------------\\n\",\n",
      "2026-01-16 20:11:34,345 [INFO]         \"hits_file_name = os.path.basename(hits_file).split(\\\".\\\")[0]\\n\",\n",
      "2026-01-16 20:11:34,346 [INFO]         \"STEP += 1\\n\",\n",
      "2026-01-16 20:11:34,346 [INFO]         \"merge_file_name = os.path.join(MAIN_OUTDIR, f\\\"[STEP:{STEP}]{hits_file_name}_merged_{rightnow}.csv\\\")\\n\",\n",
      "2026-01-16 20:11:34,346 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,347 [INFO]         \"logging.info(\\\"HMM hits file: %s\\\", hits_file)\\n\",\n",
      "2026-01-16 20:11:34,347 [INFO]         \"logging.info(\\\"Protein metadata file: %s\\\", proteins_file)\\n\",\n",
      "2026-01-16 20:11:34,347 [INFO]         \"logging.info(\\\"Merged output file: %s\\\", merge_file_name)\\n\",\n",
      "2026-01-16 20:11:34,348 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,348 [INFO]         \"# ----------------------------\\n\",\n",
      "2026-01-16 20:11:34,348 [INFO]         \"# Load HMM hits\\n\",\n",
      "2026-01-16 20:11:34,349 [INFO]         \"# ----------------------------\\n\",\n",
      "2026-01-16 20:11:34,350 [INFO]         \"try:\\n\",\n",
      "2026-01-16 20:11:34,350 [INFO]         \"    hits = pd.read_csv(hits_file)\\n\",\n",
      "2026-01-16 20:11:34,350 [INFO]         \"except Exception as e:\\n\",\n",
      "2026-01-16 20:11:34,350 [INFO]         \"    logging.error(\\\"Failed to load HMM hits file: %s\\\", e)\\n\",\n",
      "2026-01-16 20:11:34,351 [INFO]         \"    raise\\n\",\n",
      "2026-01-16 20:11:34,352 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,352 [INFO]         \"hits.columns = hits.columns.str.strip()\\n\",\n",
      "2026-01-16 20:11:34,352 [INFO]         \"logging.info(\\\"HMM hits columns: %s\\\", list(hits.columns))\\n\",\n",
      "2026-01-16 20:11:34,353 [INFO]         \"logging.info(\\\"HMM hits rows: %d\\\", len(hits))\\n\",\n",
      "2026-01-16 20:11:34,353 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,353 [INFO]         \"# ----------------------------\\n\",\n",
      "2026-01-16 20:11:34,354 [INFO]         \"# Load protein metadata\\n\",\n",
      "2026-01-16 20:11:34,354 [INFO]         \"# ----------------------------\\n\",\n",
      "2026-01-16 20:11:34,355 [INFO]         \"try:\\n\",\n",
      "2026-01-16 20:11:34,355 [INFO]         \"    proteins = pd.read_csv(proteins_file)\\n\",\n",
      "2026-01-16 20:11:34,356 [INFO]         \"except Exception as e:\\n\",\n",
      "2026-01-16 20:11:34,356 [INFO]         \"    logging.error(\\\"Failed to load protein metadata file: %s\\\", e)\\n\",\n",
      "2026-01-16 20:11:34,357 [INFO]         \"    raise\\n\",\n",
      "2026-01-16 20:11:34,357 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,357 [INFO]         \"proteins.columns = proteins.columns.str.strip()\\n\",\n",
      "2026-01-16 20:11:34,358 [INFO]         \"logging.info(\\\"Protein metadata columns: %s\\\", list(proteins.columns))\\n\",\n",
      "2026-01-16 20:11:34,358 [INFO]         \"logging.info(\\\"Protein metadata rows: %d\\\", len(proteins))\\n\",\n",
      "2026-01-16 20:11:34,358 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,359 [INFO]         \"# ----------------------------\\n\",\n",
      "2026-01-16 20:11:34,359 [INFO]         \"# Sanity checks before merge\\n\",\n",
      "2026-01-16 20:11:34,359 [INFO]         \"# ----------------------------\\n\",\n",
      "2026-01-16 20:11:34,360 [INFO]         \"required_hits_cols = {\\\"target\\\"}\\n\",\n",
      "2026-01-16 20:11:34,360 [INFO]         \"required_prot_cols = {\\\"cds_id\\\"}\\n\",\n",
      "2026-01-16 20:11:34,361 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,361 [INFO]         \"missing_hits = required_hits_cols - set(hits.columns)\\n\",\n",
      "2026-01-16 20:11:34,361 [INFO]         \"missing_prots = required_prot_cols - set(proteins.columns)\\n\",\n",
      "2026-01-16 20:11:34,361 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,362 [INFO]         \"if missing_hits:\\n\",\n",
      "2026-01-16 20:11:34,362 [INFO]         \"    logging.error(\\\"Missing required columns in HMM hits file: %s\\\", missing_hits)\\n\",\n",
      "2026-01-16 20:11:34,363 [INFO]         \"    raise ValueError(f\\\"Missing columns in hits file: {missing_hits}\\\")\\n\",\n",
      "2026-01-16 20:11:34,364 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,364 [INFO]         \"if missing_prots:\\n\",\n",
      "2026-01-16 20:11:34,364 [INFO]         \"    logging.error(\\\"Missing required columns in protein metadata file: %s\\\", missing_prots)\\n\",\n",
      "2026-01-16 20:11:34,365 [INFO]         \"    raise ValueError(f\\\"Missing columns in proteins file: {missing_prots}\\\")\\n\",\n",
      "2026-01-16 20:11:34,366 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,366 [INFO]         \"logging.info(\\\"Input sanity checks passed\\\")\\n\",\n",
      "2026-01-16 20:11:34,367 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,367 [INFO]         \"# ----------------------------\\n\",\n",
      "2026-01-16 20:11:34,368 [INFO]         \"# Merge HMM hits with protein metadata\\n\",\n",
      "2026-01-16 20:11:34,368 [INFO]         \"# ----------------------------\\n\",\n",
      "2026-01-16 20:11:34,368 [INFO]         \"logging.info(\\\"Merging HMM hits with protein metadata\\\")\\n\",\n",
      "2026-01-16 20:11:34,369 [INFO]         \"logging.info(\\\"Rows before merge: hits=%d, proteins=%d\\\", len(hits), len(proteins))\\n\",\n",
      "2026-01-16 20:11:34,369 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,370 [INFO]         \"merged = pd.merge(\\n\",\n",
      "2026-01-16 20:11:34,371 [INFO]         \"    hits,\\n\",\n",
      "2026-01-16 20:11:34,371 [INFO]         \"    proteins,\\n\",\n",
      "2026-01-16 20:11:34,371 [INFO]         \"    left_on=\\\"target\\\",\\n\",\n",
      "2026-01-16 20:11:34,372 [INFO]         \"    right_on=\\\"cds_id\\\",\\n\",\n",
      "2026-01-16 20:11:34,372 [INFO]         \"    how=\\\"inner\\\"\\n\",\n",
      "2026-01-16 20:11:34,373 [INFO]         \")\\n\",\n",
      "2026-01-16 20:11:34,373 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,373 [INFO]         \"logging.info(\\\"Rows after merge: %d\\\", len(merged))\\n\",\n",
      "2026-01-16 20:11:34,374 [INFO]         \"logging.info(\\\"Unique targets merged: %d\\\", merged[\\\"target\\\"].nunique())\\n\",\n",
      "2026-01-16 20:11:34,374 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,374 [INFO]         \"# ----------------------------\\n\",\n",
      "2026-01-16 20:11:34,375 [INFO]         \"# Write output\\n\",\n",
      "2026-01-16 20:11:34,375 [INFO]         \"# ----------------------------\\n\",\n",
      "2026-01-16 20:11:34,376 [INFO]         \"try:\\n\",\n",
      "2026-01-16 20:11:34,376 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,376 [INFO]         \"    merged.to_csv(merge_file_name, index=False)\\n\",\n",
      "2026-01-16 20:11:34,377 [INFO]         \"except Exception as e:\\n\",\n",
      "2026-01-16 20:11:34,377 [INFO]         \"    logging.error(\\\"Failed to write merged output file %s: %s\\\", merge_file_name, e)\\n\",\n",
      "2026-01-16 20:11:34,378 [INFO]         \"    raise\\n\",\n",
      "2026-01-16 20:11:34,378 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,379 [INFO]         \"logging.info(\\\"Merged output written to: %s\\\", merge_file_name)\\n\"\n",
      "2026-01-16 20:11:34,379 [INFO]       ]\n",
      "2026-01-16 20:11:34,380 [INFO]     },\n",
      "2026-01-16 20:11:34,380 [INFO]     {\n",
      "2026-01-16 20:11:34,381 [INFO]       \"cell_type\": \"markdown\",\n",
      "2026-01-16 20:11:34,381 [INFO]       \"id\": \"5357cf73\",\n",
      "2026-01-16 20:11:34,381 [INFO]       \"metadata\": {},\n",
      "2026-01-16 20:11:34,382 [INFO]       \"source\": [\n",
      "2026-01-16 20:11:34,382 [INFO]         \"Merge with the ascogs definition\"\n",
      "2026-01-16 20:11:34,383 [INFO]       ]\n",
      "2026-01-16 20:11:34,383 [INFO]     },\n",
      "2026-01-16 20:11:34,384 [INFO]     {\n",
      "2026-01-16 20:11:34,384 [INFO]       \"cell_type\": \"code\",\n",
      "2026-01-16 20:11:34,384 [INFO]       \"execution_count\": null,\n",
      "2026-01-16 20:11:34,385 [INFO]       \"id\": \"53f1d3d2\",\n",
      "2026-01-16 20:11:34,385 [INFO]       \"metadata\": {},\n",
      "2026-01-16 20:11:34,386 [INFO]       \"outputs\": [],\n",
      "2026-01-16 20:11:34,386 [INFO]       \"source\": [\n",
      "2026-01-16 20:11:34,386 [INFO]         \"# Merge HMM hits with AsCOG annotations\\n\",\n",
      "2026-01-16 20:11:34,387 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,387 [INFO]         \"# ----------------------------\\n\",\n",
      "2026-01-16 20:11:34,388 [INFO]         \"# Load AsCOG annotation table\\n\",\n",
      "2026-01-16 20:11:34,388 [INFO]         \"# ----------------------------\\n\",\n",
      "2026-01-16 20:11:34,388 [INFO]         \"ascogs_tsv = f\\\"/home/anirudh/synteny/hmms/Sources/{HMM_FILE}\\\"\\n\",\n",
      "2026-01-16 20:11:34,389 [INFO]         \"logging.info(\\\"Loading AsCOGs from: %s\\\", ascogs_tsv)\\n\",\n",
      "2026-01-16 20:11:34,389 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,390 [INFO]         \"try:\\n\",\n",
      "2026-01-16 20:11:34,390 [INFO]         \"    ascogs = pd.read_csv(ascogs_tsv, sep=\\\"\\\\t\\\")\\n\",\n",
      "2026-01-16 20:11:34,390 [INFO]         \"except Exception as e:\\n\",\n",
      "2026-01-16 20:11:34,391 [INFO]         \"    logging.error(\\\"Failed to load AsCOGs file: %s\\\", e)\\n\",\n",
      "2026-01-16 20:11:34,391 [INFO]         \"    raise\\n\",\n",
      "2026-01-16 20:11:34,392 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,392 [INFO]         \"ascogs.columns = ascogs.columns.str.strip()\\n\",\n",
      "2026-01-16 20:11:34,393 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,393 [INFO]         \"logging.info(\\\"AsCOGs columns: %s\\\", list(ascogs.columns))\\n\",\n",
      "2026-01-16 20:11:34,393 [INFO]         \"logging.info(\\\"AsCOGs rows: %d\\\", len(ascogs))\\n\",\n",
      "2026-01-16 20:11:34,394 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,394 [INFO]         \"# ----------------------------\\n\",\n",
      "2026-01-16 20:11:34,395 [INFO]         \"# Sanity checks\\n\",\n",
      "2026-01-16 20:11:34,395 [INFO]         \"# ----------------------------\\n\",\n",
      "2026-01-16 20:11:34,396 [INFO]         \"required_cols = {\\\"ascog_id\\\"}\\n\",\n",
      "2026-01-16 20:11:34,396 [INFO]         \"missing = required_cols - set(ascogs.columns)\\n\",\n",
      "2026-01-16 20:11:34,396 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,397 [INFO]         \"if missing:\\n\",\n",
      "2026-01-16 20:11:34,397 [INFO]         \"    logging.error(\\\"Missing required columns in AsCOGs file: %s\\\", missing)\\n\",\n",
      "2026-01-16 20:11:34,397 [INFO]         \"    raise ValueError(f\\\"Missing required columns: {missing}\\\")\\n\",\n",
      "2026-01-16 20:11:34,398 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,398 [INFO]         \"logging.info(\\\"AsCOGs sanity check passed\\\")\\n\",\n",
      "2026-01-16 20:11:34,399 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,399 [INFO]         \"# ----------------------------\\n\",\n",
      "2026-01-16 20:11:34,399 [INFO]         \"# Merge HMM hits with AsCOGs\\n\",\n",
      "2026-01-16 20:11:34,400 [INFO]         \"# ----------------------------\\n\",\n",
      "2026-01-16 20:11:34,400 [INFO]         \"logging.info(\\\"Merging HMM hits with AsCOG annotations\\\")\\n\",\n",
      "2026-01-16 20:11:34,401 [INFO]         \"logging.info(\\\"HMM hits rows before merge: %d\\\", len(merged))\\n\",\n",
      "2026-01-16 20:11:34,401 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,402 [INFO]         \"merged_ascogs = pd.merge(\\n\",\n",
      "2026-01-16 20:11:34,402 [INFO]         \"    merged,\\n\",\n",
      "2026-01-16 20:11:34,402 [INFO]         \"    ascogs,\\n\",\n",
      "2026-01-16 20:11:34,402 [INFO]         \"    left_on=\\\"query\\\",\\n\",\n",
      "2026-01-16 20:11:34,403 [INFO]         \"    right_on=\\\"ascog_id\\\",\\n\",\n",
      "2026-01-16 20:11:34,403 [INFO]         \"    how=\\\"left\\\"\\n\",\n",
      "2026-01-16 20:11:34,404 [INFO]         \")\\n\",\n",
      "2026-01-16 20:11:34,404 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,404 [INFO]         \"logging.info(\\\"Rows after AsCOG merge: %d\\\", len(merged_ascogs))\\n\",\n",
      "2026-01-16 20:11:34,405 [INFO]         \"logging.info(\\\"Unique AsCOGs matched: %d\\\", merged_ascogs[\\\"ascog_id\\\"].nunique())\\n\",\n",
      "2026-01-16 20:11:34,405 [INFO]         \"logging.info(\\\"Unique proteins matched: %d\\\", merged_ascogs[\\\"target\\\"].nunique())\\n\",\n",
      "2026-01-16 20:11:34,405 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,406 [INFO]         \"# ----------------------------\\n\",\n",
      "2026-01-16 20:11:34,406 [INFO]         \"# Write output\\n\",\n",
      "2026-01-16 20:11:34,407 [INFO]         \"# ----------------------------\\n\",\n",
      "2026-01-16 20:11:34,407 [INFO]         \"STEP += 1\\n\",\n",
      "2026-01-16 20:11:34,408 [INFO]         \"final_output_file = os.path.join(MAIN_OUTDIR, f\\\"[STEP:{STEP}]ESCRT_hits_final_merged_{rightnow}_ascog.csv\\\")\\n\",\n",
      "2026-01-16 20:11:34,408 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,408 [INFO]         \"merged_ascogs = merged_ascogs[[\\\"target\\\", \\\"tacc\\\", \\\"tlen\\\", \\\"query\\\", \\\"qacc\\\", \\\"qlen\\\", \\\"c_evalue\\\", \\\"i_evalue\\\", \\\"dom_score\\\",  \\\"hmm_from\\\", \\\"hmm_to\\\", \\\"ali_from\\\", \\\"ali_to\\\", \\\"env_from\\\", \\\"env_to\\\", \\\"acc\\\", \\\"Name\\\", \\\"Completeness\\\", \\\"Contamination\\\", \\\"Contig_N50\\\", \\\"Total_Contigs\\\", \\\"organism_name\\\", \\\"cds_id\\\", \\\"header\\\", \\\"gene\\\", \\\"description_y\\\"]]\\n\",\n",
      "2026-01-16 20:11:34,409 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,409 [INFO]         \"try:\\n\",\n",
      "2026-01-16 20:11:34,410 [INFO]         \"    merged_ascogs.to_csv(final_output_file, index=False)\\n\",\n",
      "2026-01-16 20:11:34,410 [INFO]         \"except Exception as e:\\n\",\n",
      "2026-01-16 20:11:34,411 [INFO]         \"    logging.error(\\\"Failed to write output file %s: %s\\\", final_output_file, e)\\n\",\n",
      "2026-01-16 20:11:34,411 [INFO]         \"    raise\\n\",\n",
      "2026-01-16 20:11:34,412 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,412 [INFO]         \"logging.info(\\\"Final merged file written to: %s\\\", final_output_file)\\n\"\n",
      "2026-01-16 20:11:34,412 [INFO]       ]\n",
      "2026-01-16 20:11:34,412 [INFO]     },\n",
      "2026-01-16 20:11:34,413 [INFO]     {\n",
      "2026-01-16 20:11:34,413 [INFO]       \"cell_type\": \"code\",\n",
      "2026-01-16 20:11:34,414 [INFO]       \"execution_count\": null,\n",
      "2026-01-16 20:11:34,414 [INFO]       \"id\": \"626cee5d\",\n",
      "2026-01-16 20:11:34,414 [INFO]       \"metadata\": {},\n",
      "2026-01-16 20:11:34,414 [INFO]       \"outputs\": [],\n",
      "2026-01-16 20:11:34,415 [INFO]       \"source\": [\n",
      "2026-01-16 20:11:34,415 [INFO]         \"# Select best hits based on e-value and coverage\\n\",\n",
      "2026-01-16 20:11:34,416 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,417 [INFO]         \"merged_ascogs[\\\"coverage\\\"] = (\\n\",\n",
      "2026-01-16 20:11:34,417 [INFO]         \"    (merged_ascogs[\\\"hmm_to\\\"] - merged_ascogs[\\\"hmm_from\\\"] + 1)\\n\",\n",
      "2026-01-16 20:11:34,418 [INFO]         \"    / merged_ascogs[\\\"qlen\\\"]\\n\",\n",
      "2026-01-16 20:11:34,418 [INFO]         \")\\n\",\n",
      "2026-01-16 20:11:34,418 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,419 [INFO]         \"merged_ascogs = (\\n\",\n",
      "2026-01-16 20:11:34,419 [INFO]         \"    merged_ascogs\\n\",\n",
      "2026-01-16 20:11:34,420 [INFO]         \"    .sort_values(\\n\",\n",
      "2026-01-16 20:11:34,420 [INFO]         \"        by=[\\\"target\\\", \\\"query\\\", \\\"i_evalue\\\", \\\"coverage\\\", \\\"dom_score\\\"],\\n\",\n",
      "2026-01-16 20:11:34,420 [INFO]         \"        ascending=[True, True, True, False, False]\\n\",\n",
      "2026-01-16 20:11:34,421 [INFO]         \"    )\\n\",\n",
      "2026-01-16 20:11:34,421 [INFO]         \")\\n\",\n",
      "2026-01-16 20:11:34,421 [INFO]         \"best_hits = (\\n\",\n",
      "2026-01-16 20:11:34,422 [INFO]         \"    merged_ascogs\\n\",\n",
      "2026-01-16 20:11:34,422 [INFO]         \"    .drop_duplicates(subset=[\\\"target\\\", \\\"query\\\"], keep=\\\"first\\\")\\n\",\n",
      "2026-01-16 20:11:34,422 [INFO]         \")\\n\",\n",
      "2026-01-16 20:11:34,423 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,423 [INFO]         \"best_hits = best_hits[(best_hits[\\\"i_evalue\\\"] <= 1e-5) & (best_hits[\\\"coverage\\\"] >= 0.65)]\\n\",\n",
      "2026-01-16 20:11:34,423 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,424 [INFO]         \"best_hits = best_hits[[\\\"target\\\",\\\"query\\\",\\\"gene\\\",\\\"dom_score\\\",\\\"i_evalue\\\",\\\"coverage\\\",\\\"description_y\\\",\\\"tacc\\\",\\\"tlen\\\",\\\"qacc\\\",\\\"qlen\\\",\\\"c_evalue\\\",\\\"hmm_from\\\",\\\"hmm_to\\\",\\\"ali_from\\\",\\\"ali_to\\\",\\\"env_from\\\",\\\"env_to\\\",\\\"acc\\\",\\\"Name\\\",\\\"Completeness\\\",\\\"Contamination\\\",\\\"Contig_N50\\\",\\\"Total_Contigs\\\",\\\"organism_name\\\"]]\\n\",\n",
      "2026-01-16 20:11:34,425 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,425 [INFO]         \"STEP += 1\\n\",\n",
      "2026-01-16 20:11:34,426 [INFO]         \"best_hits_file = os.path.join(MAIN_OUTDIR, f\\\"[STEP:{STEP}]ESCRT_hits_final_merged_{rightnow}_ascog_best_hits.csv\\\")\\n\",\n",
      "2026-01-16 20:11:34,426 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,427 [INFO]         \"best_hits.to_csv(best_hits_file, index=False)\\n\",\n",
      "2026-01-16 20:11:34,427 [INFO]         \"logging.info(\\\"Best hits file written to: %s\\\", best_hits_file)\\n\"\n",
      "2026-01-16 20:11:34,427 [INFO]       ]\n",
      "2026-01-16 20:11:34,428 [INFO]     },\n",
      "2026-01-16 20:11:34,428 [INFO]     {\n",
      "2026-01-16 20:11:34,429 [INFO]       \"cell_type\": \"code\",\n",
      "2026-01-16 20:11:34,429 [INFO]       \"execution_count\": null,\n",
      "2026-01-16 20:11:34,430 [INFO]       \"id\": \"b722f80f\",\n",
      "2026-01-16 20:11:34,430 [INFO]       \"metadata\": {},\n",
      "2026-01-16 20:11:34,430 [INFO]       \"outputs\": [],\n",
      "2026-01-16 20:11:34,431 [INFO]       \"source\": [\n",
      "2026-01-16 20:11:34,431 [INFO]         \"# Compute protein counts with AsCOG hits\\n\",\n",
      "2026-01-16 20:11:34,432 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,432 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,432 [INFO]         \"protein_counts = (\\n\",\n",
      "2026-01-16 20:11:34,433 [INFO]         \"    best_hits\\n\",\n",
      "2026-01-16 20:11:34,433 [INFO]         \"    .dropna(subset=[\\\"gene\\\", \\\"query\\\"])\\n\",\n",
      "2026-01-16 20:11:34,433 [INFO]         \"    .assign(\\n\",\n",
      "2026-01-16 20:11:34,434 [INFO]         \"        gene=lambda df: df[\\\"gene\\\"].str.strip(),\\n\",\n",
      "2026-01-16 20:11:34,434 [INFO]         \"        ascog_id=lambda df: df[\\\"query\\\"].str.strip()\\n\",\n",
      "2026-01-16 20:11:34,435 [INFO]         \"    )\\n\",\n",
      "2026-01-16 20:11:34,435 [INFO]         \"    .groupby(\\\"target\\\")\\n\",\n",
      "2026-01-16 20:11:34,436 [INFO]         \"    .agg(\\n\",\n",
      "2026-01-16 20:11:34,436 [INFO]         \"        ascog_hits=(\\\"gene\\\", lambda x: list(x.unique())),\\n\",\n",
      "2026-01-16 20:11:34,437 [INFO]         \"        ascog_ids=(\\\"query\\\", lambda x: list(x.unique()))\\n\",\n",
      "2026-01-16 20:11:34,437 [INFO]         \"    )\\n\",\n",
      "2026-01-16 20:11:34,437 [INFO]         \"    .reset_index()\\n\",\n",
      "2026-01-16 20:11:34,438 [INFO]         \"    .assign(\\n\",\n",
      "2026-01-16 20:11:34,438 [INFO]         \"        ascog_count=lambda df: df[\\\"ascog_hits\\\"].apply(len)\\n\",\n",
      "2026-01-16 20:11:34,439 [INFO]         \"    )\\n\",\n",
      "2026-01-16 20:11:34,439 [INFO]         \"    .sort_values(by = [\\\"ascog_count\\\"], ascending=[False])\\n\",\n",
      "2026-01-16 20:11:34,440 [INFO]         \"    .assign(\\n\",\n",
      "2026-01-16 20:11:34,441 [INFO]         \"        arch_str=lambda d: d[\\\"ascog_hits\\\"].apply(lambda x: \\\"+\\\".join(sorted(x)))\\n\",\n",
      "2026-01-16 20:11:34,441 [INFO]         \"    )\\n\",\n",
      "2026-01-16 20:11:34,441 [INFO]         \"    .sort_values(\\n\",\n",
      "2026-01-16 20:11:34,442 [INFO]         \"        by=[\\\"ascog_count\\\", \\\"arch_str\\\"],\\n\",\n",
      "2026-01-16 20:11:34,443 [INFO]         \"        ascending=[False, True]\\n\",\n",
      "2026-01-16 20:11:34,443 [INFO]         \"    )\\n\",\n",
      "2026-01-16 20:11:34,444 [INFO]         \")\\n\",\n",
      "2026-01-16 20:11:34,445 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,445 [INFO]         \"logging.info(\\\"Computed protein counts with AsCOG hits\\\")\\n\",\n",
      "2026-01-16 20:11:34,446 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,446 [INFO]         \"STEP += 1\\n\",\n",
      "2026-01-16 20:11:34,447 [INFO]         \"protein_counts_file = os.path.join(MAIN_OUTDIR, f\\\"[STEP:{STEP}]protein_ascog_counts_{rightnow}.csv\\\")\\n\",\n",
      "2026-01-16 20:11:34,448 [INFO]         \"try:\\n\",\n",
      "2026-01-16 20:11:34,448 [INFO]         \"    protein_counts.to_csv(protein_counts_file, index=False)\\n\",\n",
      "2026-01-16 20:11:34,449 [INFO]         \"except Exception as e:\\n\",\n",
      "2026-01-16 20:11:34,449 [INFO]         \"    logging.error(\\\"Failed to write protein counts file %s: %s\\\", protein_counts_file, e)\\n\",\n",
      "2026-01-16 20:11:34,451 [INFO]         \"    raise ValueError(f\\\"Failed to write protein counts file: {e}\\\")\"\n",
      "2026-01-16 20:11:34,452 [INFO]       ]\n",
      "2026-01-16 20:11:34,453 [INFO]     },\n",
      "2026-01-16 20:11:34,454 [INFO]     {\n",
      "2026-01-16 20:11:34,455 [INFO]       \"cell_type\": \"code\",\n",
      "2026-01-16 20:11:34,455 [INFO]       \"execution_count\": null,\n",
      "2026-01-16 20:11:34,456 [INFO]       \"id\": \"e59d2985\",\n",
      "2026-01-16 20:11:34,457 [INFO]       \"metadata\": {},\n",
      "2026-01-16 20:11:34,457 [INFO]       \"outputs\": [],\n",
      "2026-01-16 20:11:34,458 [INFO]       \"source\": [\n",
      "2026-01-16 20:11:34,459 [INFO]         \"# Infer architectures based on rules\\n\",\n",
      "2026-01-16 20:11:34,459 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,460 [INFO]         \"# Order matters: first match wins\\n\",\n",
      "2026-01-16 20:11:34,461 [INFO]         \"ARCH_RULES = [\\n\",\n",
      "2026-01-16 20:11:34,461 [INFO]         \"    # Vps23 canonical\\n\",\n",
      "2026-01-16 20:11:34,463 [INFO]         \"    ({\\\"CC-Vps23\\\", \\\"PH-Vps23\\\", \\\"Vps23\\\"}, \\\"CC-PH-Vps23\\\"),\\n\",\n",
      "2026-01-16 20:11:34,464 [INFO]         \"    ({\\\"CC-Vps23\\\", \\\"PH-Vps23\\\"}, \\\"CC-PH-Vps23\\\"),\\n\",\n",
      "2026-01-16 20:11:34,464 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,465 [INFO]         \"    # E2\\u2013Vps23 fusion\\n\",\n",
      "2026-01-16 20:11:34,465 [INFO]         \"    ({\\\"E2-Vps23\\\", \\\"E2\\\", \\\"Vps23\\\"}, \\\"E2-Vps23\\\"),\\n\",\n",
      "2026-01-16 20:11:34,466 [INFO]         \"    ({\\\"E2-Vps23\\\", \\\"E2\\\"}, \\\"E2-Vps23\\\"),\\n\",\n",
      "2026-01-16 20:11:34,467 [INFO]         \"    ({\\\"E2-Vps23\\\", \\\"Vps23\\\"}, \\\"E2-Vps23\\\"),\\n\",\n",
      "2026-01-16 20:11:34,468 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,468 [INFO]         \"    # ESCRT-I\\u2013linked E2\\n\",\n",
      "2026-01-16 20:11:34,469 [INFO]         \"    ({\\\"Vps28\\\", \\\"E2-Vps23\\\", \\\"E2\\\"}, \\\"E2-ESCRT-I\\\"),\\n\",\n",
      "2026-01-16 20:11:34,470 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,471 [INFO]         \"    # ESCRT-II Vps22-Vps36 fusion\\n\",\n",
      "2026-01-16 20:11:34,472 [INFO]         \"    ({\\\"CC-Vps23\\\", \\\"EAP30\\\"}, \\\"Vps23-EAP30\\\"),\\n\",\n",
      "2026-01-16 20:11:34,473 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,473 [INFO]         \"    # MPN-E3_doms\\n\",\n",
      "2026-01-16 20:11:34,474 [INFO]         \"    ({\\\"MPN\\\", \\\"E3-dom\\\"}, \\\"MPN-E3-dom\\\"),\\n\",\n",
      "2026-01-16 20:11:34,474 [INFO]         \"]\\n\",\n",
      "2026-01-16 20:11:34,475 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,475 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,476 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,476 [INFO]         \"import pandas as pd\\n\",\n",
      "2026-01-16 20:11:34,477 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,477 [INFO]         \"def infer_architecture(group: pd.DataFrame) -> pd.Series:\\n\",\n",
      "2026-01-16 20:11:34,478 [INFO]         \"    \\\"\\\"\\\"\\n\",\n",
      "2026-01-16 20:11:34,478 [INFO]         \"    Infer a single architecture for one protein (target)\\n\",\n",
      "2026-01-16 20:11:34,478 [INFO]         \"    using explicit rules, else fallback to best i-Evalue hit.\\n\",\n",
      "2026-01-16 20:11:34,479 [INFO]         \"    \\\"\\\"\\\"\\n\",\n",
      "2026-01-16 20:11:34,479 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,480 [INFO]         \"    # Clean gene names\\n\",\n",
      "2026-01-16 20:11:34,480 [INFO]         \"    genes = set(\\n\",\n",
      "2026-01-16 20:11:34,480 [INFO]         \"        group[\\\"gene\\\"]\\n\",\n",
      "2026-01-16 20:11:34,481 [INFO]         \"        .dropna()\\n\",\n",
      "2026-01-16 20:11:34,481 [INFO]         \"        .astype(str)\\n\",\n",
      "2026-01-16 20:11:34,482 [INFO]         \"        .str.strip()\\n\",\n",
      "2026-01-16 20:11:34,482 [INFO]         \"    )\\n\",\n",
      "2026-01-16 20:11:34,482 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,483 [INFO]         \"    # 1. Apply rule-based architecture inference\\n\",\n",
      "2026-01-16 20:11:34,483 [INFO]         \"    for rule_genes, architecture in ARCH_RULES:\\n\",\n",
      "2026-01-16 20:11:34,484 [INFO]         \"        if rule_genes.issubset(genes):\\n\",\n",
      "2026-01-16 20:11:34,484 [INFO]         \"            return pd.Series({\\n\",\n",
      "2026-01-16 20:11:34,485 [INFO]         \"                \\\"architecture\\\": architecture,\\n\",\n",
      "2026-01-16 20:11:34,485 [INFO]         \"                \\\"architecture_method\\\": \\\"rule\\\",\\n\",\n",
      "2026-01-16 20:11:34,486 [INFO]         \"                \\\"architecture_components\\\": \\\",\\\".join(sorted(rule_genes))\\n\",\n",
      "2026-01-16 20:11:34,486 [INFO]         \"            })\\n\",\n",
      "2026-01-16 20:11:34,487 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,487 [INFO]         \"    # 2. Fallback: best single hit by independent E-value\\n\",\n",
      "2026-01-16 20:11:34,488 [INFO]         \"    best_hit = (\\n\",\n",
      "2026-01-16 20:11:34,489 [INFO]         \"        group\\n\",\n",
      "2026-01-16 20:11:34,489 [INFO]         \"        .dropna(subset=[\\\"i_evalue\\\"])\\n\",\n",
      "2026-01-16 20:11:34,489 [INFO]         \"        .sort_values(\\\"i_evalue\\\", ascending=True)\\n\",\n",
      "2026-01-16 20:11:34,490 [INFO]         \"        .iloc[0]\\n\",\n",
      "2026-01-16 20:11:34,490 [INFO]         \"    )\\n\",\n",
      "2026-01-16 20:11:34,491 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,491 [INFO]         \"    return pd.Series({\\n\",\n",
      "2026-01-16 20:11:34,492 [INFO]         \"        \\\"architecture\\\": best_hit[\\\"gene\\\"],\\n\",\n",
      "2026-01-16 20:11:34,493 [INFO]         \"        \\\"architecture_method\\\": \\\"best_i_evalue\\\",\\n\",\n",
      "2026-01-16 20:11:34,494 [INFO]         \"        \\\"architecture_components\\\": best_hit[\\\"gene\\\"]\\n\",\n",
      "2026-01-16 20:11:34,494 [INFO]         \"    })\\n\",\n",
      "2026-01-16 20:11:34,494 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,495 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,495 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,496 [INFO]         \"architecture_df = (\\n\",\n",
      "2026-01-16 20:11:34,496 [INFO]         \"    best_hits\\n\",\n",
      "2026-01-16 20:11:34,496 [INFO]         \"    .dropna(subset=[\\\"target\\\", \\\"gene\\\", \\\"i_evalue\\\"])\\n\",\n",
      "2026-01-16 20:11:34,497 [INFO]         \"    .groupby(\\\"target\\\", group_keys=False)\\n\",\n",
      "2026-01-16 20:11:34,497 [INFO]         \"    .apply(infer_architecture)\\n\",\n",
      "2026-01-16 20:11:34,497 [INFO]         \"    .reset_index()\\n\",\n",
      "2026-01-16 20:11:34,498 [INFO]         \")\\n\",\n",
      "2026-01-16 20:11:34,498 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,498 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,499 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,500 [INFO]         \"df_with_arch = best_hits.merge(\\n\",\n",
      "2026-01-16 20:11:34,500 [INFO]         \"    architecture_df,\\n\",\n",
      "2026-01-16 20:11:34,500 [INFO]         \"    on=\\\"target\\\",\\n\",\n",
      "2026-01-16 20:11:34,501 [INFO]         \"    how=\\\"left\\\"\\n\",\n",
      "2026-01-16 20:11:34,501 [INFO]         \")\\n\",\n",
      "2026-01-16 20:11:34,502 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,502 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,503 [INFO]         \"df_with_arch = df_with_arch[[\\\"target\\\",\\\"Name\\\",\\\"Completeness\\\",\\\"Contamination\\\",\\\"Contig_N50\\\",\\\"Total_Contigs\\\",\\\"organism_name\\\",\\\"architecture\\\",\\\"architecture_method\\\",\\\"architecture_components\\\"]]\\n\",\n",
      "2026-01-16 20:11:34,503 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,503 [INFO]         \"df_with_arch.drop_duplicates(inplace=True)\\n\",\n",
      "2026-01-16 20:11:34,504 [INFO]         \"logging.info(\\\"Inferred architectures for proteins\\\")\\n\",\n",
      "2026-01-16 20:11:34,504 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,505 [INFO]         \"STEP += 1\\n\",\n",
      "2026-01-16 20:11:34,505 [INFO]         \"df_with_arch_file = os.path.join(MAIN_OUTDIR, f\\\"[STEP:{STEP}]ESCRT_hits_with_architectures_{rightnow}.csv\\\")\\n\",\n",
      "2026-01-16 20:11:34,506 [INFO]         \"try:\\n\",\n",
      "2026-01-16 20:11:34,506 [INFO]         \"    df_with_arch.to_csv(df_with_arch_file, index=False)\\n\",\n",
      "2026-01-16 20:11:34,506 [INFO]         \"except Exception as e: \\n\",\n",
      "2026-01-16 20:11:34,507 [INFO]         \"    logging.error(\\\"Failed to write architecture output file %s: %s\\\", df_with_arch_file, e)\\n\",\n",
      "2026-01-16 20:11:34,507 [INFO]         \"    raise ValueError(f\\\"Failed to write architecture output file: {e}\\\")\"\n",
      "2026-01-16 20:11:34,508 [INFO]       ]\n",
      "2026-01-16 20:11:34,508 [INFO]     },\n",
      "2026-01-16 20:11:34,509 [INFO]     {\n",
      "2026-01-16 20:11:34,509 [INFO]       \"cell_type\": \"code\",\n",
      "2026-01-16 20:11:34,509 [INFO]       \"execution_count\": null,\n",
      "2026-01-16 20:11:34,510 [INFO]       \"id\": \"109ab58b\",\n",
      "2026-01-16 20:11:34,510 [INFO]       \"metadata\": {},\n",
      "2026-01-16 20:11:34,511 [INFO]       \"outputs\": [],\n",
      "2026-01-16 20:11:34,511 [INFO]       \"source\": [\n",
      "2026-01-16 20:11:34,512 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,512 [INFO]         \"# ------------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,513 [INFO]         \"# Parse Prodigal GFF and assign gene order PER CONTIG\\n\",\n",
      "2026-01-16 20:11:34,513 [INFO]         \"# ------------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,513 [INFO]         \"def parse_prodigal_gff(gff_path, genome_id):\\n\",\n",
      "2026-01-16 20:11:34,514 [INFO]         \"    \\\"\\\"\\\"\\n\",\n",
      "2026-01-16 20:11:34,514 [INFO]         \"    Parse a Prodigal GFF3 file and extract CDS gene order per contig.\\n\",\n",
      "2026-01-16 20:11:34,515 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,515 [INFO]         \"    Gene order is defined by appearance order in the GFF,\\n\",\n",
      "2026-01-16 20:11:34,515 [INFO]         \"    which corresponds to genomic order for Prodigal output.\\n\",\n",
      "2026-01-16 20:11:34,516 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,516 [INFO]         \"    Args:\\n\",\n",
      "2026-01-16 20:11:34,516 [INFO]         \"        gff_path: Path to the GFF file\\n\",\n",
      "2026-01-16 20:11:34,517 [INFO]         \"        genome_id: Identifier for the genome\\n\",\n",
      "2026-01-16 20:11:34,517 [INFO]         \"        \\n\",\n",
      "2026-01-16 20:11:34,518 [INFO]         \"    Returns:\\n\",\n",
      "2026-01-16 20:11:34,518 [INFO]         \"        DataFrame with columns: genome_id, contig, gene_index, \\n\",\n",
      "2026-01-16 20:11:34,518 [INFO]         \"                                protein_id, start, end, strand\\n\",\n",
      "2026-01-16 20:11:34,519 [INFO]         \"    \\\"\\\"\\\"\\n\",\n",
      "2026-01-16 20:11:34,519 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,520 [INFO]         \"    rows = []\\n\",\n",
      "2026-01-16 20:11:34,520 [INFO]         \"    gene_counter = {}  # separate gene index per contig\\n\",\n",
      "2026-01-16 20:11:34,521 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,521 [INFO]         \"    logging.info(f\\\"Parsing GFF: {gff_path.name}\\\")\\n\",\n",
      "2026-01-16 20:11:34,522 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,522 [INFO]         \"    with open(gff_path) as f:\\n\",\n",
      "2026-01-16 20:11:34,522 [INFO]         \"        for line in f:\\n\",\n",
      "2026-01-16 20:11:34,523 [INFO]         \"            # Skip comment lines\\n\",\n",
      "2026-01-16 20:11:34,523 [INFO]         \"            if line.startswith(\\\"#\\\"):\\n\",\n",
      "2026-01-16 20:11:34,524 [INFO]         \"                continue\\n\",\n",
      "2026-01-16 20:11:34,524 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,525 [INFO]         \"            parts = line.rstrip().split(\\\"\\\\t\\\")\\n\",\n",
      "2026-01-16 20:11:34,525 [INFO]         \"            if len(parts) != 9:\\n\",\n",
      "2026-01-16 20:11:34,526 [INFO]         \"                continue\\n\",\n",
      "2026-01-16 20:11:34,526 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,526 [INFO]         \"            contig, source, feature, start, end, score, strand, phase, attrs = parts\\n\",\n",
      "2026-01-16 20:11:34,527 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,527 [INFO]         \"            # Only process CDS features\\n\",\n",
      "2026-01-16 20:11:34,527 [INFO]         \"            if feature != \\\"CDS\\\":\\n\",\n",
      "2026-01-16 20:11:34,528 [INFO]         \"                continue\\n\",\n",
      "2026-01-16 20:11:34,528 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,529 [INFO]         \"            # Parse attributes to extract protein ID\\n\",\n",
      "2026-01-16 20:11:34,529 [INFO]         \"            attr_dict = {}\\n\",\n",
      "2026-01-16 20:11:34,529 [INFO]         \"            for item in attrs.split(\\\";\\\"):\\n\",\n",
      "2026-01-16 20:11:34,530 [INFO]         \"                if \\\"=\\\" in item:\\n\",\n",
      "2026-01-16 20:11:34,531 [INFO]         \"                    k, v = item.split(\\\"=\\\", 1)\\n\",\n",
      "2026-01-16 20:11:34,531 [INFO]         \"                    attr_dict[k] = v\\n\",\n",
      "2026-01-16 20:11:34,531 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,532 [INFO]         \"            protein_id = attr_dict.get(\\\"ID\\\")\\n\",\n",
      "2026-01-16 20:11:34,532 [INFO]         \"            if protein_id is None:\\n\",\n",
      "2026-01-16 20:11:34,533 [INFO]         \"                continue\\n\",\n",
      "2026-01-16 20:11:34,533 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,534 [INFO]         \"            # Increment gene index PER CONTIG (not genome-wide)\\n\",\n",
      "2026-01-16 20:11:34,534 [INFO]         \"            gene_counter.setdefault(contig, 0)\\n\",\n",
      "2026-01-16 20:11:34,534 [INFO]         \"            gene_counter[contig] += 1\\n\",\n",
      "2026-01-16 20:11:34,534 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,535 [INFO]         \"            rows.append({\\n\",\n",
      "2026-01-16 20:11:34,535 [INFO]         \"                \\\"genome_id\\\": genome_id,\\n\",\n",
      "2026-01-16 20:11:34,536 [INFO]         \"                \\\"contig\\\": contig,\\n\",\n",
      "2026-01-16 20:11:34,536 [INFO]         \"                \\\"gene_index\\\": gene_counter[contig],\\n\",\n",
      "2026-01-16 20:11:34,537 [INFO]         \"                \\\"protein_id\\\": protein_id,\\n\",\n",
      "2026-01-16 20:11:34,537 [INFO]         \"                \\\"start\\\": int(start),\\n\",\n",
      "2026-01-16 20:11:34,538 [INFO]         \"                \\\"end\\\": int(end),\\n\",\n",
      "2026-01-16 20:11:34,538 [INFO]         \"                \\\"strand\\\": strand\\n\",\n",
      "2026-01-16 20:11:34,538 [INFO]         \"            })\\n\",\n",
      "2026-01-16 20:11:34,539 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,539 [INFO]         \"    logging.info(f\\\"  \\u2192 Parsed {len(rows)} CDS features across {len(gene_counter)} contigs\\\")\\n\",\n",
      "2026-01-16 20:11:34,539 [INFO]         \"    return pd.DataFrame(rows)\\n\",\n",
      "2026-01-16 20:11:34,540 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,540 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,541 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,541 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,541 [INFO]         \"# ------------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,542 [INFO]         \"# Load only the GFFs required by the hits table\\n\",\n",
      "2026-01-16 20:11:34,542 [INFO]         \"# ------------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,543 [INFO]         \"def load_gffs_from_hits(hits_df, gff_dir):\\n\",\n",
      "2026-01-16 20:11:34,543 [INFO]         \"    \\\"\\\"\\\"\\n\",\n",
      "2026-01-16 20:11:34,543 [INFO]         \"    Parse GFF files only for genomes present in the hits dataframe.\\n\",\n",
      "2026-01-16 20:11:34,544 [INFO]         \"    GFF filenames are inferred as: <genome_file>_genomic/<genome_file>_genomic.gff\\n\",\n",
      "2026-01-16 20:11:34,544 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,545 [INFO]         \"    Args:\\n\",\n",
      "2026-01-16 20:11:34,545 [INFO]         \"        hits_df: DataFrame containing hits with 'genome_file' column\\n\",\n",
      "2026-01-16 20:11:34,546 [INFO]         \"        gff_dir: Directory containing GFF files\\n\",\n",
      "2026-01-16 20:11:34,546 [INFO]         \"        \\n\",\n",
      "2026-01-16 20:11:34,546 [INFO]         \"    Returns:\\n\",\n",
      "2026-01-16 20:11:34,547 [INFO]         \"        Combined DataFrame of all parsed GFF files\\n\",\n",
      "2026-01-16 20:11:34,547 [INFO]         \"    \\\"\\\"\\\"\\n\",\n",
      "2026-01-16 20:11:34,548 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,548 [INFO]         \"    all_gff_rows = []\\n\",\n",
      "2026-01-16 20:11:34,548 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,549 [INFO]         \"    unique_genomes = hits_df[\\\"Name\\\"].unique()\\n\",\n",
      "2026-01-16 20:11:34,549 [INFO]         \"    logging.info(f\\\"Loading GFFs for {len(unique_genomes)} unique genomes\\\")\\n\",\n",
      "2026-01-16 20:11:34,550 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,550 [INFO]         \"    for genome_id in unique_genomes:\\n\",\n",
      "2026-01-16 20:11:34,551 [INFO]         \"        gff_path = gff_dir / f\\\"{genome_id}_genomic\\\" / f\\\"{genome_id}_genomic.gff\\\"\\n\",\n",
      "2026-01-16 20:11:34,551 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,552 [INFO]         \"        if not gff_path.exists():\\n\",\n",
      "2026-01-16 20:11:34,552 [INFO]         \"            logging.warning(f\\\"Missing GFF: {gff_path}\\\")\\n\",\n",
      "2026-01-16 20:11:34,552 [INFO]         \"            continue\\n\",\n",
      "2026-01-16 20:11:34,553 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,553 [INFO]         \"        gff_df = parse_prodigal_gff(gff_path, genome_id)\\n\",\n",
      "2026-01-16 20:11:34,554 [INFO]         \"        all_gff_rows.append(gff_df)\\n\",\n",
      "2026-01-16 20:11:34,554 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,554 [INFO]         \"    if not all_gff_rows:\\n\",\n",
      "2026-01-16 20:11:34,555 [INFO]         \"        logging.error(\\\"No GFF files were successfully loaded.\\\")\\n\",\n",
      "2026-01-16 20:11:34,555 [INFO]         \"        sys.exit(1)\\n\",\n",
      "2026-01-16 20:11:34,556 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,556 [INFO]         \"    combined_gff = pd.concat(all_gff_rows, ignore_index=True)\\n\",\n",
      "2026-01-16 20:11:34,557 [INFO]         \"    logging.info(f\\\"Total CDS features loaded: {len(combined_gff)}\\\")\\n\",\n",
      "2026-01-16 20:11:34,557 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,558 [INFO]         \"    return combined_gff\\n\",\n",
      "2026-01-16 20:11:34,558 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,558 [INFO]         \"def compress_others(tokens, threshold=10):\\n\",\n",
      "2026-01-16 20:11:34,558 [INFO]         \"    \\\"\\\"\\\"\\n\",\n",
      "2026-01-16 20:11:34,559 [INFO]         \"    Compress long runs of 'other' into 'other[n]' if n > threshold.\\n\",\n",
      "2026-01-16 20:11:34,560 [INFO]         \"    \\\"\\\"\\\"\\n\",\n",
      "2026-01-16 20:11:34,560 [INFO]         \"    compressed = []\\n\",\n",
      "2026-01-16 20:11:34,561 [INFO]         \"    i = 0\\n\",\n",
      "2026-01-16 20:11:34,561 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,562 [INFO]         \"    while i < len(tokens):\\n\",\n",
      "2026-01-16 20:11:34,562 [INFO]         \"        if tokens[i] != \\\"other\\\":\\n\",\n",
      "2026-01-16 20:11:34,563 [INFO]         \"            compressed.append(tokens[i])\\n\",\n",
      "2026-01-16 20:11:34,563 [INFO]         \"            i += 1\\n\",\n",
      "2026-01-16 20:11:34,564 [INFO]         \"            continue\\n\",\n",
      "2026-01-16 20:11:34,564 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,564 [INFO]         \"        # count run\\n\",\n",
      "2026-01-16 20:11:34,565 [INFO]         \"        j = i\\n\",\n",
      "2026-01-16 20:11:34,565 [INFO]         \"        while j < len(tokens) and tokens[j] == \\\"other\\\":\\n\",\n",
      "2026-01-16 20:11:34,565 [INFO]         \"            j += 1\\n\",\n",
      "2026-01-16 20:11:34,566 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,566 [INFO]         \"        run_len = j - i\\n\",\n",
      "2026-01-16 20:11:34,566 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,567 [INFO]         \"        if run_len > threshold:\\n\",\n",
      "2026-01-16 20:11:34,568 [INFO]         \"            compressed.append(f\\\"other[{run_len}]\\\")\\n\",\n",
      "2026-01-16 20:11:34,568 [INFO]         \"        else:\\n\",\n",
      "2026-01-16 20:11:34,568 [INFO]         \"            compressed.extend([\\\"other\\\"] * run_len)\\n\",\n",
      "2026-01-16 20:11:34,569 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,569 [INFO]         \"        i = j\\n\",\n",
      "2026-01-16 20:11:34,570 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,570 [INFO]         \"    return compressed\\n\",\n",
      "2026-01-16 20:11:34,570 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,571 [INFO]         \"def extract_neighborhoods(anchor_df, gff_df, window=5):\\n\",\n",
      "2026-01-16 20:11:34,571 [INFO]         \"    \\\"\\\"\\\"\\n\",\n",
      "2026-01-16 20:11:34,572 [INFO]         \"    Extract genomic neighborhoods around ESCRT-related anchor genes.\\n\",\n",
      "2026-01-16 20:11:34,572 [INFO]         \"    \\\"\\\"\\\"\\n\",\n",
      "2026-01-16 20:11:34,572 [INFO]         \"    keywords = CORE_TARGETS\\n\",\n",
      "2026-01-16 20:11:34,573 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,573 [INFO]         \"    logging.info(f\\\"Starting neighborhood extraction (\\u00b1{window} genes)\\\")\\n\",\n",
      "2026-01-16 20:11:34,573 [INFO]         \"    logging.info(f\\\"Initial anchors: {len(anchor_df)}\\\")\\n\",\n",
      "2026-01-16 20:11:34,574 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,574 [INFO]         \"    # Filter anchors by keywords\\n\",\n",
      "2026-01-16 20:11:34,575 [INFO]         \"    filtered_anchors = anchor_df[\\n\",\n",
      "2026-01-16 20:11:34,575 [INFO]         \"        anchor_df[\\\"architecture\\\"]\\n\",\n",
      "2026-01-16 20:11:34,576 [INFO]         \"        .astype(str)\\n\",\n",
      "2026-01-16 20:11:34,576 [INFO]         \"        .str.lower()\\n\",\n",
      "2026-01-16 20:11:34,576 [INFO]         \"        .str.contains(\\\"|\\\".join(keywords))\\n\",\n",
      "2026-01-16 20:11:34,577 [INFO]         \"    ].copy()\\n\",\n",
      "2026-01-16 20:11:34,577 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,577 [INFO]         \"    logging.info(f\\\"Anchors after ESCRT filter: {len(filtered_anchors)}\\\")\\n\",\n",
      "2026-01-16 20:11:34,578 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,578 [INFO]         \"    if filtered_anchors.empty:\\n\",\n",
      "2026-01-16 20:11:34,578 [INFO]         \"        logging.error(\\\"No ESCRT-related anchors found\\\")\\n\",\n",
      "2026-01-16 20:11:34,579 [INFO]         \"        sys.exit(1)\\n\",\n",
      "2026-01-16 20:11:34,579 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,580 [INFO]         \"    neighborhoods = []\\n\",\n",
      "2026-01-16 20:11:34,581 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,581 [INFO]         \"    # Group by genome and contig\\n\",\n",
      "2026-01-16 20:11:34,581 [INFO]         \"    for (genome, contig), anchors in filtered_anchors.groupby(\\n\",\n",
      "2026-01-16 20:11:34,581 [INFO]         \"        [\\\"genome_id\\\", \\\"contig\\\"]\\n\",\n",
      "2026-01-16 20:11:34,582 [INFO]         \"    ):\\n\",\n",
      "2026-01-16 20:11:34,583 [INFO]         \"        anchor_indices = anchors[\\\"gene_index\\\"].values\\n\",\n",
      "2026-01-16 20:11:34,583 [INFO]         \"        \\n\",\n",
      "2026-01-16 20:11:34,583 [INFO]         \"        # Define window boundaries\\n\",\n",
      "2026-01-16 20:11:34,584 [INFO]         \"        start = anchor_indices.min() - window\\n\",\n",
      "2026-01-16 20:11:34,584 [INFO]         \"        end = anchor_indices.max() + window\\n\",\n",
      "2026-01-16 20:11:34,585 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,585 [INFO]         \"        # Extract all genes in the window from gff_df\\n\",\n",
      "2026-01-16 20:11:34,585 [INFO]         \"        block = gff_df[\\n\",\n",
      "2026-01-16 20:11:34,586 [INFO]         \"            (gff_df[\\\"genome_id\\\"] == genome) &\\n\",\n",
      "2026-01-16 20:11:34,586 [INFO]         \"            (gff_df[\\\"contig\\\"] == contig) &\\n\",\n",
      "2026-01-16 20:11:34,586 [INFO]         \"            (gff_df[\\\"gene_index\\\"].between(start, end))\\n\",\n",
      "2026-01-16 20:11:34,587 [INFO]         \"        ].copy()\\n\",\n",
      "2026-01-16 20:11:34,587 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,587 [INFO]         \"        if block.empty:\\n\",\n",
      "2026-01-16 20:11:34,588 [INFO]         \"            continue\\n\",\n",
      "2026-01-16 20:11:34,589 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,589 [INFO]         \"        # Sort by gene position\\n\",\n",
      "2026-01-16 20:11:34,590 [INFO]         \"        block = block.sort_values(\\\"gene_index\\\").reset_index(drop=True)\\n\",\n",
      "2026-01-16 20:11:34,590 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,590 [INFO]         \"        # Create a mapping of protein_id to architecture from ALL anchors\\n\",\n",
      "2026-01-16 20:11:34,591 [INFO]         \"        # (not just filtered ones), so we get complete architecture info\\n\",\n",
      "2026-01-16 20:11:34,591 [INFO]         \"        arch_map = anchor_df.set_index(\\\"protein_id\\\")[\\\"architecture\\\"].to_dict()\\n\",\n",
      "2026-01-16 20:11:34,592 [INFO]         \"        \\n\",\n",
      "2026-01-16 20:11:34,592 [INFO]         \"        # Apply architecture: use anchor architecture if available, else \\\"other\\\"\\n\",\n",
      "2026-01-16 20:11:34,592 [INFO]         \"        block[\\\"architecture\\\"] = block[\\\"protein_id\\\"].map(arch_map).fillna(\\\"other\\\")\\n\",\n",
      "2026-01-16 20:11:34,593 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,593 [INFO]         \"        # Build architecture token list for the ENTIRE neighborhood\\n\",\n",
      "2026-01-16 20:11:34,594 [INFO]         \"        tokens = block[\\\"architecture\\\"].tolist()\\n\",\n",
      "2026-01-16 20:11:34,594 [INFO]         \"        compressed_tokens = compress_others(tokens, threshold=10)\\n\",\n",
      "2026-01-16 20:11:34,595 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,597 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,597 [INFO]         \"        # Add metadata about the neighborhood\\n\",\n",
      "2026-01-16 20:11:34,598 [INFO]         \"        block[\\\"num_anchors\\\"] = len(anchors)\\n\",\n",
      "2026-01-16 20:11:34,598 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,598 [INFO]         \"        block[\\\"window_start\\\"] = start\\n\",\n",
      "2026-01-16 20:11:34,599 [INFO]         \"        block[\\\"window_end\\\"] = end\\n\",\n",
      "2026-01-16 20:11:34,599 [INFO]         \"        block[\\\"neighborhood_architecture_compressed\\\"] = \\\",\\\".join(compressed_tokens)\\n\",\n",
      "2026-01-16 20:11:34,600 [INFO]         \"        block[\\\"position_in_neighborhood\\\"] = range(len(block))\\n\",\n",
      "2026-01-16 20:11:34,600 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,600 [INFO]         \"        neighborhoods.append(block)\\n\",\n",
      "2026-01-16 20:11:34,601 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,602 [INFO]         \"    if not neighborhoods:\\n\",\n",
      "2026-01-16 20:11:34,602 [INFO]         \"        logging.error(\\\"No neighborhoods extracted after processing\\\")\\n\",\n",
      "2026-01-16 20:11:34,603 [INFO]         \"        sys.exit(1)\\n\",\n",
      "2026-01-16 20:11:34,603 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,603 [INFO]         \"    combined = pd.concat(neighborhoods, ignore_index=True)\\n\",\n",
      "2026-01-16 20:11:34,604 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,604 [INFO]         \"    logging.info(\\n\",\n",
      "2026-01-16 20:11:34,605 [INFO]         \"        f\\\"Extracted {len(combined)} genes from \\\"\\n\",\n",
      "2026-01-16 20:11:34,605 [INFO]         \"        f\\\"{len(neighborhoods)} contig neighborhoods\\\"\\n\",\n",
      "2026-01-16 20:11:34,605 [INFO]         \"    )\\n\",\n",
      "2026-01-16 20:11:34,606 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,606 [INFO]         \"    return combined\\n\",\n",
      "2026-01-16 20:11:34,606 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,607 [INFO]         \"# ------------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,607 [INFO]         \"# Load GTDB taxonomy\\n\",\n",
      "2026-01-16 20:11:34,608 [INFO]         \"# ------------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,608 [INFO]         \"def load_gtdb_taxonomy(gtdb_tsv):\\n\",\n",
      "2026-01-16 20:11:34,609 [INFO]         \"    \\\"\\\"\\\"\\n\",\n",
      "2026-01-16 20:11:34,609 [INFO]         \"    Load GTDB taxonomy file and split into rank-specific columns.\\n\",\n",
      "2026-01-16 20:11:34,610 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,610 [INFO]         \"    GTDB format: genome_id\\\\td__Domain;p__Phylum;c__Class;o__Order;f__Family;g__Genus;s__Species\\n\",\n",
      "2026-01-16 20:11:34,611 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,611 [INFO]         \"    Args:\\n\",\n",
      "2026-01-16 20:11:34,611 [INFO]         \"        gtdb_tsv: Path to GTDB taxonomy TSV file\\n\",\n",
      "2026-01-16 20:11:34,612 [INFO]         \"        \\n\",\n",
      "2026-01-16 20:11:34,612 [INFO]         \"    Returns:\\n\",\n",
      "2026-01-16 20:11:34,613 [INFO]         \"        DataFrame with columns: genome_id, domain, phylum, class, order, family, genus, species\\n\",\n",
      "2026-01-16 20:11:34,613 [INFO]         \"    \\\"\\\"\\\"\\n\",\n",
      "2026-01-16 20:11:34,614 [INFO]         \"    logging.info(f\\\"Loading GTDB taxonomy from: {gtdb_tsv}\\\")\\n\",\n",
      "2026-01-16 20:11:34,614 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,615 [INFO]         \"    # Read the taxonomy file\\n\",\n",
      "2026-01-16 20:11:34,615 [INFO]         \"    tax_df = pd.read_csv(\\n\",\n",
      "2026-01-16 20:11:34,615 [INFO]         \"        gtdb_tsv,\\n\",\n",
      "2026-01-16 20:11:34,616 [INFO]         \"        sep=\\\"\\\\t\\\",\\n\",\n",
      "2026-01-16 20:11:34,616 [INFO]         \"        header=None,\\n\",\n",
      "2026-01-16 20:11:34,617 [INFO]         \"        names=[\\\"genome_id\\\", \\\"taxonomy\\\"],\\n\",\n",
      "2026-01-16 20:11:34,617 [INFO]         \"        dtype=str\\n\",\n",
      "2026-01-16 20:11:34,617 [INFO]         \"    )\\n\",\n",
      "2026-01-16 20:11:34,618 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,618 [INFO]         \"    tax_df[\\\"genome_id_base\\\"] = tax_df[\\\"genome_id\\\"].str.split(\\\"_\\\", n=1).str[1]\\n\",\n",
      "2026-01-16 20:11:34,619 [INFO]         \"    logging.info(f\\\"Loaded taxonomy for {len(tax_df)} genomes\\\")\\n\",\n",
      "2026-01-16 20:11:34,619 [INFO]         \"    logging.info(f\\\"Sample taxonomy entry: {tax_df['taxonomy'].iloc[0]}\\\")\\n\",\n",
      "2026-01-16 20:11:34,619 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,620 [INFO]         \"    # Split taxonomy string by semicolons\\n\",\n",
      "2026-01-16 20:11:34,620 [INFO]         \"    tax_split = tax_df[\\\"taxonomy\\\"].str.split(\\\";\\\", expand=True)\\n\",\n",
      "2026-01-16 20:11:34,621 [INFO]         \"    logging.info(f\\\"Taxonomy split into {tax_split.shape[1]} columns\\\")\\n\",\n",
      "2026-01-16 20:11:34,621 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,622 [INFO]         \"    # Map column indices to taxonomic ranks\\n\",\n",
      "2026-01-16 20:11:34,622 [INFO]         \"    rank_map = {\\n\",\n",
      "2026-01-16 20:11:34,623 [INFO]         \"        0: \\\"domain\\\",\\n\",\n",
      "2026-01-16 20:11:34,623 [INFO]         \"        1: \\\"phylum\\\",\\n\",\n",
      "2026-01-16 20:11:34,624 [INFO]         \"        2: \\\"class\\\",\\n\",\n",
      "2026-01-16 20:11:34,624 [INFO]         \"        3: \\\"order\\\",\\n\",\n",
      "2026-01-16 20:11:34,624 [INFO]         \"        4: \\\"family\\\",\\n\",\n",
      "2026-01-16 20:11:34,625 [INFO]         \"        5: \\\"genus\\\",\\n\",\n",
      "2026-01-16 20:11:34,625 [INFO]         \"        6: \\\"species\\\"\\n\",\n",
      "2026-01-16 20:11:34,626 [INFO]         \"    }\\n\",\n",
      "2026-01-16 20:11:34,626 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,626 [INFO]         \"    # Extract each rank and remove the prefix (e.g., \\\"d__\\\", \\\"p__\\\")\\n\",\n",
      "2026-01-16 20:11:34,627 [INFO]         \"    for idx, rank in rank_map.items():\\n\",\n",
      "2026-01-16 20:11:34,627 [INFO]         \"        if idx < tax_split.shape[1]:\\n\",\n",
      "2026-01-16 20:11:34,628 [INFO]         \"            # Remove the rank prefix using regex (e.g., \\\"p__\\\" from \\\"p__Crenarchaeota\\\")\\n\",\n",
      "2026-01-16 20:11:34,628 [INFO]         \"            tax_df[rank] = tax_split[idx].str.replace(r\\\"^[a-z]__\\\", \\\"\\\", regex=True)\\n\",\n",
      "2026-01-16 20:11:34,629 [INFO]         \"            logging.info(f\\\"Extracted {rank}: {tax_df[rank].notna().sum()} non-null values\\\")\\n\",\n",
      "2026-01-16 20:11:34,629 [INFO]         \"        else:\\n\",\n",
      "2026-01-16 20:11:34,629 [INFO]         \"            tax_df[rank] = pd.NA\\n\",\n",
      "2026-01-16 20:11:34,630 [INFO]         \"            logging.warning(f\\\"Column {idx} for rank '{rank}' not found in taxonomy data\\\")\\n\",\n",
      "2026-01-16 20:11:34,630 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,631 [INFO]         \"    # Drop the original concatenated taxonomy string\\n\",\n",
      "2026-01-16 20:11:34,631 [INFO]         \"    tax_df.drop(columns=[\\\"taxonomy\\\"], inplace=True)\\n\",\n",
      "2026-01-16 20:11:34,631 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,632 [INFO]         \"    # Log sample of parsed taxonomy\\n\",\n",
      "2026-01-16 20:11:34,632 [INFO]         \"    logging.info(f\\\"Sample parsed taxonomy:\\\")\\n\",\n",
      "2026-01-16 20:11:34,633 [INFO]         \"    logging.info(tax_df[[\\\"genome_id\\\", \\\"domain\\\", \\\"phylum\\\", \\\"class\\\"]].head().to_string())\\n\",\n",
      "2026-01-16 20:11:34,633 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,634 [INFO]         \"    return tax_df\\n\",\n",
      "2026-01-16 20:11:34,634 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,634 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,635 [INFO]         \"# ------------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,635 [INFO]         \"# Merge taxonomy into neighborhood data\\n\",\n",
      "2026-01-16 20:11:34,635 [INFO]         \"# ------------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,636 [INFO]         \"def merge_taxonomy(escrt_csv, gtdb_tsv, out_csv):\\n\",\n",
      "2026-01-16 20:11:34,637 [INFO]         \"    \\\"\\\"\\\"\\n\",\n",
      "2026-01-16 20:11:34,637 [INFO]         \"    Merge GTDB taxonomy into ESCRT neighborhood dataframe.\\n\",\n",
      "2026-01-16 20:11:34,638 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,638 [INFO]         \"    Args:\\n\",\n",
      "2026-01-16 20:11:34,638 [INFO]         \"        escrt_csv: Path to ESCRT neighborhoods CSV\\n\",\n",
      "2026-01-16 20:11:34,639 [INFO]         \"        gtdb_tsv: Path to GTDB taxonomy TSV\\n\",\n",
      "2026-01-16 20:11:34,639 [INFO]         \"        out_csv: Output path for merged CSV\\n\",\n",
      "2026-01-16 20:11:34,639 [INFO]         \"        \\n\",\n",
      "2026-01-16 20:11:34,640 [INFO]         \"    Returns:\\n\",\n",
      "2026-01-16 20:11:34,640 [INFO]         \"        Merged DataFrame with taxonomy columns added\\n\",\n",
      "2026-01-16 20:11:34,641 [INFO]         \"    \\\"\\\"\\\"\\n\",\n",
      "2026-01-16 20:11:34,641 [INFO]         \"    logging.info(\\\"=\\\" * 70)\\n\",\n",
      "2026-01-16 20:11:34,641 [INFO]         \"    logging.info(\\\"STARTING TAXONOMY MERGE\\\")\\n\",\n",
      "2026-01-16 20:11:34,642 [INFO]         \"    logging.info(\\\"=\\\" * 70)\\n\",\n",
      "2026-01-16 20:11:34,642 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,643 [INFO]         \"    # Load neighborhood data\\n\",\n",
      "2026-01-16 20:11:34,643 [INFO]         \"    logging.info(f\\\"Loading neighborhood data from: {escrt_csv}\\\")\\n\",\n",
      "2026-01-16 20:11:34,644 [INFO]         \"    escrt_df = pd.read_csv(escrt_csv)\\n\",\n",
      "2026-01-16 20:11:34,644 [INFO]         \"    escrt_df['genome_id_base'] = (\\n\",\n",
      "2026-01-16 20:11:34,644 [INFO]         \"    escrt_df['genome_id']\\n\",\n",
      "2026-01-16 20:11:34,645 [INFO]         \"    .str.split(\\\"_\\\", n=2)\\n\",\n",
      "2026-01-16 20:11:34,645 [INFO]         \"    .str[0:2]\\n\",\n",
      "2026-01-16 20:11:34,645 [INFO]         \"    .str.join(\\\"_\\\")\\n\",\n",
      "2026-01-16 20:11:34,646 [INFO]         \")\\n\",\n",
      "2026-01-16 20:11:34,647 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,647 [INFO]         \"    logging.info(f\\\"Neighborhood data: {len(escrt_df)} rows, {len(escrt_df['genome_id'].unique())} unique genomes\\\")\\n\",\n",
      "2026-01-16 20:11:34,648 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,648 [INFO]         \"    # Load taxonomy data\\n\",\n",
      "2026-01-16 20:11:34,649 [INFO]         \"    tax_df = load_gtdb_taxonomy(gtdb_tsv)\\n\",\n",
      "2026-01-16 20:11:34,649 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,649 [INFO]         \"    # Check for overlap between datasets\\n\",\n",
      "2026-01-16 20:11:34,650 [INFO]         \"    escrt_genomes = set(escrt_df[\\\"genome_id_base\\\"].unique())\\n\",\n",
      "2026-01-16 20:11:34,650 [INFO]         \"    tax_genomes = set(tax_df[\\\"genome_id_base\\\"].unique())\\n\",\n",
      "2026-01-16 20:11:34,651 [INFO]         \"    overlap = escrt_genomes.intersection(tax_genomes)\\n\",\n",
      "2026-01-16 20:11:34,651 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,652 [INFO]         \"    logging.info(f\\\"Genome ID overlap check:\\\")\\n\",\n",
      "2026-01-16 20:11:34,652 [INFO]         \"    logging.info(f\\\"  Genomes in neighborhood data: {len(escrt_genomes)}\\\")\\n\",\n",
      "2026-01-16 20:11:34,652 [INFO]         \"    logging.info(f\\\"  Genomes in taxonomy data: {len(tax_genomes)}\\\")\\n\",\n",
      "2026-01-16 20:11:34,653 [INFO]         \"    logging.info(f\\\"  Overlapping genomes: {len(overlap)}\\\")\\n\",\n",
      "2026-01-16 20:11:34,653 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,653 [INFO]         \"    if len(overlap) == 0:\\n\",\n",
      "2026-01-16 20:11:34,654 [INFO]         \"        logging.error(\\\"NO OVERLAP between genome IDs!\\\")\\n\",\n",
      "2026-01-16 20:11:34,654 [INFO]         \"        logging.error(f\\\"Sample neighborhood genome IDs: {list(escrt_genomes)[:5]}\\\")\\n\",\n",
      "2026-01-16 20:11:34,654 [INFO]         \"        logging.error(f\\\"Sample taxonomy genome IDs: {list(tax_genomes)[:5]}\\\")\\n\",\n",
      "2026-01-16 20:11:34,655 [INFO]         \"        logging.error(\\\"Check if genome_id formats match between files!\\\")\\n\",\n",
      "2026-01-16 20:11:34,655 [INFO]         \"        sys.exit(1)\\n\",\n",
      "2026-01-16 20:11:34,656 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,656 [INFO]         \"    if len(overlap) < len(escrt_genomes):\\n\",\n",
      "2026-01-16 20:11:34,657 [INFO]         \"        missing = len(escrt_genomes) - len(overlap)\\n\",\n",
      "2026-01-16 20:11:34,657 [INFO]         \"        logging.warning(f\\\"{missing} genomes from neighborhood data not found in taxonomy!\\\")\\n\",\n",
      "2026-01-16 20:11:34,657 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,658 [INFO]         \"    # Perform the merge\\n\",\n",
      "2026-01-16 20:11:34,659 [INFO]         \"    logging.info(\\\"Performing left join on genome_id...\\\")\\n\",\n",
      "2026-01-16 20:11:34,659 [INFO]         \"    merged = escrt_df.merge(\\n\",\n",
      "2026-01-16 20:11:34,659 [INFO]         \"        tax_df,\\n\",\n",
      "2026-01-16 20:11:34,660 [INFO]         \"        on=\\\"genome_id_base\\\",\\n\",\n",
      "2026-01-16 20:11:34,660 [INFO]         \"        how=\\\"left\\\"\\n\",\n",
      "2026-01-16 20:11:34,660 [INFO]         \"    )\\n\",\n",
      "2026-01-16 20:11:34,661 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,661 [INFO]         \"    # Validate merge results\\n\",\n",
      "2026-01-16 20:11:34,661 [INFO]         \"    logging.info(f\\\"Merge complete: {len(merged)} rows\\\")\\n\",\n",
      "2026-01-16 20:11:34,662 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,662 [INFO]         \"    # Check how many rows got taxonomy data\\n\",\n",
      "2026-01-16 20:11:34,663 [INFO]         \"    tax_columns = [\\\"domain\\\", \\\"phylum\\\", \\\"class\\\", \\\"order\\\", \\\"family\\\", \\\"genus\\\", \\\"species\\\"]\\n\",\n",
      "2026-01-16 20:11:34,663 [INFO]         \"    for col in tax_columns:\\n\",\n",
      "2026-01-16 20:11:34,664 [INFO]         \"        non_null = merged[col].notna().sum()\\n\",\n",
      "2026-01-16 20:11:34,664 [INFO]         \"        pct = (non_null / len(merged)) * 100\\n\",\n",
      "2026-01-16 20:11:34,664 [INFO]         \"        logging.info(f\\\"  {col}: {non_null} non-null ({pct:.1f}%)\\\")\\n\",\n",
      "2026-01-16 20:11:34,665 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,665 [INFO]         \"    # Save to CSV\\n\",\n",
      "2026-01-16 20:11:34,666 [INFO]         \"    logging.info(f\\\"Saving merged data to: {out_csv}\\\")\\n\",\n",
      "2026-01-16 20:11:34,666 [INFO]         \"    merged.to_csv(out_csv, index=False)\\n\",\n",
      "2026-01-16 20:11:34,666 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,667 [INFO]         \"    # Show sample of merged data\\n\",\n",
      "2026-01-16 20:11:34,667 [INFO]         \"    logging.info(\\\"Sample of merged data with taxonomy:\\\")\\n\",\n",
      "2026-01-16 20:11:34,668 [INFO]         \"    sample_cols = [\\\"genome_id\\\", \\\"protein_id\\\", \\\"domain\\\", \\\"phylum\\\", \\\"class\\\", \\\"order\\\"]\\n\",\n",
      "2026-01-16 20:11:34,668 [INFO]         \"    available_cols = [col for col in sample_cols if col in merged.columns]\\n\",\n",
      "2026-01-16 20:11:34,668 [INFO]         \"    logging.info(merged[available_cols].head(10).to_string())\\n\",\n",
      "2026-01-16 20:11:34,669 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,669 [INFO]         \"    logging.info(\\\"=\\\" * 70)\\n\",\n",
      "2026-01-16 20:11:34,670 [INFO]         \"    logging.info(\\\"TAXONOMY MERGE COMPLETE\\\")\\n\",\n",
      "2026-01-16 20:11:34,670 [INFO]         \"    logging.info(\\\"=\\\" * 70)\\n\",\n",
      "2026-01-16 20:11:34,670 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,671 [INFO]         \"    return merged\\n\",\n",
      "2026-01-16 20:11:34,671 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,672 [INFO]         \"\\n\"\n",
      "2026-01-16 20:11:34,672 [INFO]       ]\n",
      "2026-01-16 20:11:34,673 [INFO]     },\n",
      "2026-01-16 20:11:34,673 [INFO]     {\n",
      "2026-01-16 20:11:34,673 [INFO]       \"cell_type\": \"code\",\n",
      "2026-01-16 20:11:34,674 [INFO]       \"execution_count\": null,\n",
      "2026-01-16 20:11:34,674 [INFO]       \"id\": \"d774e625\",\n",
      "2026-01-16 20:11:34,675 [INFO]       \"metadata\": {},\n",
      "2026-01-16 20:11:34,675 [INFO]       \"outputs\": [],\n",
      "2026-01-16 20:11:34,676 [INFO]       \"source\": [\n",
      "2026-01-16 20:11:34,676 [INFO]         \"from pathlib import Path\\n\",\n",
      "2026-01-16 20:11:34,677 [INFO]         \"import os\\n\",\n",
      "2026-01-16 20:11:34,677 [INFO]         \"import sys\\n\",\n",
      "2026-01-16 20:11:34,677 [INFO]         \"import logging\\n\",\n",
      "2026-01-16 20:11:34,678 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,678 [INFO]         \"# ------------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,679 [INFO]         \"# SYNTENY PIPELINE (NOTEBOOK / LINEAR EXECUTION)\\n\",\n",
      "2026-01-16 20:11:34,680 [INFO]         \"# ------------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,680 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,681 [INFO]         \"logging.info(\\\"=\\\" * 70)\\n\",\n",
      "2026-01-16 20:11:34,681 [INFO]         \"logging.info(\\\"STARTING SYNTENY PIPELINE\\\")\\n\",\n",
      "2026-01-16 20:11:34,682 [INFO]         \"logging.info(\\\"=\\\" * 70)\\n\",\n",
      "2026-01-16 20:11:34,682 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,683 [INFO]         \"logging.info(f\\\"Hits DataFrame: {df_with_arch.shape}\\\")\\n\",\n",
      "2026-01-16 20:11:34,683 [INFO]         \"logging.info(f\\\"GFF directory: /home/anirudh/genomes/selected_genomes/prokka_results\\\")\\n\",\n",
      "2026-01-16 20:11:34,684 [INFO]         \"logging.info(f\\\"Window size: \\u00b1{WINDOW} genes\\\")\\n\",\n",
      "2026-01-16 20:11:34,684 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,685 [INFO]         \"gff_dir = Path(\\\"/home/anirudh/genomes/selected_genomes/prokka_results\\\")\\n\",\n",
      "2026-01-16 20:11:34,685 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,685 [INFO]         \"# --------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,686 [INFO]         \"# STEP 1: Load GFFs\\n\",\n",
      "2026-01-16 20:11:34,686 [INFO]         \"# --------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,687 [INFO]         \"logging.info(\\\"\\\\n[STEP 1] Loading GFF files...\\\")\\n\",\n",
      "2026-01-16 20:11:34,687 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,688 [INFO]         \"gff_df = load_gffs_from_hits(df_with_arch, gff_dir)\\n\",\n",
      "2026-01-16 20:11:34,688 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,689 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,689 [INFO]         \"gff_df_file = os.path.join(\\n\",\n",
      "2026-01-16 20:11:34,689 [INFO]         \"    MAIN_OUTDIR,\\n\",\n",
      "2026-01-16 20:11:34,690 [INFO]         \"    f\\\"[STEP:{STEP}.5]gff_dataframe_{rightnow}.csv\\\"\\n\",\n",
      "2026-01-16 20:11:34,690 [INFO]         \")\\n\",\n",
      "2026-01-16 20:11:34,691 [INFO]         \"gff_df.to_csv(gff_df_file, index=False)\\n\",\n",
      "2026-01-16 20:11:34,691 [INFO]         \"logging.info(\\\"Saved GFF dataframe: %s\\\", gff_df_file)\\n\",\n",
      "2026-01-16 20:11:34,691 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,692 [INFO]         \"# --------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,692 [INFO]         \"# STEP 2: Map anchor hits to gene order\\n\",\n",
      "2026-01-16 20:11:34,692 [INFO]         \"# --------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,693 [INFO]         \"logging.info(\\\"\\\\n[STEP 2] Mapping hits to gene order...\\\")\\n\",\n",
      "2026-01-16 20:11:34,693 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,693 [INFO]         \"anchor_df = df_with_arch.merge(\\n\",\n",
      "2026-01-16 20:11:34,694 [INFO]         \"    gff_df,\\n\",\n",
      "2026-01-16 20:11:34,694 [INFO]         \"    left_on=\\\"target\\\",\\n\",\n",
      "2026-01-16 20:11:34,694 [INFO]         \"    right_on=\\\"protein_id\\\",\\n\",\n",
      "2026-01-16 20:11:34,695 [INFO]         \"    how=\\\"inner\\\"\\n\",\n",
      "2026-01-16 20:11:34,696 [INFO]         \")\\n\",\n",
      "2026-01-16 20:11:34,696 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,696 [INFO]         \"if anchor_df.empty:\\n\",\n",
      "2026-01-16 20:11:34,697 [INFO]         \"    logging.error(\\\"No hits could be mapped to GFFs (protein ID mismatch?)\\\")\\n\",\n",
      "2026-01-16 20:11:34,697 [INFO]         \"    sys.exit(1)\\n\",\n",
      "2026-01-16 20:11:34,698 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,698 [INFO]         \"logging.info(\\\"Mapped %d anchor hits to GFF entries\\\", len(anchor_df))\\n\",\n",
      "2026-01-16 20:11:34,698 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,699 [INFO]         \"logging.info(\\\"Saving anchor hits...\\\")\\n\",\n",
      "2026-01-16 20:11:34,699 [INFO]         \"STEP += 1\\n\",\n",
      "2026-01-16 20:11:34,699 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,700 [INFO]         \"anchor_hits_file = os.path.join(\\n\",\n",
      "2026-01-16 20:11:34,700 [INFO]         \"    MAIN_OUTDIR,\\n\",\n",
      "2026-01-16 20:11:34,701 [INFO]         \"    f\\\"[STEP:{STEP}]escrt_anchor_hits_{rightnow}.csv\\\"\\n\",\n",
      "2026-01-16 20:11:34,701 [INFO]         \")\\n\",\n",
      "2026-01-16 20:11:34,701 [INFO]         \"anchor_df.to_csv(anchor_hits_file, index=False)\\n\",\n",
      "2026-01-16 20:11:34,702 [INFO]         \"logging.info(\\\"Saved: %s\\\", anchor_hits_file)\\n\",\n",
      "2026-01-16 20:11:34,702 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,703 [INFO]         \"# --------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,703 [INFO]         \"# STEP 3: Extract contig architectures / neighborhoods\\n\",\n",
      "2026-01-16 20:11:34,704 [INFO]         \"# --------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,704 [INFO]         \"logging.info(\\\"\\\\n[STEP 3] Extracting contig architectures...\\\")\\n\",\n",
      "2026-01-16 20:11:34,704 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,705 [INFO]         \"neigh_df = extract_neighborhoods(\\n\",\n",
      "2026-01-16 20:11:34,705 [INFO]         \"    anchor_df=anchor_df,\\n\",\n",
      "2026-01-16 20:11:34,705 [INFO]         \"    gff_df=gff_df,\\n\",\n",
      "2026-01-16 20:11:34,705 [INFO]         \"    window=WINDOW\\n\",\n",
      "2026-01-16 20:11:34,706 [INFO]         \")\\n\",\n",
      "2026-01-16 20:11:34,707 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,707 [INFO]         \"logging.info(\\\"Extracted %d neighborhood gene rows\\\", len(neigh_df))\\n\",\n",
      "2026-01-16 20:11:34,707 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,708 [INFO]         \"logging.info(\\\"=\\\" * 70)\\n\",\n",
      "2026-01-16 20:11:34,708 [INFO]         \"logging.info(\\\"SYNTENY PIPELINE COMPLETED SUCCESSFULLY\\\")\\n\",\n",
      "2026-01-16 20:11:34,708 [INFO]         \"logging.info(\\\"=\\\" * 70)\\n\",\n",
      "2026-01-16 20:11:34,709 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,709 [INFO]         \"# --------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,710 [INFO]         \"# SANITY CHECKS\\n\",\n",
      "2026-01-16 20:11:34,710 [INFO]         \"# --------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,711 [INFO]         \"logging.info(\\\"\\\\n\\\" + \\\"=\\\" * 70)\\n\",\n",
      "2026-01-16 20:11:34,711 [INFO]         \"logging.info(\\\"PIPELINE OUTPUT SUMMARY\\\")\\n\",\n",
      "2026-01-16 20:11:34,711 [INFO]         \"logging.info(\\\"=\\\" * 70)\\n\",\n",
      "2026-01-16 20:11:34,712 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,712 [INFO]         \"logging.info(\\\"\\\\nAnchor gene architecture distribution:\\\")\\n\",\n",
      "2026-01-16 20:11:34,712 [INFO]         \"logging.info(anchor_df[\\\"architecture\\\"].value_counts().to_string())\\n\",\n",
      "2026-01-16 20:11:34,713 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,713 [INFO]         \"logging.info(\\\"\\\\nFirst few neighborhood entries:\\\")\\n\",\n",
      "2026-01-16 20:11:34,714 [INFO]         \"logging.info(neigh_df.head().to_string())\\n\",\n",
      "2026-01-16 20:11:34,714 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,714 [INFO]         \"logging.info(\\\"\\\\nGenome distribution:\\\")\\n\",\n",
      "2026-01-16 20:11:34,715 [INFO]         \"logging.info(neigh_df[\\\"genome_id\\\"].value_counts().to_string())\\n\",\n",
      "2026-01-16 20:11:34,715 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,716 [INFO]         \"# --------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,716 [INFO]         \"# SAVE INTERMEDIATE RESULTS\\n\",\n",
      "2026-01-16 20:11:34,717 [INFO]         \"# --------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,717 [INFO]         \"logging.info(\\\"\\\\nSaving neighborhood data...\\\")\\n\",\n",
      "2026-01-16 20:11:34,717 [INFO]         \"STEP += 1\\n\",\n",
      "2026-01-16 20:11:34,717 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,718 [INFO]         \"escrt_neighborhoods_file = os.path.join(\\n\",\n",
      "2026-01-16 20:11:34,718 [INFO]         \"    MAIN_OUTDIR,\\n\",\n",
      "2026-01-16 20:11:34,718 [INFO]         \"    f\\\"[STEP:{STEP}]escrt_neighborhoods_{rightnow}.csv\\\"\\n\",\n",
      "2026-01-16 20:11:34,719 [INFO]         \")\\n\",\n",
      "2026-01-16 20:11:34,719 [INFO]         \"neigh_df.to_csv(escrt_neighborhoods_file, index=False)\\n\",\n",
      "2026-01-16 20:11:34,720 [INFO]         \"logging.info(\\\"Saved: %s\\\", escrt_neighborhoods_file)\\n\",\n",
      "2026-01-16 20:11:34,721 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,721 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,721 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,721 [INFO]         \"# --------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,722 [INFO]         \"# STEP 4: Merge with taxonomy\\n\",\n",
      "2026-01-16 20:11:34,722 [INFO]         \"# --------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,723 [INFO]         \"logging.info(\\\"\\\\n\\\" + \\\"=\\\" * 70)\\n\",\n",
      "2026-01-16 20:11:34,723 [INFO]         \"logging.info(\\\"ADDING TAXONOMY INFORMATION\\\")\\n\",\n",
      "2026-01-16 20:11:34,724 [INFO]         \"logging.info(\\\"=\\\" * 70)\\n\",\n",
      "2026-01-16 20:11:34,725 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,726 [INFO]         \"STEP += 1\\n\",\n",
      "2026-01-16 20:11:34,727 [INFO]         \"out_csv = os.path.join(\\n\",\n",
      "2026-01-16 20:11:34,727 [INFO]         \"    MAIN_OUTDIR,\\n\",\n",
      "2026-01-16 20:11:34,728 [INFO]         \"    f\\\"[STEP:{STEP}]escrt_neighborhoods_with_taxonomy_{rightnow}.csv\\\"\\n\",\n",
      "2026-01-16 20:11:34,728 [INFO]         \")\\n\",\n",
      "2026-01-16 20:11:34,729 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,729 [INFO]         \"df = merge_taxonomy(\\n\",\n",
      "2026-01-16 20:11:34,730 [INFO]         \"    escrt_neighborhoods_file,\\n\",\n",
      "2026-01-16 20:11:34,730 [INFO]         \"    \\\"/home/anirudh/synteny/ar53_taxonomy_r226.tsv\\\",\\n\",\n",
      "2026-01-16 20:11:34,731 [INFO]         \"    out_csv\\n\",\n",
      "2026-01-16 20:11:34,731 [INFO]         \")\\n\",\n",
      "2026-01-16 20:11:34,732 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,732 [INFO]         \"logging.info(\\\"Saved taxonomy-annotated neighborhoods: %s\\\", out_csv)\\n\",\n",
      "2026-01-16 20:11:34,733 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,733 [INFO]         \"# --------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,734 [INFO]         \"# TAXONOMIC SUMMARY\\n\",\n",
      "2026-01-16 20:11:34,734 [INFO]         \"# --------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,735 [INFO]         \"logging.info(\\\"\\\\nTaxonomic diversity in results:\\\")\\n\",\n",
      "2026-01-16 20:11:34,735 [INFO]         \"logging.info(\\\"Unique phyla: %d\\\", df[\\\"phylum\\\"].nunique())\\n\",\n",
      "2026-01-16 20:11:34,736 [INFO]         \"logging.info(\\\"Unique classes: %d\\\", df[\\\"class\\\"].nunique())\\n\",\n",
      "2026-01-16 20:11:34,736 [INFO]         \"logging.info(\\\"Unique orders: %d\\\", df[\\\"order\\\"].nunique())\\n\",\n",
      "2026-01-16 20:11:34,737 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,738 [INFO]         \"logging.info(\\\"\\\\nSample genomes with taxonomy:\\\")\\n\",\n",
      "2026-01-16 20:11:34,739 [INFO]         \"logging.info(\\n\",\n",
      "2026-01-16 20:11:34,740 [INFO]         \"    df[\\n\",\n",
      "2026-01-16 20:11:34,741 [INFO]         \"        [\\\"genome_id_base\\\", \\\"phylum\\\", \\\"class\\\", \\\"order\\\"]\\n\",\n",
      "2026-01-16 20:11:34,741 [INFO]         \"    ]\\n\",\n",
      "2026-01-16 20:11:34,742 [INFO]         \"    .drop_duplicates()\\n\",\n",
      "2026-01-16 20:11:34,743 [INFO]         \"    .head(10)\\n\",\n",
      "2026-01-16 20:11:34,744 [INFO]         \"    .to_string()\\n\",\n",
      "2026-01-16 20:11:34,744 [INFO]         \")\\n\",\n",
      "2026-01-16 20:11:34,745 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,745 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,746 [INFO]         \"contigs = (\\n\",\n",
      "2026-01-16 20:11:34,746 [INFO]         \"    df\\n\",\n",
      "2026-01-16 20:11:34,747 [INFO]         \"    .groupby([\\\"genome_id_x\\\", \\\"contig\\\"], as_index=False)\\n\",\n",
      "2026-01-16 20:11:34,748 [INFO]         \"    .agg(\\n\",\n",
      "2026-01-16 20:11:34,748 [INFO]         \"        neighborhood_architecture_compressed=(\\n\",\n",
      "2026-01-16 20:11:34,749 [INFO]         \"            \\\"neighborhood_architecture_compressed\\\", \\\"first\\\"\\n\",\n",
      "2026-01-16 20:11:34,750 [INFO]         \"        )\\n\",\n",
      "2026-01-16 20:11:34,751 [INFO]         \"    )\\n\",\n",
      "2026-01-16 20:11:34,752 [INFO]         \")\\n\",\n",
      "2026-01-16 20:11:34,753 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,753 [INFO]         \"logging.info(\\\"\\\\nContig architecture summary (first 10):\\\")\\n\",\n",
      "2026-01-16 20:11:34,754 [INFO]         \"logging.info(contigs.head(10).to_string())\\n\",\n",
      "2026-01-16 20:11:34,755 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,756 [INFO]         \"contigs.to_csv(os.path.join(MAIN_OUTDIR, f\\\"[STEP:{STEP}.5]contig_architecture_summary_{rightnow}.csv\\\"), index=False)\\n\",\n",
      "2026-01-16 20:11:34,756 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,757 [INFO]         \"logging.info(\\\"\\\\n\\\" + \\\"=\\\" * 70)\\n\",\n",
      "2026-01-16 20:11:34,758 [INFO]         \"logging.info(\\\"ALL PROCESSING COMPLETE\\\")\\n\",\n",
      "2026-01-16 20:11:34,759 [INFO]         \"logging.info(\\\"=\\\" * 70)\\n\"\n",
      "2026-01-16 20:11:34,759 [INFO]       ]\n",
      "2026-01-16 20:11:34,759 [INFO]     },\n",
      "2026-01-16 20:11:34,760 [INFO]     {\n",
      "2026-01-16 20:11:34,760 [INFO]       \"cell_type\": \"code\",\n",
      "2026-01-16 20:11:34,760 [INFO]       \"execution_count\": null,\n",
      "2026-01-16 20:11:34,761 [INFO]       \"id\": \"bc91f487\",\n",
      "2026-01-16 20:11:34,761 [INFO]       \"metadata\": {},\n",
      "2026-01-16 20:11:34,761 [INFO]       \"outputs\": [],\n",
      "2026-01-16 20:11:34,762 [INFO]       \"source\": [\n",
      "2026-01-16 20:11:34,762 [INFO]         \"# ------------------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,762 [INFO]         \"# DIAGNOSTIC: Check which architectures would be filtered by keywords\\n\",\n",
      "2026-01-16 20:11:34,762 [INFO]         \"# ------------------------------------------------------------------\\n\",\n",
      "2026-01-16 20:11:34,763 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,764 [INFO]         \"keyword_pattern = \\\"|\\\".join(CORE_TARGETS)\\n\",\n",
      "2026-01-16 20:11:34,764 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,764 [INFO]         \"# Create a boolean mask for architectures matching keywords\\n\",\n",
      "2026-01-16 20:11:34,765 [INFO]         \"matches_keywords = (\\n\",\n",
      "2026-01-16 20:11:34,765 [INFO]         \"    df_with_arch[\\\"architecture\\\"]\\n\",\n",
      "2026-01-16 20:11:34,765 [INFO]         \"    .astype(str)\\n\",\n",
      "2026-01-16 20:11:34,766 [INFO]         \"    .str.lower()\\n\",\n",
      "2026-01-16 20:11:34,766 [INFO]         \"    .str.contains(keyword_pattern)\\n\",\n",
      "2026-01-16 20:11:34,766 [INFO]         \")\\n\",\n",
      "2026-01-16 20:11:34,767 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,767 [INFO]         \"# Count what would be kept vs discarded\\n\",\n",
      "2026-01-16 20:11:34,768 [INFO]         \"kept_count = matches_keywords.sum()\\n\",\n",
      "2026-01-16 20:11:34,768 [INFO]         \"discarded_count = (~matches_keywords).sum()\\n\",\n",
      "2026-01-16 20:11:34,769 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,770 [INFO]         \"logging.info(\\\"\\\\n\\\" + \\\"=\\\" * 70)\\n\",\n",
      "2026-01-16 20:11:34,770 [INFO]         \"logging.info(\\\"ARCHITECTURE FILTERING DIAGNOSTIC\\\")\\n\",\n",
      "2026-01-16 20:11:34,771 [INFO]         \"logging.info(\\\"=\\\" * 70)\\n\",\n",
      "2026-01-16 20:11:34,771 [INFO]         \"logging.info(f\\\"Total proteins with valid HMM hits (i_evalue \\u2264 1e-5, coverage \\u2265 0.65): {len(df_with_arch)}\\\")\\n\",\n",
      "2026-01-16 20:11:34,772 [INFO]         \"logging.info(f\\\"Proteins with architectures matching keywords: {kept_count}\\\")\\n\",\n",
      "2026-01-16 20:11:34,772 [INFO]         \"logging.info(f\\\"Proteins with architectures NOT matching keywords: {discarded_count}\\\")\\n\",\n",
      "2026-01-16 20:11:34,773 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,773 [INFO]         \"if discarded_count > 0:\\n\",\n",
      "2026-01-16 20:11:34,775 [INFO]         \"    logging.warning(f\\\"\\\\n\\u26a0\\ufe0f  WARNING: {discarded_count} proteins will be EXCLUDED from neighborhoods:\\\")\\n\",\n",
      "2026-01-16 20:11:34,775 [INFO]         \"    excluded = df_with_arch[~matches_keywords][[\\\"target\\\", \\\"architecture\\\"]].drop_duplicates()\\n\",\n",
      "2026-01-16 20:11:34,776 [INFO]         \"    logging.warning(excluded.head(20).to_string())\\n\",\n",
      "2026-01-16 20:11:34,776 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,777 [INFO]         \"    # Count distribution of excluded architectures\\n\",\n",
      "2026-01-16 20:11:34,777 [INFO]         \"    logging.warning(\\\"\\\\nDistribution of excluded architectures:\\\")\\n\",\n",
      "2026-01-16 20:11:34,778 [INFO]         \"    arch_counts = df_with_arch[~matches_keywords][\\\"architecture\\\"].value_counts()\\n\",\n",
      "2026-01-16 20:11:34,779 [INFO]         \"    logging.warning(arch_counts.head(20).to_string())\\n\",\n",
      "2026-01-16 20:11:34,779 [INFO]         \"    \\n\",\n",
      "2026-01-16 20:11:34,779 [INFO]         \"    # Save for review\\n\",\n",
      "2026-01-16 20:11:34,780 [INFO]         \"    excluded_file = os.path.join(MAIN_OUTDIR, f\\\"[DIAGNOSTIC]excluded_proteins_by_keyword_filter_{rightnow}.csv\\\")\\n\",\n",
      "2026-01-16 20:11:34,780 [INFO]         \"    df_with_arch[~matches_keywords][[\\\"target\\\", \\\"architecture\\\", \\\"architecture_method\\\"]].drop_duplicates().sort_values(by = ['architecture'], ascending=False).to_csv(excluded_file, index=False)\\n\",\n",
      "2026-01-16 20:11:34,781 [INFO]         \"    logging.warning(f\\\"\\\\nSaved excluded proteins to: {excluded_file}\\\")\\n\",\n",
      "2026-01-16 20:11:34,781 [INFO]         \"else:\\n\",\n",
      "2026-01-16 20:11:34,781 [INFO]         \"    logging.info(\\\"\\u2713 All proteins with valid hits match keyword filter - no data loss\\\")\\n\",\n",
      "2026-01-16 20:11:34,782 [INFO]         \"\\n\",\n",
      "2026-01-16 20:11:34,782 [INFO]         \"logging.info(\\\"=\\\" * 70)\"\n",
      "2026-01-16 20:11:34,783 [INFO]       ]\n",
      "2026-01-16 20:11:34,783 [INFO]     }\n",
      "2026-01-16 20:11:34,784 [INFO]   ],\n",
      "2026-01-16 20:11:34,784 [INFO]   \"metadata\": {\n",
      "2026-01-16 20:11:34,784 [INFO]     \"kernelspec\": {\n",
      "2026-01-16 20:11:34,785 [INFO]       \"display_name\": \"Reg\",\n",
      "2026-01-16 20:11:34,785 [INFO]       \"language\": \"python\",\n",
      "2026-01-16 20:11:34,786 [INFO]       \"name\": \"python3\"\n",
      "2026-01-16 20:11:34,786 [INFO]     },\n",
      "2026-01-16 20:11:34,786 [INFO]     \"language_info\": {\n",
      "2026-01-16 20:11:34,786 [INFO]       \"codemirror_mode\": {\n",
      "2026-01-16 20:11:34,787 [INFO]         \"name\": \"ipython\",\n",
      "2026-01-16 20:11:34,788 [INFO]         \"version\": 3\n",
      "2026-01-16 20:11:34,788 [INFO]       },\n",
      "2026-01-16 20:11:34,788 [INFO]       \"file_extension\": \".py\",\n",
      "2026-01-16 20:11:34,789 [INFO]       \"mimetype\": \"text/x-python\",\n",
      "2026-01-16 20:11:34,789 [INFO]       \"name\": \"python\",\n",
      "2026-01-16 20:11:34,790 [INFO]       \"nbconvert_exporter\": \"python\",\n",
      "2026-01-16 20:11:34,790 [INFO]       \"pygments_lexer\": \"ipython3\",\n",
      "2026-01-16 20:11:34,790 [INFO]       \"version\": \"3.13.9\"\n",
      "2026-01-16 20:11:34,791 [INFO]     }\n",
      "2026-01-16 20:11:34,791 [INFO]   },\n",
      "2026-01-16 20:11:34,792 [INFO]   \"nbformat\": 4,\n",
      "2026-01-16 20:11:34,792 [INFO]   \"nbformat_minor\": 5\n",
      "2026-01-16 20:11:34,793 [INFO] }\n"
     ]
    }
   ],
   "source": [
    "import json\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Load the current notebook file (Jupyter notebooks are JSON)\n",
    "# ------------------------------------------------------------------\n",
    "with open(NOTEBOOK_PATH, \"r\", encoding=\"utf-8\") as nb:\n",
    "    notebook_json = json.load(nb)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Remove execution outputs from code cells\n",
    "#   - prevents massive logs\n",
    "#   - avoids embedding binary blobs (plots, images)\n",
    "#   - keeps only the executable logic + structure\n",
    "# ------------------------------------------------------------------\n",
    "def strip_outputs(nb):\n",
    "    for cell in nb.get(\"cells\", []):\n",
    "        if cell.get(\"cell_type\") == \"code\":\n",
    "            cell[\"outputs\"] = []\n",
    "            cell[\"execution_count\"] = None\n",
    "    return nb\n",
    "\n",
    "notebook_json = strip_outputs(notebook_json)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Serialize notebook JSON to a formatted string\n",
    "#   - indentation preserves readability in logs\n",
    "#   - done once to avoid repeated JSON encoding\n",
    "# ------------------------------------------------------------------\n",
    "notebook_str = json.dumps(notebook_json, indent=2)\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------------\n",
    "# Append notebook snapshot to the log file\n",
    "#   - logged line-by-line to preserve formatting\n",
    "#   - ensures compatibility with logging handlers\n",
    "# ------------------------------------------------------------------\n",
    "logging.info(\"=\" * 80)\n",
    "logging.info(\"NOTEBOOK JSON SNAPSHOT (Start of Run)\")\n",
    "logging.info(\"=\" * 80)\n",
    "\n",
    "for line in notebook_str.splitlines():\n",
    "    logging.info(line)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "id": "25546ebc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-16 20:11:34,799 [INFO] HMM hits file: /home/anirudh/synteny/hmms/ESCRT_results/hits/ESCRT_hits_final.csv\n",
      "2026-01-16 20:11:34,800 [INFO] Protein metadata file: /home/anirudh/synteny/proteins_genomes_cp90_con5.csv\n",
      "2026-01-16 20:11:34,801 [INFO] Merged output file: /home/anirudh/synteny/hmms/ESCRT_synteny_pipeline_m3v2_family_2026-01-16-20-11-34/[STEP:1]ESCRT_hits_final_merged_2026-01-16-20-11-34.csv\n",
      "2026-01-16 20:11:34,828 [INFO] HMM hits columns: ['target', 'tacc', 'tlen', 'query', 'qacc', 'qlen', 'full_evalue', 'full_score', 'full_bias', 'dom_idx', 'dom_count', 'c_evalue', 'i_evalue', 'dom_score', 'dom_bias', 'hmm_from', 'hmm_to', 'ali_from', 'ali_to', 'env_from', 'env_to', 'acc', 'description', 'source_file']\n",
      "2026-01-16 20:11:34,828 [INFO] HMM hits rows: 24597\n",
      "2026-01-16 20:11:35,916 [INFO] Protein metadata columns: ['Name', 'Completeness', 'Contamination', 'Completeness_Model_Used', 'Translation_Table_Used', 'Coding_Density', 'Contig_N50', 'Average_Gene_Length', 'Genome_Size', 'GC_Content', 'Total_Coding_Sequences', 'Total_Contigs', 'Max_Contig_Length', 'Additional_Notes', 'Unnamed: 0', 'organism_name', 'breed', 'strain', 'cds_id', 'header', 'genome_file', 'sequence']\n",
      "2026-01-16 20:11:35,917 [INFO] Protein metadata rows: 390025\n",
      "2026-01-16 20:11:35,918 [INFO] Input sanity checks passed\n",
      "2026-01-16 20:11:35,918 [INFO] Merging HMM hits with protein metadata\n",
      "2026-01-16 20:11:35,918 [INFO] Rows before merge: hits=24597, proteins=390025\n",
      "2026-01-16 20:11:35,979 [INFO] Rows after merge: 24597\n",
      "2026-01-16 20:11:35,981 [INFO] Unique targets merged: 7285\n",
      "2026-01-16 20:11:36,309 [INFO] Merged output written to: /home/anirudh/synteny/hmms/ESCRT_synteny_pipeline_m3v2_family_2026-01-16-20-11-34/[STEP:1]ESCRT_hits_final_merged_2026-01-16-20-11-34.csv\n"
     ]
    }
   ],
   "source": [
    "# Merge HMM hits with protein metadata\n",
    "\n",
    "\n",
    "# ----------------------------\n",
    "# Input files\n",
    "# ----------------------------\n",
    "hits_file = \"/home/anirudh/synteny/hmms/ESCRT_results/hits/ESCRT_hits_final.csv\"\n",
    "proteins_file = \"/home/anirudh/synteny/proteins_genomes_cp90_con5.csv\"\n",
    "\n",
    "# ----------------------------\n",
    "# Output file naming\n",
    "# ----------------------------\n",
    "hits_file_name = os.path.basename(hits_file).split(\".\")[0]\n",
    "STEP += 1\n",
    "merge_file_name = os.path.join(MAIN_OUTDIR, f\"[STEP:{STEP}]{hits_file_name}_merged_{rightnow}.csv\")\n",
    "\n",
    "logging.info(\"HMM hits file: %s\", hits_file)\n",
    "logging.info(\"Protein metadata file: %s\", proteins_file)\n",
    "logging.info(\"Merged output file: %s\", merge_file_name)\n",
    "\n",
    "# ----------------------------\n",
    "# Load HMM hits\n",
    "# ----------------------------\n",
    "try:\n",
    "    hits = pd.read_csv(hits_file)\n",
    "except Exception as e:\n",
    "    logging.error(\"Failed to load HMM hits file: %s\", e)\n",
    "    raise\n",
    "\n",
    "hits.columns = hits.columns.str.strip()\n",
    "logging.info(\"HMM hits columns: %s\", list(hits.columns))\n",
    "logging.info(\"HMM hits rows: %d\", len(hits))\n",
    "\n",
    "# ----------------------------\n",
    "# Load protein metadata\n",
    "# ----------------------------\n",
    "try:\n",
    "    proteins = pd.read_csv(proteins_file)\n",
    "except Exception as e:\n",
    "    logging.error(\"Failed to load protein metadata file: %s\", e)\n",
    "    raise\n",
    "\n",
    "proteins.columns = proteins.columns.str.strip()\n",
    "logging.info(\"Protein metadata columns: %s\", list(proteins.columns))\n",
    "logging.info(\"Protein metadata rows: %d\", len(proteins))\n",
    "\n",
    "# ----------------------------\n",
    "# Sanity checks before merge\n",
    "# ----------------------------\n",
    "required_hits_cols = {\"target\"}\n",
    "required_prot_cols = {\"cds_id\"}\n",
    "\n",
    "missing_hits = required_hits_cols - set(hits.columns)\n",
    "missing_prots = required_prot_cols - set(proteins.columns)\n",
    "\n",
    "if missing_hits:\n",
    "    logging.error(\"Missing required columns in HMM hits file: %s\", missing_hits)\n",
    "    raise ValueError(f\"Missing columns in hits file: {missing_hits}\")\n",
    "\n",
    "if missing_prots:\n",
    "    logging.error(\"Missing required columns in protein metadata file: %s\", missing_prots)\n",
    "    raise ValueError(f\"Missing columns in proteins file: {missing_prots}\")\n",
    "\n",
    "logging.info(\"Input sanity checks passed\")\n",
    "\n",
    "# ----------------------------\n",
    "# Merge HMM hits with protein metadata\n",
    "# ----------------------------\n",
    "logging.info(\"Merging HMM hits with protein metadata\")\n",
    "logging.info(\"Rows before merge: hits=%d, proteins=%d\", len(hits), len(proteins))\n",
    "\n",
    "merged = pd.merge(\n",
    "    hits,\n",
    "    proteins,\n",
    "    left_on=\"target\",\n",
    "    right_on=\"cds_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "logging.info(\"Rows after merge: %d\", len(merged))\n",
    "logging.info(\"Unique targets merged: %d\", merged[\"target\"].nunique())\n",
    "\n",
    "# ----------------------------\n",
    "# Write output\n",
    "# ----------------------------\n",
    "try:\n",
    "    \n",
    "    merged.to_csv(merge_file_name, index=False)\n",
    "except Exception as e:\n",
    "    logging.error(\"Failed to write merged output file %s: %s\", merge_file_name, e)\n",
    "    raise\n",
    "\n",
    "logging.info(\"Merged output written to: %s\", merge_file_name)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5357cf73",
   "metadata": {},
   "source": [
    "Merge with the ascogs definition"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 147,
   "id": "53f1d3d2",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-16 20:11:36,315 [INFO] Loading AsCOGs from: /home/anirudh/synteny/hmms/Sources/ascogs_escrt_system_20260116_121403.tsv\n",
      "2026-01-16 20:11:36,317 [INFO] AsCOGs columns: ['ascog_id', 'arcog_id', 'category', 'gene', 'description']\n",
      "2026-01-16 20:11:36,318 [INFO] AsCOGs rows: 89\n",
      "2026-01-16 20:11:36,318 [INFO] AsCOGs sanity check passed\n",
      "2026-01-16 20:11:36,319 [INFO] Merging HMM hits with AsCOG annotations\n",
      "2026-01-16 20:11:36,320 [INFO] HMM hits rows before merge: 24597\n",
      "2026-01-16 20:11:36,329 [INFO] Rows after AsCOG merge: 24597\n",
      "2026-01-16 20:11:36,331 [INFO] Unique AsCOGs matched: 88\n",
      "2026-01-16 20:11:36,333 [INFO] Unique proteins matched: 7285\n",
      "2026-01-16 20:11:36,468 [INFO] Final merged file written to: /home/anirudh/synteny/hmms/ESCRT_synteny_pipeline_m3v2_family_2026-01-16-20-11-34/[STEP:2]ESCRT_hits_final_merged_2026-01-16-20-11-34_ascog.csv\n"
     ]
    }
   ],
   "source": [
    "# Merge HMM hits with AsCOG annotations\n",
    "\n",
    "# ----------------------------\n",
    "# Load AsCOG annotation table\n",
    "# ----------------------------\n",
    "ascogs_tsv = f\"/home/anirudh/synteny/hmms/Sources/{HMM_FILE}\"\n",
    "logging.info(\"Loading AsCOGs from: %s\", ascogs_tsv)\n",
    "\n",
    "try:\n",
    "    ascogs = pd.read_csv(ascogs_tsv, sep=\"\\t\")\n",
    "except Exception as e:\n",
    "    logging.error(\"Failed to load AsCOGs file: %s\", e)\n",
    "    raise\n",
    "\n",
    "ascogs.columns = ascogs.columns.str.strip()\n",
    "\n",
    "logging.info(\"AsCOGs columns: %s\", list(ascogs.columns))\n",
    "logging.info(\"AsCOGs rows: %d\", len(ascogs))\n",
    "\n",
    "# ----------------------------\n",
    "# Sanity checks\n",
    "# ----------------------------\n",
    "required_cols = {\"ascog_id\"}\n",
    "missing = required_cols - set(ascogs.columns)\n",
    "\n",
    "if missing:\n",
    "    logging.error(\"Missing required columns in AsCOGs file: %s\", missing)\n",
    "    raise ValueError(f\"Missing required columns: {missing}\")\n",
    "\n",
    "logging.info(\"AsCOGs sanity check passed\")\n",
    "\n",
    "# ----------------------------\n",
    "# Merge HMM hits with AsCOGs\n",
    "# ----------------------------\n",
    "logging.info(\"Merging HMM hits with AsCOG annotations\")\n",
    "logging.info(\"HMM hits rows before merge: %d\", len(merged))\n",
    "\n",
    "merged_ascogs = pd.merge(\n",
    "    merged,\n",
    "    ascogs,\n",
    "    left_on=\"query\",\n",
    "    right_on=\"ascog_id\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "logging.info(\"Rows after AsCOG merge: %d\", len(merged_ascogs))\n",
    "logging.info(\"Unique AsCOGs matched: %d\", merged_ascogs[\"ascog_id\"].nunique())\n",
    "logging.info(\"Unique proteins matched: %d\", merged_ascogs[\"target\"].nunique())\n",
    "\n",
    "# ----------------------------\n",
    "# Write output\n",
    "# ----------------------------\n",
    "STEP += 1\n",
    "final_output_file = os.path.join(MAIN_OUTDIR, f\"[STEP:{STEP}]ESCRT_hits_final_merged_{rightnow}_ascog.csv\")\n",
    "\n",
    "merged_ascogs = merged_ascogs[[\"target\", \"tacc\", \"tlen\", \"query\", \"qacc\", \"qlen\", \"c_evalue\", \"i_evalue\", \"dom_score\",  \"hmm_from\", \"hmm_to\", \"ali_from\", \"ali_to\", \"env_from\", \"env_to\", \"acc\", \"Name\", \"Completeness\", \"Contamination\", \"Contig_N50\", \"Total_Contigs\", \"organism_name\", \"cds_id\", \"header\", \"gene\", \"description_y\"]]\n",
    "\n",
    "try:\n",
    "    merged_ascogs.to_csv(final_output_file, index=False)\n",
    "except Exception as e:\n",
    "    logging.error(\"Failed to write output file %s: %s\", final_output_file, e)\n",
    "    raise\n",
    "\n",
    "logging.info(\"Final merged file written to: %s\", final_output_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 148,
   "id": "626cee5d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-16 20:11:36,533 [INFO] Best hits file written to: /home/anirudh/synteny/hmms/ESCRT_synteny_pipeline_m3v2_family_2026-01-16-20-11-34/[STEP:3]ESCRT_hits_final_merged_2026-01-16-20-11-34_ascog_best_hits.csv\n"
     ]
    }
   ],
   "source": [
    "# Select best hits based on e-value and coverage\n",
    "\n",
    "merged_ascogs[\"coverage\"] = (\n",
    "    (merged_ascogs[\"hmm_to\"] - merged_ascogs[\"hmm_from\"] + 1)\n",
    "    / merged_ascogs[\"qlen\"]\n",
    ")\n",
    "\n",
    "merged_ascogs = (\n",
    "    merged_ascogs\n",
    "    .sort_values(\n",
    "        by=[\"target\", \"query\", \"i_evalue\", \"coverage\", \"dom_score\"],\n",
    "        ascending=[True, True, True, False, False]\n",
    "    )\n",
    ")\n",
    "best_hits = (\n",
    "    merged_ascogs\n",
    "    .drop_duplicates(subset=[\"target\", \"query\"], keep=\"first\")\n",
    ")\n",
    "\n",
    "best_hits = best_hits[(best_hits[\"i_evalue\"] <= 1e-5) & (best_hits[\"coverage\"] >= 0.65)]\n",
    "\n",
    "best_hits = best_hits[[\"target\",\"query\",\"gene\",\"dom_score\",\"i_evalue\",\"coverage\",\"description_y\",\"tacc\",\"tlen\",\"qacc\",\"qlen\",\"c_evalue\",\"hmm_from\",\"hmm_to\",\"ali_from\",\"ali_to\",\"env_from\",\"env_to\",\"acc\",\"Name\",\"Completeness\",\"Contamination\",\"Contig_N50\",\"Total_Contigs\",\"organism_name\"]]\n",
    "\n",
    "STEP += 1\n",
    "best_hits_file = os.path.join(MAIN_OUTDIR, f\"[STEP:{STEP}]ESCRT_hits_final_merged_{rightnow}_ascog_best_hits.csv\")\n",
    "\n",
    "best_hits.to_csv(best_hits_file, index=False)\n",
    "logging.info(\"Best hits file written to: %s\", best_hits_file)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 149,
   "id": "b722f80f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-16 20:11:36,621 [INFO] Computed protein counts with AsCOG hits\n"
     ]
    }
   ],
   "source": [
    "# Compute protein counts with AsCOG hits\n",
    "\n",
    "\n",
    "protein_counts = (\n",
    "    best_hits\n",
    "    .dropna(subset=[\"gene\", \"query\"])\n",
    "    .assign(\n",
    "        gene=lambda df: df[\"gene\"].str.strip(),\n",
    "        ascog_id=lambda df: df[\"query\"].str.strip()\n",
    "    )\n",
    "    .groupby(\"target\")\n",
    "    .agg(\n",
    "        ascog_hits=(\"gene\", lambda x: list(x.unique())),\n",
    "        ascog_ids=(\"query\", lambda x: list(x.unique()))\n",
    "    )\n",
    "    .reset_index()\n",
    "    .assign(\n",
    "        ascog_count=lambda df: df[\"ascog_hits\"].apply(len)\n",
    "    )\n",
    "    .sort_values(by = [\"ascog_count\"], ascending=[False])\n",
    "    .assign(\n",
    "        arch_str=lambda d: d[\"ascog_hits\"].apply(lambda x: \"+\".join(sorted(x)))\n",
    "    )\n",
    "    .sort_values(\n",
    "        by=[\"ascog_count\", \"arch_str\"],\n",
    "        ascending=[False, True]\n",
    "    )\n",
    ")\n",
    "\n",
    "logging.info(\"Computed protein counts with AsCOG hits\")\n",
    "\n",
    "STEP += 1\n",
    "protein_counts_file = os.path.join(MAIN_OUTDIR, f\"[STEP:{STEP}]protein_ascog_counts_{rightnow}.csv\")\n",
    "try:\n",
    "    protein_counts.to_csv(protein_counts_file, index=False)\n",
    "except Exception as e:\n",
    "    logging.error(\"Failed to write protein counts file %s: %s\", protein_counts_file, e)\n",
    "    raise ValueError(f\"Failed to write protein counts file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 150,
   "id": "e59d2985",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-16 20:11:38,680 [INFO] Inferred architectures for proteins\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_212142/1008619236.py:71: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(infer_architecture)\n"
     ]
    }
   ],
   "source": [
    "# Infer architectures based on rules\n",
    "\n",
    "# Order matters: first match wins\n",
    "ARCH_RULES = [\n",
    "    # Vps23 canonical\n",
    "    ({\"CC-Vps23\", \"PH-Vps23\", \"Vps23\"}, \"CC-PH-Vps23\"),\n",
    "    ({\"CC-Vps23\", \"PH-Vps23\"}, \"CC-PH-Vps23\"),\n",
    "\n",
    "    # E2Vps23 fusion\n",
    "    ({\"E2-Vps23\", \"E2\", \"Vps23\"}, \"E2-Vps23\"),\n",
    "    ({\"E2-Vps23\", \"E2\"}, \"E2-Vps23\"),\n",
    "    ({\"E2-Vps23\", \"Vps23\"}, \"E2-Vps23\"),\n",
    "\n",
    "    # ESCRT-Ilinked E2\n",
    "    ({\"Vps28\", \"E2-Vps23\", \"E2\"}, \"E2-ESCRT-I\"),\n",
    "\n",
    "    # ESCRT-II Vps22-Vps36 fusion\n",
    "    ({\"CC-Vps23\", \"EAP30\"}, \"Vps23-EAP30\"),\n",
    "\n",
    "    # MPN-E3_doms\n",
    "    ({\"MPN\", \"E3-dom\"}, \"MPN-E3-dom\"),\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "def infer_architecture(group: pd.DataFrame) -> pd.Series:\n",
    "    \"\"\"\n",
    "    Infer a single architecture for one protein (target)\n",
    "    using explicit rules, else fallback to best i-Evalue hit.\n",
    "    \"\"\"\n",
    "\n",
    "    # Clean gene names\n",
    "    genes = set(\n",
    "        group[\"gene\"]\n",
    "        .dropna()\n",
    "        .astype(str)\n",
    "        .str.strip()\n",
    "    )\n",
    "\n",
    "    # 1. Apply rule-based architecture inference\n",
    "    for rule_genes, architecture in ARCH_RULES:\n",
    "        if rule_genes.issubset(genes):\n",
    "            return pd.Series({\n",
    "                \"architecture\": architecture,\n",
    "                \"architecture_method\": \"rule\",\n",
    "                \"architecture_components\": \",\".join(sorted(rule_genes))\n",
    "            })\n",
    "\n",
    "    # 2. Fallback: best single hit by independent E-value\n",
    "    best_hit = (\n",
    "        group\n",
    "        .dropna(subset=[\"i_evalue\"])\n",
    "        .sort_values(\"i_evalue\", ascending=True)\n",
    "        .iloc[0]\n",
    "    )\n",
    "\n",
    "    return pd.Series({\n",
    "        \"architecture\": best_hit[\"gene\"],\n",
    "        \"architecture_method\": \"best_i_evalue\",\n",
    "        \"architecture_components\": best_hit[\"gene\"]\n",
    "    })\n",
    "\n",
    "\n",
    "\n",
    "architecture_df = (\n",
    "    best_hits\n",
    "    .dropna(subset=[\"target\", \"gene\", \"i_evalue\"])\n",
    "    .groupby(\"target\", group_keys=False)\n",
    "    .apply(infer_architecture)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "\n",
    "\n",
    "df_with_arch = best_hits.merge(\n",
    "    architecture_df,\n",
    "    on=\"target\",\n",
    "    how=\"left\"\n",
    ")\n",
    "\n",
    "\n",
    "df_with_arch = df_with_arch[[\"target\",\"Name\",\"Completeness\",\"Contamination\",\"Contig_N50\",\"Total_Contigs\",\"organism_name\",\"architecture\",\"architecture_method\",\"architecture_components\"]]\n",
    "\n",
    "df_with_arch.drop_duplicates(inplace=True)\n",
    "logging.info(\"Inferred architectures for proteins\")\n",
    "\n",
    "STEP += 1\n",
    "df_with_arch_file = os.path.join(MAIN_OUTDIR, f\"[STEP:{STEP}]ESCRT_hits_with_architectures_{rightnow}.csv\")\n",
    "try:\n",
    "    df_with_arch.to_csv(df_with_arch_file, index=False)\n",
    "except Exception as e: \n",
    "    logging.error(\"Failed to write architecture output file %s: %s\", df_with_arch_file, e)\n",
    "    raise ValueError(f\"Failed to write architecture output file: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 151,
   "id": "109ab58b",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# ------------------------------------------------------------\n",
    "# Parse Prodigal GFF and assign gene order PER CONTIG\n",
    "# ------------------------------------------------------------\n",
    "def parse_prodigal_gff(gff_path, genome_id):\n",
    "    \"\"\"\n",
    "    Parse a Prodigal GFF3 file and extract CDS gene order per contig.\n",
    "\n",
    "    Gene order is defined by appearance order in the GFF,\n",
    "    which corresponds to genomic order for Prodigal output.\n",
    "    \n",
    "    Args:\n",
    "        gff_path: Path to the GFF file\n",
    "        genome_id: Identifier for the genome\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with columns: genome_id, contig, gene_index, \n",
    "                                protein_id, start, end, strand\n",
    "    \"\"\"\n",
    "\n",
    "    rows = []\n",
    "    gene_counter = {}  # separate gene index per contig\n",
    "\n",
    "    logging.info(f\"Parsing GFF: {gff_path.name}\")\n",
    "\n",
    "    with open(gff_path) as f:\n",
    "        for line in f:\n",
    "            # Skip comment lines\n",
    "            if line.startswith(\"#\"):\n",
    "                continue\n",
    "\n",
    "            parts = line.rstrip().split(\"\\t\")\n",
    "            if len(parts) != 9:\n",
    "                continue\n",
    "\n",
    "            contig, source, feature, start, end, score, strand, phase, attrs = parts\n",
    "\n",
    "            # Only process CDS features\n",
    "            if feature != \"CDS\":\n",
    "                continue\n",
    "\n",
    "            # Parse attributes to extract protein ID\n",
    "            attr_dict = {}\n",
    "            for item in attrs.split(\";\"):\n",
    "                if \"=\" in item:\n",
    "                    k, v = item.split(\"=\", 1)\n",
    "                    attr_dict[k] = v\n",
    "\n",
    "            protein_id = attr_dict.get(\"ID\")\n",
    "            if protein_id is None:\n",
    "                continue\n",
    "\n",
    "            # Increment gene index PER CONTIG (not genome-wide)\n",
    "            gene_counter.setdefault(contig, 0)\n",
    "            gene_counter[contig] += 1\n",
    "\n",
    "            rows.append({\n",
    "                \"genome_id\": genome_id,\n",
    "                \"contig\": contig,\n",
    "                \"gene_index\": gene_counter[contig],\n",
    "                \"protein_id\": protein_id,\n",
    "                \"start\": int(start),\n",
    "                \"end\": int(end),\n",
    "                \"strand\": strand\n",
    "            })\n",
    "\n",
    "    logging.info(f\"   Parsed {len(rows)} CDS features across {len(gene_counter)} contigs\")\n",
    "    return pd.DataFrame(rows)\n",
    "\n",
    "\n",
    "    \n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load only the GFFs required by the hits table\n",
    "# ------------------------------------------------------------\n",
    "def load_gffs_from_hits(hits_df, gff_dir):\n",
    "    \"\"\"\n",
    "    Parse GFF files only for genomes present in the hits dataframe.\n",
    "    GFF filenames are inferred as: <genome_file>_genomic/<genome_file>_genomic.gff\n",
    "    \n",
    "    Args:\n",
    "        hits_df: DataFrame containing hits with 'genome_file' column\n",
    "        gff_dir: Directory containing GFF files\n",
    "        \n",
    "    Returns:\n",
    "        Combined DataFrame of all parsed GFF files\n",
    "    \"\"\"\n",
    "\n",
    "    all_gff_rows = []\n",
    "    \n",
    "    unique_genomes = hits_df[\"Name\"].unique()\n",
    "    logging.info(f\"Loading GFFs for {len(unique_genomes)} unique genomes\")\n",
    "\n",
    "    for genome_id in unique_genomes:\n",
    "        gff_path = gff_dir / f\"{genome_id}_genomic\" / f\"{genome_id}_genomic.gff\"\n",
    "\n",
    "        if not gff_path.exists():\n",
    "            logging.warning(f\"Missing GFF: {gff_path}\")\n",
    "            continue\n",
    "\n",
    "        gff_df = parse_prodigal_gff(gff_path, genome_id)\n",
    "        all_gff_rows.append(gff_df)\n",
    "\n",
    "    if not all_gff_rows:\n",
    "        logging.error(\"No GFF files were successfully loaded.\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    combined_gff = pd.concat(all_gff_rows, ignore_index=True)\n",
    "    logging.info(f\"Total CDS features loaded: {len(combined_gff)}\")\n",
    "    \n",
    "    return combined_gff\n",
    "\n",
    "def compress_others(tokens, threshold=10):\n",
    "    \"\"\"\n",
    "    Compress long runs of 'other' into 'other[n]' if n > threshold.\n",
    "    \"\"\"\n",
    "    compressed = []\n",
    "    i = 0\n",
    "\n",
    "    while i < len(tokens):\n",
    "        if tokens[i] != \"other\":\n",
    "            compressed.append(tokens[i])\n",
    "            i += 1\n",
    "            continue\n",
    "\n",
    "        # count run\n",
    "        j = i\n",
    "        while j < len(tokens) and tokens[j] == \"other\":\n",
    "            j += 1\n",
    "\n",
    "        run_len = j - i\n",
    "\n",
    "        if run_len > threshold:\n",
    "            compressed.append(f\"other[{run_len}]\")\n",
    "        else:\n",
    "            compressed.extend([\"other\"] * run_len)\n",
    "\n",
    "        i = j\n",
    "\n",
    "    return compressed\n",
    "\n",
    "def extract_neighborhoods(anchor_df, gff_df, window=5):\n",
    "    \"\"\"\n",
    "    Extract genomic neighborhoods around ESCRT-related anchor genes.\n",
    "    \"\"\"\n",
    "    keywords = CORE_TARGETS\n",
    "\n",
    "    logging.info(f\"Starting neighborhood extraction ({window} genes)\")\n",
    "    logging.info(f\"Initial anchors: {len(anchor_df)}\")\n",
    "\n",
    "    # Filter anchors by keywords\n",
    "    filtered_anchors = anchor_df[\n",
    "        anchor_df[\"architecture\"]\n",
    "        .astype(str)\n",
    "        .str.lower()\n",
    "        .str.contains(\"|\".join(keywords))\n",
    "    ].copy()\n",
    "\n",
    "    logging.info(f\"Anchors after ESCRT filter: {len(filtered_anchors)}\")\n",
    "\n",
    "    if filtered_anchors.empty:\n",
    "        logging.error(\"No ESCRT-related anchors found\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    neighborhoods = []\n",
    "\n",
    "    # Group by genome and contig\n",
    "    for (genome, contig), anchors in filtered_anchors.groupby(\n",
    "        [\"genome_id\", \"contig\"]\n",
    "    ):\n",
    "        anchor_indices = anchors[\"gene_index\"].values\n",
    "        \n",
    "        # Define window boundaries\n",
    "        start = anchor_indices.min() - window\n",
    "        end = anchor_indices.max() + window\n",
    "\n",
    "        # Extract all genes in the window from gff_df\n",
    "        block = gff_df[\n",
    "            (gff_df[\"genome_id\"] == genome) &\n",
    "            (gff_df[\"contig\"] == contig) &\n",
    "            (gff_df[\"gene_index\"].between(start, end))\n",
    "        ].copy()\n",
    "\n",
    "        if block.empty:\n",
    "            continue\n",
    "\n",
    "        # Sort by gene position\n",
    "        block = block.sort_values(\"gene_index\").reset_index(drop=True)\n",
    "\n",
    "        # Create a mapping of protein_id to architecture from ALL anchors\n",
    "        # (not just filtered ones), so we get complete architecture info\n",
    "        arch_map = anchor_df.set_index(\"protein_id\")[\"architecture\"].to_dict()\n",
    "        \n",
    "        # Apply architecture: use anchor architecture if available, else \"other\"\n",
    "        block[\"architecture\"] = block[\"protein_id\"].map(arch_map).fillna(\"other\")\n",
    "\n",
    "        # Build architecture token list for the ENTIRE neighborhood\n",
    "        tokens = block[\"architecture\"].tolist()\n",
    "        compressed_tokens = compress_others(tokens, threshold=10)\n",
    "\n",
    "\n",
    "        # Add metadata about the neighborhood\n",
    "        block[\"num_anchors\"] = len(anchors)\n",
    "\n",
    "        block[\"window_start\"] = start\n",
    "        block[\"window_end\"] = end\n",
    "        block[\"neighborhood_architecture_compressed\"] = \",\".join(compressed_tokens)\n",
    "        block[\"position_in_neighborhood\"] = range(len(block))\n",
    "\n",
    "        neighborhoods.append(block)\n",
    "\n",
    "    if not neighborhoods:\n",
    "        logging.error(\"No neighborhoods extracted after processing\")\n",
    "        sys.exit(1)\n",
    "\n",
    "    combined = pd.concat(neighborhoods, ignore_index=True)\n",
    "\n",
    "    logging.info(\n",
    "        f\"Extracted {len(combined)} genes from \"\n",
    "        f\"{len(neighborhoods)} contig neighborhoods\"\n",
    "    )\n",
    "\n",
    "    return combined\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Load GTDB taxonomy\n",
    "# ------------------------------------------------------------\n",
    "def load_gtdb_taxonomy(gtdb_tsv):\n",
    "    \"\"\"\n",
    "    Load GTDB taxonomy file and split into rank-specific columns.\n",
    "    \n",
    "    GTDB format: genome_id\\td__Domain;p__Phylum;c__Class;o__Order;f__Family;g__Genus;s__Species\n",
    "    \n",
    "    Args:\n",
    "        gtdb_tsv: Path to GTDB taxonomy TSV file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame with columns: genome_id, domain, phylum, class, order, family, genus, species\n",
    "    \"\"\"\n",
    "    logging.info(f\"Loading GTDB taxonomy from: {gtdb_tsv}\")\n",
    "    \n",
    "    # Read the taxonomy file\n",
    "    tax_df = pd.read_csv(\n",
    "        gtdb_tsv,\n",
    "        sep=\"\\t\",\n",
    "        header=None,\n",
    "        names=[\"genome_id\", \"taxonomy\"],\n",
    "        dtype=str\n",
    "    )\n",
    "    \n",
    "    tax_df[\"genome_id_base\"] = tax_df[\"genome_id\"].str.split(\"_\", n=1).str[1]\n",
    "    logging.info(f\"Loaded taxonomy for {len(tax_df)} genomes\")\n",
    "    logging.info(f\"Sample taxonomy entry: {tax_df['taxonomy'].iloc[0]}\")\n",
    "    \n",
    "    # Split taxonomy string by semicolons\n",
    "    tax_split = tax_df[\"taxonomy\"].str.split(\";\", expand=True)\n",
    "    logging.info(f\"Taxonomy split into {tax_split.shape[1]} columns\")\n",
    "    \n",
    "    # Map column indices to taxonomic ranks\n",
    "    rank_map = {\n",
    "        0: \"domain\",\n",
    "        1: \"phylum\",\n",
    "        2: \"class\",\n",
    "        3: \"order\",\n",
    "        4: \"family\",\n",
    "        5: \"genus\",\n",
    "        6: \"species\"\n",
    "    }\n",
    "    \n",
    "    # Extract each rank and remove the prefix (e.g., \"d__\", \"p__\")\n",
    "    for idx, rank in rank_map.items():\n",
    "        if idx < tax_split.shape[1]:\n",
    "            # Remove the rank prefix using regex (e.g., \"p__\" from \"p__Crenarchaeota\")\n",
    "            tax_df[rank] = tax_split[idx].str.replace(r\"^[a-z]__\", \"\", regex=True)\n",
    "            logging.info(f\"Extracted {rank}: {tax_df[rank].notna().sum()} non-null values\")\n",
    "        else:\n",
    "            tax_df[rank] = pd.NA\n",
    "            logging.warning(f\"Column {idx} for rank '{rank}' not found in taxonomy data\")\n",
    "    \n",
    "    # Drop the original concatenated taxonomy string\n",
    "    tax_df.drop(columns=[\"taxonomy\"], inplace=True)\n",
    "    \n",
    "    # Log sample of parsed taxonomy\n",
    "    logging.info(f\"Sample parsed taxonomy:\")\n",
    "    logging.info(tax_df[[\"genome_id\", \"domain\", \"phylum\", \"class\"]].head().to_string())\n",
    "    \n",
    "    return tax_df\n",
    "\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# Merge taxonomy into neighborhood data\n",
    "# ------------------------------------------------------------\n",
    "def merge_taxonomy(escrt_csv, gtdb_tsv, out_csv):\n",
    "    \"\"\"\n",
    "    Merge GTDB taxonomy into ESCRT neighborhood dataframe.\n",
    "    \n",
    "    Args:\n",
    "        escrt_csv: Path to ESCRT neighborhoods CSV\n",
    "        gtdb_tsv: Path to GTDB taxonomy TSV\n",
    "        out_csv: Output path for merged CSV\n",
    "        \n",
    "    Returns:\n",
    "        Merged DataFrame with taxonomy columns added\n",
    "    \"\"\"\n",
    "    logging.info(\"=\" * 70)\n",
    "    logging.info(\"STARTING TAXONOMY MERGE\")\n",
    "    logging.info(\"=\" * 70)\n",
    "    \n",
    "    # Load neighborhood data\n",
    "    logging.info(f\"Loading neighborhood data from: {escrt_csv}\")\n",
    "    escrt_df = pd.read_csv(escrt_csv)\n",
    "    escrt_df['genome_id_base'] = (\n",
    "    escrt_df['genome_id']\n",
    "    .str.split(\"_\", n=2)\n",
    "    .str[0:2]\n",
    "    .str.join(\"_\")\n",
    ")\n",
    "\n",
    "    logging.info(f\"Neighborhood data: {len(escrt_df)} rows, {len(escrt_df['genome_id'].unique())} unique genomes\")\n",
    "    \n",
    "    # Load taxonomy data\n",
    "    tax_df = load_gtdb_taxonomy(gtdb_tsv)\n",
    "    \n",
    "    # Check for overlap between datasets\n",
    "    escrt_genomes = set(escrt_df[\"genome_id_base\"].unique())\n",
    "    tax_genomes = set(tax_df[\"genome_id_base\"].unique())\n",
    "    overlap = escrt_genomes.intersection(tax_genomes)\n",
    "    \n",
    "    logging.info(f\"Genome ID overlap check:\")\n",
    "    logging.info(f\"  Genomes in neighborhood data: {len(escrt_genomes)}\")\n",
    "    logging.info(f\"  Genomes in taxonomy data: {len(tax_genomes)}\")\n",
    "    logging.info(f\"  Overlapping genomes: {len(overlap)}\")\n",
    "    \n",
    "    if len(overlap) == 0:\n",
    "        logging.error(\"NO OVERLAP between genome IDs!\")\n",
    "        logging.error(f\"Sample neighborhood genome IDs: {list(escrt_genomes)[:5]}\")\n",
    "        logging.error(f\"Sample taxonomy genome IDs: {list(tax_genomes)[:5]}\")\n",
    "        logging.error(\"Check if genome_id formats match between files!\")\n",
    "        sys.exit(1)\n",
    "    \n",
    "    if len(overlap) < len(escrt_genomes):\n",
    "        missing = len(escrt_genomes) - len(overlap)\n",
    "        logging.warning(f\"{missing} genomes from neighborhood data not found in taxonomy!\")\n",
    "    \n",
    "    # Perform the merge\n",
    "    logging.info(\"Performing left join on genome_id...\")\n",
    "    merged = escrt_df.merge(\n",
    "        tax_df,\n",
    "        on=\"genome_id_base\",\n",
    "        how=\"left\"\n",
    "    )\n",
    "    \n",
    "    # Validate merge results\n",
    "    logging.info(f\"Merge complete: {len(merged)} rows\")\n",
    "    \n",
    "    # Check how many rows got taxonomy data\n",
    "    tax_columns = [\"domain\", \"phylum\", \"class\", \"order\", \"family\", \"genus\", \"species\"]\n",
    "    for col in tax_columns:\n",
    "        non_null = merged[col].notna().sum()\n",
    "        pct = (non_null / len(merged)) * 100\n",
    "        logging.info(f\"  {col}: {non_null} non-null ({pct:.1f}%)\")\n",
    "    \n",
    "    # Save to CSV\n",
    "    logging.info(f\"Saving merged data to: {out_csv}\")\n",
    "    merged.to_csv(out_csv, index=False)\n",
    "    \n",
    "    # Show sample of merged data\n",
    "    logging.info(\"Sample of merged data with taxonomy:\")\n",
    "    sample_cols = [\"genome_id\", \"protein_id\", \"domain\", \"phylum\", \"class\", \"order\"]\n",
    "    available_cols = [col for col in sample_cols if col in merged.columns]\n",
    "    logging.info(merged[available_cols].head(10).to_string())\n",
    "    \n",
    "    logging.info(\"=\" * 70)\n",
    "    logging.info(\"TAXONOMY MERGE COMPLETE\")\n",
    "    logging.info(\"=\" * 70)\n",
    "    \n",
    "    return merged\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 152,
   "id": "d774e625",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-16 20:11:38,724 [INFO] ======================================================================\n",
      "2026-01-16 20:11:38,725 [INFO] STARTING SYNTENY PIPELINE\n",
      "2026-01-16 20:11:38,725 [INFO] ======================================================================\n",
      "2026-01-16 20:11:38,726 [INFO] Hits DataFrame: (3325, 10)\n",
      "2026-01-16 20:11:38,726 [INFO] GFF directory: /home/anirudh/genomes/selected_genomes/prokka_results\n",
      "2026-01-16 20:11:38,727 [INFO] Window size: 5 genes\n",
      "2026-01-16 20:11:38,727 [INFO] \n",
      "[STEP 1] Loading GFF files...\n",
      "2026-01-16 20:11:38,728 [INFO] Loading GFFs for 129 unique genomes\n",
      "2026-01-16 20:11:38,728 [INFO] Parsing GFF: GCA_021498095.1_ASM2149809v1_genomic.gff\n",
      "2026-01-16 20:11:38,746 [INFO]    Parsed 3694 CDS features across 1 contigs\n",
      "2026-01-16 20:11:38,749 [INFO] Parsing GFF: GCA_005191425.1_ASM519142v1_genomic.gff\n",
      "2026-01-16 20:11:38,762 [INFO]    Parsed 3164 CDS features across 182 contigs\n",
      "2026-01-16 20:11:38,766 [INFO] Parsing GFF: GCA_048882285.1_ASM4888228v1_genomic.gff\n",
      "2026-01-16 20:11:38,779 [INFO]    Parsed 3583 CDS features across 321 contigs\n",
      "2026-01-16 20:11:38,783 [INFO] Parsing GFF: GCA_048882045.1_ASM4888204v1_genomic.gff\n",
      "2026-01-16 20:11:38,797 [INFO]    Parsed 3427 CDS features across 93 contigs\n",
      "2026-01-16 20:11:38,800 [INFO] Parsing GFF: GCA_048941715.1_ASM4894171v1_genomic.gff\n",
      "2026-01-16 20:11:38,810 [INFO]    Parsed 2417 CDS features across 120 contigs\n",
      "2026-01-16 20:11:38,813 [INFO] Parsing GFF: GCA_019058445.1_ASM1905844v1_genomic.gff\n",
      "2026-01-16 20:11:38,828 [INFO]    Parsed 4140 CDS features across 10 contigs\n",
      "2026-01-16 20:11:38,832 [INFO] Parsing GFF: GCA_038866325.1_ASM3886632v1_genomic.gff\n",
      "2026-01-16 20:11:38,842 [INFO]    Parsed 2379 CDS features across 27 contigs\n",
      "2026-01-16 20:11:38,844 [INFO] Parsing GFF: GCA_016839585.1_ASM1683958v1_genomic.gff\n",
      "2026-01-16 20:11:38,857 [INFO]    Parsed 3289 CDS features across 91 contigs\n",
      "2026-01-16 20:11:38,860 [INFO] Parsing GFF: GCA_052584815.1_ASM5258481v1_genomic.gff\n",
      "2026-01-16 20:11:38,871 [INFO]    Parsed 2573 CDS features across 105 contigs\n",
      "2026-01-16 20:11:38,874 [INFO] Parsing GFF: GCA_023705685.1_ASM2370568v1_genomic.gff\n",
      "2026-01-16 20:11:38,885 [INFO]    Parsed 2723 CDS features across 720 contigs\n",
      "2026-01-16 20:11:38,887 [INFO] Parsing GFF: GCA_030018135.1_ASM3001813v1_genomic.gff\n",
      "2026-01-16 20:11:38,892 [INFO]    Parsed 1290 CDS features across 42 contigs\n",
      "2026-01-16 20:11:38,894 [INFO] Parsing GFF: GCA_048941605.1_ASM4894160v1_genomic.gff\n",
      "2026-01-16 20:11:38,907 [INFO]    Parsed 3314 CDS features across 241 contigs\n",
      "2026-01-16 20:11:38,910 [INFO] Parsing GFF: GCA_019058165.1_ASM1905816v1_genomic.gff\n",
      "2026-01-16 20:11:38,923 [INFO]    Parsed 3102 CDS features across 253 contigs\n",
      "2026-01-16 20:11:38,926 [INFO] Parsing GFF: GCA_019057575.1_ASM1905757v1_genomic.gff\n",
      "2026-01-16 20:11:38,941 [INFO]    Parsed 3832 CDS features across 71 contigs\n",
      "2026-01-16 20:11:38,945 [INFO] Parsing GFF: GCA_030641055.1_ASM3064105v1_genomic.gff\n",
      "2026-01-16 20:11:38,954 [INFO]    Parsed 2083 CDS features across 271 contigs\n",
      "2026-01-16 20:11:38,956 [INFO] Parsing GFF: GCA_048941015.1_ASM4894101v1_genomic.gff\n",
      "2026-01-16 20:11:38,970 [INFO]    Parsed 3325 CDS features across 144 contigs\n",
      "2026-01-16 20:11:38,974 [INFO] Parsing GFF: GCA_029855935.1_ASM2985593v1_genomic.gff\n",
      "2026-01-16 20:11:38,985 [INFO]    Parsed 2731 CDS features across 32 contigs\n",
      "2026-01-16 20:11:38,988 [INFO] Parsing GFF: GCA_048885525.1_ASM4888552v1_genomic.gff\n",
      "2026-01-16 20:11:39,002 [INFO]    Parsed 3342 CDS features across 87 contigs\n",
      "2026-01-16 20:11:39,006 [INFO] Parsing GFF: GCA_038861695.1_ASM3886169v1_genomic.gff\n",
      "2026-01-16 20:11:39,015 [INFO]    Parsed 2148 CDS features across 20 contigs\n",
      "2026-01-16 20:11:39,018 [INFO] Parsing GFF: GCA_048882565.1_ASM4888256v1_genomic.gff\n",
      "2026-01-16 20:11:39,032 [INFO]    Parsed 3362 CDS features across 131 contigs\n",
      "2026-01-16 20:11:39,035 [INFO] Parsing GFF: GCA_019058095.1_ASM1905809v1_genomic.gff\n",
      "2026-01-16 20:11:39,050 [INFO]    Parsed 4094 CDS features across 174 contigs\n",
      "2026-01-16 20:11:39,054 [INFO] Parsing GFF: GCA_048885575.1_ASM4888557v1_genomic.gff\n",
      "2026-01-16 20:11:39,066 [INFO]    Parsed 2880 CDS features across 491 contigs\n",
      "2026-01-16 20:11:39,068 [INFO] Parsing GFF: GCA_018335335.1_ASM1833533v1_genomic.gff\n",
      "2026-01-16 20:11:39,080 [INFO]    Parsed 2695 CDS features across 176 contigs\n",
      "2026-01-16 20:11:39,083 [INFO] Parsing GFF: GCA_048882745.1_ASM4888274v1_genomic.gff\n",
      "2026-01-16 20:11:39,097 [INFO]    Parsed 3633 CDS features across 216 contigs\n",
      "2026-01-16 20:11:39,101 [INFO] Parsing GFF: GCA_019056805.1_ASM1905680v1_genomic.gff\n",
      "2026-01-16 20:11:39,109 [INFO]    Parsed 1888 CDS features across 18 contigs\n",
      "2026-01-16 20:11:39,111 [INFO] Parsing GFF: GCA_048885705.1_ASM4888570v1_genomic.gff\n",
      "2026-01-16 20:11:39,124 [INFO]    Parsed 3157 CDS features across 238 contigs\n",
      "2026-01-16 20:11:39,128 [INFO] Parsing GFF: GCA_046482545.1_ASM4648254v1_genomic.gff\n",
      "2026-01-16 20:11:39,139 [INFO]    Parsed 2899 CDS features across 45 contigs\n",
      "2026-01-16 20:11:39,142 [INFO] Parsing GFF: GCA_019057865.1_ASM1905786v1_genomic.gff\n",
      "2026-01-16 20:11:39,158 [INFO]    Parsed 4113 CDS features across 87 contigs\n",
      "2026-01-16 20:11:39,162 [INFO] Parsing GFF: GCA_048938295.1_ASM4893829v1_genomic.gff\n",
      "2026-01-16 20:11:39,173 [INFO]    Parsed 2472 CDS features across 66 contigs\n",
      "2026-01-16 20:11:39,176 [INFO] Parsing GFF: GCA_029856045.1_ASM2985604v1_genomic.gff\n",
      "2026-01-16 20:11:39,190 [INFO]    Parsed 3076 CDS features across 55 contigs\n",
      "2026-01-16 20:11:39,194 [INFO] Parsing GFF: GCA_016839765.1_ASM1683976v1_genomic.gff\n",
      "2026-01-16 20:11:39,207 [INFO]    Parsed 3131 CDS features across 96 contigs\n",
      "2026-01-16 20:11:39,210 [INFO] Parsing GFF: GCA_003144275.1_ASM314427v1_genomic.gff\n",
      "2026-01-16 20:11:39,225 [INFO]    Parsed 3520 CDS features across 103 contigs\n",
      "2026-01-16 20:11:39,228 [INFO] Parsing GFF: GCA_004524535.1_ASM452453v1_genomic.gff\n",
      "2026-01-16 20:11:39,242 [INFO]    Parsed 3464 CDS features across 198 contigs\n",
      "2026-01-16 20:11:39,246 [INFO] Parsing GFF: GCA_030587545.2_ASM3058754v2_genomic.gff\n",
      "2026-01-16 20:11:39,267 [INFO]    Parsed 5086 CDS features across 293 contigs\n",
      "2026-01-16 20:11:39,272 [INFO] Parsing GFF: GCA_030606485.1_ASM3060648v1_genomic.gff\n",
      "2026-01-16 20:11:39,286 [INFO]    Parsed 3504 CDS features across 13 contigs\n",
      "2026-01-16 20:11:39,289 [INFO] Parsing GFF: GCA_038826135.1_ASM3882613v1_genomic.gff\n",
      "2026-01-16 20:11:39,295 [INFO]    Parsed 1320 CDS features across 104 contigs\n",
      "2026-01-16 20:11:39,296 [INFO] Parsing GFF: GCA_013166775.1_ASM1316677v1_genomic.gff\n",
      "2026-01-16 20:11:39,311 [INFO]    Parsed 3642 CDS features across 115 contigs\n",
      "2026-01-16 20:11:39,314 [INFO] Parsing GFF: GCA_048939465.1_ASM4893946v1_genomic.gff\n",
      "2026-01-16 20:11:39,322 [INFO]    Parsed 1995 CDS features across 152 contigs\n",
      "2026-01-16 20:11:39,324 [INFO] Parsing GFF: GCA_051746095.1_ASM5174609v1_genomic.gff\n",
      "2026-01-16 20:11:39,338 [INFO]    Parsed 3416 CDS features across 200 contigs\n",
      "2026-01-16 20:11:39,341 [INFO] Parsing GFF: GCA_021498085.1_ASM2149808v1_genomic.gff\n",
      "2026-01-16 20:11:39,353 [INFO]    Parsed 2793 CDS features across 11 contigs\n",
      "2026-01-16 20:11:39,356 [INFO] Parsing GFF: GCA_048938825.1_ASM4893882v1_genomic.gff\n",
      "2026-01-16 20:11:39,367 [INFO]    Parsed 2966 CDS features across 460 contigs\n",
      "2026-01-16 20:11:39,370 [INFO] Parsing GFF: GCA_048938885.1_ASM4893888v1_genomic.gff\n",
      "2026-01-16 20:11:39,387 [INFO]    Parsed 4164 CDS features across 263 contigs\n",
      "2026-01-16 20:11:39,391 [INFO] Parsing GFF: GCA_048885535.1_ASM4888553v1_genomic.gff\n",
      "2026-01-16 20:11:39,403 [INFO]    Parsed 2919 CDS features across 112 contigs\n",
      "2026-01-16 20:11:39,405 [INFO] Parsing GFF: GCA_048939995.1_ASM4893999v1_genomic.gff\n",
      "2026-01-16 20:11:39,419 [INFO]    Parsed 3535 CDS features across 38 contigs\n",
      "2026-01-16 20:11:39,422 [INFO] Parsing GFF: GCA_016926735.1_ASM1692673v1_genomic.gff\n",
      "2026-01-16 20:11:39,436 [INFO]    Parsed 3506 CDS features across 238 contigs\n",
      "2026-01-16 20:11:39,439 [INFO] Parsing GFF: GCA_029856525.1_ASM2985652v1_genomic.gff\n",
      "2026-01-16 20:11:39,445 [INFO]    Parsed 1344 CDS features across 126 contigs\n",
      "2026-01-16 20:11:39,447 [INFO] Parsing GFF: GCA_048941465.1_ASM4894146v1_genomic.gff\n",
      "2026-01-16 20:11:39,459 [INFO]    Parsed 3186 CDS features across 124 contigs\n",
      "2026-01-16 20:11:39,462 [INFO] Parsing GFF: GCA_018238345.1_ASM1823834v1_genomic.gff\n",
      "2026-01-16 20:11:39,475 [INFO]    Parsed 3317 CDS features across 288 contigs\n",
      "2026-01-16 20:11:39,477 [INFO] Parsing GFF: GCA_038858525.1_ASM3885852v1_genomic.gff\n",
      "2026-01-16 20:11:39,487 [INFO]    Parsed 2145 CDS features across 63 contigs\n",
      "2026-01-16 20:11:39,489 [INFO] Parsing GFF: GCA_019058035.1_ASM1905803v1_genomic.gff\n",
      "2026-01-16 20:11:39,505 [INFO]    Parsed 3768 CDS features across 377 contigs\n",
      "2026-01-16 20:11:39,508 [INFO] Parsing GFF: GCA_049671145.1_ASM4967114v1_genomic.gff\n",
      "2026-01-16 20:11:39,522 [INFO]    Parsed 3522 CDS features across 346 contigs\n",
      "2026-01-16 20:11:39,526 [INFO] Parsing GFF: GCA_016839265.1_ASM1683926v1_genomic.gff\n",
      "2026-01-16 20:11:39,532 [INFO]    Parsed 1388 CDS features across 6 contigs\n",
      "2026-01-16 20:11:39,533 [INFO] Parsing GFF: GCA_038889235.1_ASM3888923v1_genomic.gff\n",
      "2026-01-16 20:11:39,542 [INFO]    Parsed 2119 CDS features across 8 contigs\n",
      "2026-01-16 20:11:39,545 [INFO] Parsing GFF: GCA_038854135.1_ASM3885413v1_genomic.gff\n",
      "2026-01-16 20:11:39,555 [INFO]    Parsed 2335 CDS features across 67 contigs\n",
      "2026-01-16 20:11:39,557 [INFO] Parsing GFF: GCA_046483985.1_ASM4648398v1_genomic.gff\n",
      "2026-01-16 20:11:39,574 [INFO]    Parsed 3746 CDS features across 127 contigs\n",
      "2026-01-16 20:11:39,577 [INFO] Parsing GFF: GCA_038855695.1_ASM3885569v1_genomic.gff\n",
      "2026-01-16 20:11:39,587 [INFO]    Parsed 2185 CDS features across 68 contigs\n",
      "2026-01-16 20:11:39,590 [INFO] Parsing GFF: GCA_048882325.1_ASM4888232v1_genomic.gff\n",
      "2026-01-16 20:11:39,606 [INFO]    Parsed 3400 CDS features across 89 contigs\n",
      "2026-01-16 20:11:39,609 [INFO] Parsing GFF: GCA_038846665.1_ASM3884666v1_genomic.gff\n",
      "2026-01-16 20:11:39,620 [INFO]    Parsed 2446 CDS features across 35 contigs\n",
      "2026-01-16 20:11:39,622 [INFO] Parsing GFF: GCA_029856545.1_ASM2985654v1_genomic.gff\n",
      "2026-01-16 20:11:39,633 [INFO]    Parsed 3115 CDS features across 318 contigs\n",
      "2026-01-16 20:11:39,637 [INFO] Parsing GFF: GCA_046497275.1_ASM4649727v1_genomic.gff\n",
      "2026-01-16 20:11:39,652 [INFO]    Parsed 3535 CDS features across 188 contigs\n",
      "2026-01-16 20:11:39,656 [INFO] Parsing GFF: GCA_021498125.1_ASM2149812v1_genomic.gff\n",
      "2026-01-16 20:11:39,671 [INFO]    Parsed 2866 CDS features across 14 contigs\n",
      "2026-01-16 20:11:39,676 [INFO] Parsing GFF: GCA_048939265.1_ASM4893926v1_genomic.gff\n",
      "2026-01-16 20:11:39,699 [INFO]    Parsed 3494 CDS features across 342 contigs\n",
      "2026-01-16 20:11:39,706 [INFO] Parsing GFF: GCA_048885325.1_ASM4888532v1_genomic.gff\n",
      "2026-01-16 20:11:39,724 [INFO]    Parsed 3117 CDS features across 320 contigs\n",
      "2026-01-16 20:11:39,730 [INFO] Parsing GFF: GCA_038819355.1_ASM3881935v1_genomic.gff\n",
      "2026-01-16 20:11:39,741 [INFO]    Parsed 2360 CDS features across 37 contigs\n",
      "2026-01-16 20:11:39,744 [INFO] Parsing GFF: GCA_016840425.1_ASM1684042v1_genomic.gff\n",
      "2026-01-16 20:11:39,753 [INFO]    Parsed 2100 CDS features across 277 contigs\n",
      "2026-01-16 20:11:39,755 [INFO] Parsing GFF: GCA_016839295.1_ASM1683929v1_genomic.gff\n",
      "2026-01-16 20:11:39,770 [INFO]    Parsed 3498 CDS features across 71 contigs\n",
      "2026-01-16 20:11:39,773 [INFO] Parsing GFF: GCA_030614005.1_ASM3061400v1_genomic.gff\n",
      "2026-01-16 20:11:39,790 [INFO]    Parsed 3303 CDS features across 126 contigs\n",
      "2026-01-16 20:11:39,793 [INFO] Parsing GFF: GCA_018238545.1_ASM1823854v1_genomic.gff\n",
      "2026-01-16 20:11:39,806 [INFO]    Parsed 3436 CDS features across 128 contigs\n",
      "2026-01-16 20:11:39,810 [INFO] Parsing GFF: GCA_048938985.1_ASM4893898v1_genomic.gff\n",
      "2026-01-16 20:11:39,824 [INFO]    Parsed 3442 CDS features across 530 contigs\n",
      "2026-01-16 20:11:39,828 [INFO] Parsing GFF: GCA_019058055.1_ASM1905805v1_genomic.gff\n",
      "2026-01-16 20:11:39,841 [INFO]    Parsed 2842 CDS features across 443 contigs\n",
      "2026-01-16 20:11:39,845 [INFO] Parsing GFF: GCA_027031765.1_ASM2703176v1_genomic.gff\n",
      "2026-01-16 20:11:39,856 [INFO]    Parsed 2805 CDS features across 229 contigs\n",
      "2026-01-16 20:11:39,859 [INFO] Parsing GFF: GCA_048886755.1_ASM4888675v1_genomic.gff\n",
      "2026-01-16 20:11:39,866 [INFO]    Parsed 1814 CDS features across 60 contigs\n",
      "2026-01-16 20:11:39,868 [INFO] Parsing GFF: GCA_030149205.1_ASM3014920v1_genomic.gff\n",
      "2026-01-16 20:11:39,880 [INFO]    Parsed 3000 CDS features across 12 contigs\n",
      "2026-01-16 20:11:39,883 [INFO] Parsing GFF: GCA_018238585.1_ASM1823858v1_genomic.gff\n",
      "2026-01-16 20:11:39,895 [INFO]    Parsed 3132 CDS features across 276 contigs\n",
      "2026-01-16 20:11:39,898 [INFO] Parsing GFF: GCA_003345555.1_ASM334555v1_genomic.gff\n",
      "2026-01-16 20:11:39,906 [INFO]    Parsed 2118 CDS features across 82 contigs\n",
      "2026-01-16 20:11:39,908 [INFO] Parsing GFF: GCA_013388835.1_ASM1338883v1_genomic.gff\n",
      "2026-01-16 20:11:39,920 [INFO]    Parsed 2761 CDS features across 123 contigs\n",
      "2026-01-16 20:11:39,922 [INFO] Parsing GFF: GCA_008080745.1_ASM808074v1_genomic.gff\n",
      "2026-01-16 20:11:39,934 [INFO]    Parsed 2874 CDS features across 19 contigs\n",
      "2026-01-16 20:11:39,937 [INFO] Parsing GFF: GCA_026993975.1_ASM2699397v1_genomic.gff\n",
      "2026-01-16 20:11:39,945 [INFO]    Parsed 2287 CDS features across 216 contigs\n",
      "2026-01-16 20:11:39,948 [INFO] Parsing GFF: GCA_021513695.1_ASM2151369v1_genomic.gff\n",
      "2026-01-16 20:11:39,959 [INFO]    Parsed 2671 CDS features across 1 contigs\n",
      "2026-01-16 20:11:39,962 [INFO] Parsing GFF: GCA_016840465.1_ASM1684046v1_genomic.gff\n",
      "2026-01-16 20:11:39,970 [INFO]    Parsed 2367 CDS features across 100 contigs\n",
      "2026-01-16 20:11:39,972 [INFO] Parsing GFF: GCA_019057635.1_ASM1905763v1_genomic.gff\n",
      "2026-01-16 20:11:39,985 [INFO]    Parsed 2957 CDS features across 214 contigs\n",
      "2026-01-16 20:11:39,987 [INFO] Parsing GFF: GCA_048941285.1_ASM4894128v1_genomic.gff\n",
      "2026-01-16 20:11:40,001 [INFO]    Parsed 3337 CDS features across 101 contigs\n",
      "2026-01-16 20:11:40,004 [INFO] Parsing GFF: GCA_021513715.1_ASM2151371v1_genomic.gff\n",
      "2026-01-16 20:11:40,016 [INFO]    Parsed 2842 CDS features across 1 contigs\n",
      "2026-01-16 20:11:40,020 [INFO] Parsing GFF: GCA_038897395.1_ASM3889739v1_genomic.gff\n",
      "2026-01-16 20:11:40,028 [INFO]    Parsed 2095 CDS features across 37 contigs\n",
      "2026-01-16 20:11:40,030 [INFO] Parsing GFF: GCA_038825615.1_ASM3882561v1_genomic.gff\n",
      "2026-01-16 20:11:40,044 [INFO]    Parsed 3096 CDS features across 125 contigs\n",
      "2026-01-16 20:11:40,047 [INFO] Parsing GFF: GCA_048885925.1_ASM4888592v1_genomic.gff\n",
      "2026-01-16 20:11:40,062 [INFO]    Parsed 3623 CDS features across 181 contigs\n",
      "2026-01-16 20:11:40,065 [INFO] Parsing GFF: GCA_048882505.1_ASM4888250v1_genomic.gff\n",
      "2026-01-16 20:11:40,081 [INFO]    Parsed 3757 CDS features across 83 contigs\n",
      "2026-01-16 20:11:40,084 [INFO] Parsing GFF: GCA_048882965.1_ASM4888296v1_genomic.gff\n",
      "2026-01-16 20:11:40,097 [INFO]    Parsed 3368 CDS features across 302 contigs\n",
      "2026-01-16 20:11:40,100 [INFO] Parsing GFF: GCA_014730275.1_ASM1473027v1_genomic.gff\n",
      "2026-01-16 20:11:40,114 [INFO]    Parsed 3226 CDS features across 151 contigs\n",
      "2026-01-16 20:11:40,117 [INFO] Parsing GFF: GCA_038824485.1_ASM3882448v1_genomic.gff\n",
      "2026-01-16 20:11:40,125 [INFO]    Parsed 2121 CDS features across 20 contigs\n",
      "2026-01-16 20:11:40,128 [INFO] Parsing GFF: GCA_029882335.1_ASM2988233v1_genomic.gff\n",
      "2026-01-16 20:11:40,143 [INFO]    Parsed 3851 CDS features across 634 contigs\n",
      "2026-01-16 20:11:40,147 [INFO] Parsing GFF: GCA_048882945.1_ASM4888294v1_genomic.gff\n",
      "2026-01-16 20:11:40,161 [INFO]    Parsed 3368 CDS features across 302 contigs\n",
      "2026-01-16 20:11:40,164 [INFO] Parsing GFF: GCA_027031745.1_ASM2703174v1_genomic.gff\n",
      "2026-01-16 20:11:40,177 [INFO]    Parsed 2697 CDS features across 20 contigs\n",
      "2026-01-16 20:11:40,180 [INFO] Parsing GFF: GCA_038822405.1_ASM3882240v1_genomic.gff\n",
      "2026-01-16 20:11:40,188 [INFO]    Parsed 2032 CDS features across 17 contigs\n",
      "2026-01-16 20:11:40,191 [INFO] Parsing GFF: GCA_048885565.1_ASM4888556v1_genomic.gff\n",
      "2026-01-16 20:11:40,203 [INFO]    Parsed 3452 CDS features across 316 contigs\n",
      "2026-01-16 20:11:40,207 [INFO] Parsing GFF: GCA_019894715.1_ASM1989471v1_genomic.gff\n",
      "2026-01-16 20:11:40,219 [INFO]    Parsed 3039 CDS features across 285 contigs\n",
      "2026-01-16 20:11:40,221 [INFO] Parsing GFF: GCA_019057815.1_ASM1905781v1_genomic.gff\n",
      "2026-01-16 20:11:40,235 [INFO]    Parsed 3269 CDS features across 48 contigs\n",
      "2026-01-16 20:11:40,238 [INFO] Parsing GFF: GCA_014730165.1_ASM1473016v1_genomic.gff\n",
      "2026-01-16 20:11:40,254 [INFO]    Parsed 4131 CDS features across 391 contigs\n",
      "2026-01-16 20:11:40,258 [INFO] Parsing GFF: GCA_029856435.1_ASM2985643v1_genomic.gff\n",
      "2026-01-16 20:11:40,271 [INFO]    Parsed 3257 CDS features across 180 contigs\n",
      "2026-01-16 20:11:40,273 [INFO] Parsing GFF: GCA_019057955.1_ASM1905795v1_genomic.gff\n",
      "2026-01-16 20:11:40,284 [INFO]    Parsed 2873 CDS features across 258 contigs\n",
      "2026-01-16 20:11:40,287 [INFO] Parsing GFF: GCA_038881135.1_ASM3888113v1_genomic.gff\n",
      "2026-01-16 20:11:40,297 [INFO]    Parsed 2366 CDS features across 34 contigs\n",
      "2026-01-16 20:11:40,299 [INFO] Parsing GFF: GCA_048887225.1_ASM4888722v1_genomic.gff\n",
      "2026-01-16 20:11:40,311 [INFO]    Parsed 3005 CDS features across 49 contigs\n",
      "2026-01-16 20:11:40,314 [INFO] Parsing GFF: GCA_029210805.1_ASM2921080v1_genomic.gff\n",
      "2026-01-16 20:11:40,329 [INFO]    Parsed 3958 CDS features across 675 contigs\n",
      "2026-01-16 20:11:40,332 [INFO] Parsing GFF: GCA_048887165.1_ASM4888716v1_genomic.gff\n",
      "2026-01-16 20:11:40,347 [INFO]    Parsed 3859 CDS features across 219 contigs\n",
      "2026-01-16 20:11:40,350 [INFO] Parsing GFF: GCA_038884015.1_ASM3888401v1_genomic.gff\n",
      "2026-01-16 20:11:40,360 [INFO]    Parsed 2542 CDS features across 26 contigs\n",
      "2026-01-16 20:11:40,363 [INFO] Parsing GFF: GCA_030667705.1_ASM3066770v1_genomic.gff\n",
      "2026-01-16 20:11:40,375 [INFO]    Parsed 3016 CDS features across 228 contigs\n",
      "2026-01-16 20:11:40,377 [INFO] Parsing GFF: GCA_023705985.1_ASM2370598v1_genomic.gff\n",
      "2026-01-16 20:11:40,391 [INFO]    Parsed 3383 CDS features across 378 contigs\n",
      "2026-01-16 20:11:40,394 [INFO] Parsing GFF: GCA_019057475.1_ASM1905747v1_genomic.gff\n",
      "2026-01-16 20:11:40,408 [INFO]    Parsed 3164 CDS features across 56 contigs\n",
      "2026-01-16 20:11:40,411 [INFO] Parsing GFF: GCA_965612995.1_SRR6823440_concoct_113_genomic.gff\n",
      "2026-01-16 20:11:40,422 [INFO]    Parsed 2836 CDS features across 119 contigs\n",
      "2026-01-16 20:11:40,425 [INFO] Parsing GFF: GCA_029856505.1_ASM2985650v1_genomic.gff\n",
      "2026-01-16 20:11:40,434 [INFO]    Parsed 2300 CDS features across 132 contigs\n",
      "2026-01-16 20:11:40,436 [INFO] Parsing GFF: GCA_016933055.1_ASM1693305v1_genomic.gff\n",
      "2026-01-16 20:11:40,451 [INFO]    Parsed 3322 CDS features across 36 contigs\n",
      "2026-01-16 20:11:40,454 [INFO] Parsing GFF: GCA_001940665.2_ASM194066v2_genomic.gff\n",
      "2026-01-16 20:11:40,460 [INFO]    Parsed 1464 CDS features across 1 contigs\n",
      "2026-01-16 20:11:40,462 [INFO] Parsing GFF: GCA_048940445.1_ASM4894044v1_genomic.gff\n",
      "2026-01-16 20:11:40,475 [INFO]    Parsed 3387 CDS features across 138 contigs\n",
      "2026-01-16 20:11:40,478 [INFO] Parsing GFF: GCA_038131005.1_139573_S174.scaffolds.fasta_genomic.gff\n",
      "2026-01-16 20:11:40,484 [INFO]    Parsed 1501 CDS features across 50 contigs\n",
      "2026-01-16 20:11:40,486 [INFO] Parsing GFF: GCA_937877725.1_SRR6823441_bin.193_CONCOCT_v1.1_MAG_genomic.gff\n",
      "2026-01-16 20:11:40,499 [INFO]    Parsed 3701 CDS features across 240 contigs\n",
      "2026-01-16 20:11:40,502 [INFO] Parsing GFF: GCA_019058135.1_ASM1905813v1_genomic.gff\n",
      "2026-01-16 20:11:40,515 [INFO]    Parsed 3523 CDS features across 547 contigs\n",
      "2026-01-16 20:11:40,518 [INFO] Parsing GFF: GCA_004376705.1_ASM437670v1_genomic.gff\n",
      "2026-01-16 20:11:40,527 [INFO]    Parsed 2529 CDS features across 307 contigs\n",
      "2026-01-16 20:11:40,529 [INFO] Parsing GFF: GCA_011364925.1_ASM1136492v1_genomic.gff\n",
      "2026-01-16 20:11:40,544 [INFO]    Parsed 3616 CDS features across 112 contigs\n",
      "2026-01-16 20:11:40,547 [INFO] Parsing GFF: GCA_048938025.1_ASM4893802v1_genomic.gff\n",
      "2026-01-16 20:11:40,559 [INFO]    Parsed 3277 CDS features across 88 contigs\n",
      "2026-01-16 20:11:40,562 [INFO] Parsing GFF: GCA_038820475.1_ASM3882047v1_genomic.gff\n",
      "2026-01-16 20:11:40,570 [INFO]    Parsed 2006 CDS features across 33 contigs\n",
      "2026-01-16 20:11:40,572 [INFO] Parsing GFF: GCA_030668875.1_ASM3066887v1_genomic.gff\n",
      "2026-01-16 20:11:40,584 [INFO]    Parsed 2972 CDS features across 99 contigs\n",
      "2026-01-16 20:11:40,587 [INFO] Parsing GFF: GCA_029856875.1_ASM2985687v1_genomic.gff\n",
      "2026-01-16 20:11:40,597 [INFO]    Parsed 2741 CDS features across 361 contigs\n",
      "2026-01-16 20:11:40,599 [INFO] Parsing GFF: GCA_030612005.1_ASM3061200v1_genomic.gff\n",
      "2026-01-16 20:11:40,613 [INFO]    Parsed 3441 CDS features across 270 contigs\n",
      "2026-01-16 20:11:40,615 [INFO] Parsing GFF: GCA_014730125.1_ASM1473012v1_genomic.gff\n",
      "2026-01-16 20:11:40,634 [INFO]    Parsed 4818 CDS features across 487 contigs\n",
      "2026-01-16 20:11:40,637 [INFO] Parsing GFF: GCA_048938455.1_ASM4893845v1_genomic.gff\n",
      "2026-01-16 20:11:40,654 [INFO]    Parsed 4144 CDS features across 90 contigs\n",
      "2026-01-16 20:11:40,657 [INFO] Parsing GFF: GCA_038840315.1_ASM3884031v1_genomic.gff\n",
      "2026-01-16 20:11:40,665 [INFO]    Parsed 2311 CDS features across 25 contigs\n",
      "2026-01-16 20:11:40,668 [INFO] Parsing GFF: GCA_015523565.1_ASM1552356v1_genomic.gff\n",
      "2026-01-16 20:11:40,680 [INFO]    Parsed 3158 CDS features across 233 contigs\n",
      "2026-01-16 20:11:40,683 [INFO] Parsing GFF: GCA_005223125.1_ASM522312v1_genomic.gff\n",
      "2026-01-16 20:11:40,698 [INFO]    Parsed 4010 CDS features across 71 contigs\n",
      "2026-01-16 20:11:40,701 [INFO] Parsing GFF: GCA_019058015.1_ASM1905801v1_genomic.gff\n",
      "2026-01-16 20:11:40,715 [INFO]    Parsed 3660 CDS features across 283 contigs\n",
      "2026-01-16 20:11:40,729 [INFO] Total CDS features loaded: 390025\n",
      "2026-01-16 20:11:41,162 [INFO] Saved GFF dataframe: /home/anirudh/synteny/hmms/ESCRT_synteny_pipeline_m3v2_family_2026-01-16-20-11-34/[STEP:5.5]gff_dataframe_2026-01-16-20-11-34.csv\n",
      "2026-01-16 20:11:41,162 [INFO] \n",
      "[STEP 2] Mapping hits to gene order...\n",
      "2026-01-16 20:11:41,196 [INFO] Mapped 3325 anchor hits to GFF entries\n",
      "2026-01-16 20:11:41,197 [INFO] Saving anchor hits...\n",
      "2026-01-16 20:11:41,210 [INFO] Saved: /home/anirudh/synteny/hmms/ESCRT_synteny_pipeline_m3v2_family_2026-01-16-20-11-34/[STEP:6]escrt_anchor_hits_2026-01-16-20-11-34.csv\n",
      "2026-01-16 20:11:41,211 [INFO] \n",
      "[STEP 3] Extracting contig architectures...\n",
      "2026-01-16 20:11:41,212 [INFO] Starting neighborhood extraction (5 genes)\n",
      "2026-01-16 20:11:41,212 [INFO] Initial anchors: 3325\n",
      "2026-01-16 20:11:41,216 [INFO] Anchors after ESCRT filter: 1470\n",
      "2026-01-16 20:12:04,507 [INFO] Extracted 28145 genes from 751 contig neighborhoods\n",
      "2026-01-16 20:12:04,515 [INFO] Extracted 28145 neighborhood gene rows\n",
      "2026-01-16 20:12:04,516 [INFO] ======================================================================\n",
      "2026-01-16 20:12:04,517 [INFO] SYNTENY PIPELINE COMPLETED SUCCESSFULLY\n",
      "2026-01-16 20:12:04,517 [INFO] ======================================================================\n",
      "2026-01-16 20:12:04,520 [INFO] \n",
      "======================================================================\n",
      "2026-01-16 20:12:04,520 [INFO] PIPELINE OUTPUT SUMMARY\n",
      "2026-01-16 20:12:04,523 [INFO] ======================================================================\n",
      "2026-01-16 20:12:04,523 [INFO] \n",
      "Anchor gene architecture distribution:\n",
      "2026-01-16 20:12:04,524 [INFO] architecture\n",
      "MPN             662\n",
      "Vps4            497\n",
      "E1              415\n",
      "Ub-like         282\n",
      "EAP30           280\n",
      "ESCRTIII        245\n",
      "E2              212\n",
      "VPS25           119\n",
      "Vps28           113\n",
      "BRO1             70\n",
      "E2-VPS23         69\n",
      "PH-VPS23         51\n",
      "E3-HEAT          47\n",
      "CC-VPS23         45\n",
      "E3-UFM           42\n",
      "E2-E3            37\n",
      "URM1             36\n",
      "FlaD             20\n",
      "E3               20\n",
      "E3-dom           18\n",
      "BAR-VPS28        17\n",
      "Zn-PH            10\n",
      "VPS28-longin      6\n",
      "VPS23             5\n",
      "katanin           3\n",
      "Ub-dom            2\n",
      "E3-PCI            1\n",
      "MPN-E3-dom        1\n",
      "2026-01-16 20:12:04,526 [INFO] \n",
      "First few neighborhood entries:\n",
      "2026-01-16 20:12:04,527 [INFO]                      genome_id      contig  gene_index      protein_id   start     end strand architecture  num_anchors  window_start  window_end                                                                                                                                                                                                    neighborhood_architecture_compressed  position_in_neighborhood\n",
      "0  GCA_001940665.2_ASM194066v2  CP091871.1         136  NDAMOAGK_00136  139607  140479      +        other           11           136        1104  other,other,other,other,other,E2-VPS23,other[30],Vps4,other[42],MPN,other[126],Vps4,other[309],E1,other[111],Vps4,other[323],E1,E2,Ub-like,other,other,E2-VPS23,Vps28,EAP30,VPS25,Vps4,ESCRTIII,ESCRTIII,other,other,other,other,other                         0\n",
      "1  GCA_001940665.2_ASM194066v2  CP091871.1         137  NDAMOAGK_00137  140483  141823      +        other           11           136        1104  other,other,other,other,other,E2-VPS23,other[30],Vps4,other[42],MPN,other[126],Vps4,other[309],E1,other[111],Vps4,other[323],E1,E2,Ub-like,other,other,E2-VPS23,Vps28,EAP30,VPS25,Vps4,ESCRTIII,ESCRTIII,other,other,other,other,other                         1\n",
      "2  GCA_001940665.2_ASM194066v2  CP091871.1         138  NDAMOAGK_00138  141813  143192      +        other           11           136        1104  other,other,other,other,other,E2-VPS23,other[30],Vps4,other[42],MPN,other[126],Vps4,other[309],E1,other[111],Vps4,other[323],E1,E2,Ub-like,other,other,E2-VPS23,Vps28,EAP30,VPS25,Vps4,ESCRTIII,ESCRTIII,other,other,other,other,other                         2\n",
      "3  GCA_001940665.2_ASM194066v2  CP091871.1         139  NDAMOAGK_00139  143217  144101      +        other           11           136        1104  other,other,other,other,other,E2-VPS23,other[30],Vps4,other[42],MPN,other[126],Vps4,other[309],E1,other[111],Vps4,other[323],E1,E2,Ub-like,other,other,E2-VPS23,Vps28,EAP30,VPS25,Vps4,ESCRTIII,ESCRTIII,other,other,other,other,other                         3\n",
      "4  GCA_001940665.2_ASM194066v2  CP091871.1         140  NDAMOAGK_00140  144124  145431      -        other           11           136        1104  other,other,other,other,other,E2-VPS23,other[30],Vps4,other[42],MPN,other[126],Vps4,other[309],E1,other[111],Vps4,other[323],E1,E2,Ub-like,other,other,E2-VPS23,Vps28,EAP30,VPS25,Vps4,ESCRTIII,ESCRTIII,other,other,other,other,other                         4\n",
      "2026-01-16 20:12:04,529 [INFO] \n",
      "Genome distribution:\n",
      "2026-01-16 20:12:04,530 [INFO] genome_id\n",
      "GCA_021498095.1_ASM2149809v1                           3116\n",
      "GCA_021513715.1_ASM2151371v1                           2222\n",
      "GCA_021513695.1_ASM2151369v1                           2138\n",
      "GCA_021498125.1_ASM2149812v1                           1280\n",
      "GCA_019058445.1_ASM1905844v1                           1101\n",
      "GCA_001940665.2_ASM194066v2                             969\n",
      "GCA_008080745.1_ASM808074v1                             947\n",
      "GCA_016839265.1_ASM1683926v1                            934\n",
      "GCA_027031745.1_ASM2703174v1                            869\n",
      "GCA_030606485.1_ASM3060648v1                            679\n",
      "GCA_030149205.1_ASM3014920v1                            587\n",
      "GCA_038889235.1_ASM3888923v1                            587\n",
      "GCA_016933055.1_ASM1693305v1                            525\n",
      "GCA_021498085.1_ASM2149808v1                            449\n",
      "GCA_016839295.1_ASM1683929v1                            387\n",
      "GCA_038846665.1_ASM3884666v1                            349\n",
      "GCA_038840315.1_ASM3884031v1                            347\n",
      "GCA_038861695.1_ASM3886169v1                            346\n",
      "GCA_038866325.1_ASM3886632v1                            346\n",
      "GCA_046482545.1_ASM4648254v1                            283\n",
      "GCA_038825615.1_ASM3882561v1                            248\n",
      "GCA_029856045.1_ASM2985604v1                            240\n",
      "GCA_019056805.1_ASM1905680v1                            229\n",
      "GCA_048887225.1_ASM4888722v1                            226\n",
      "GCA_048939995.1_ASM4893999v1                            202\n",
      "GCA_003144275.1_ASM314427v1                             189\n",
      "GCA_048938455.1_ASM4893845v1                            186\n",
      "GCA_013166775.1_ASM1316677v1                            160\n",
      "GCA_016839765.1_ASM1683976v1                            158\n",
      "GCA_019057635.1_ASM1905763v1                            157\n",
      "GCA_029855935.1_ASM2985593v1                            157\n",
      "GCA_030587545.2_ASM3058754v2                            149\n",
      "GCA_019057475.1_ASM1905747v1                            142\n",
      "GCA_052584815.1_ASM5258481v1                            138\n",
      "GCA_048882745.1_ASM4888274v1                            124\n",
      "GCA_048941015.1_ASM4894101v1                            123\n",
      "GCA_048887165.1_ASM4888716v1                            123\n",
      "GCA_046483985.1_ASM4648398v1                            117\n",
      "GCA_038884015.1_ASM3888401v1                            115\n",
      "GCA_030614005.1_ASM3061400v1                            115\n",
      "GCA_038881135.1_ASM3888113v1                            114\n",
      "GCA_019057815.1_ASM1905781v1                            113\n",
      "GCA_038819355.1_ASM3881935v1                            112\n",
      "GCA_038824485.1_ASM3882448v1                            112\n",
      "GCA_038897395.1_ASM3889739v1                            111\n",
      "GCA_038820475.1_ASM3882047v1                            109\n",
      "GCA_048882325.1_ASM4888232v1                            108\n",
      "GCA_038854135.1_ASM3885413v1                            106\n",
      "GCA_018238545.1_ASM1823854v1                            105\n",
      "GCA_003345555.1_ASM334555v1                             105\n",
      "GCA_005191425.1_ASM519142v1                             105\n",
      "GCA_048882945.1_ASM4888294v1                            104\n",
      "GCA_048882965.1_ASM4888296v1                            104\n",
      "GCA_048938885.1_ASM4893888v1                            104\n",
      "GCA_048882505.1_ASM4888250v1                            102\n",
      "GCA_038822405.1_ASM3882240v1                            102\n",
      "GCA_048882565.1_ASM4888256v1                            101\n",
      "GCA_018238585.1_ASM1823858v1                             99\n",
      "GCA_038855695.1_ASM3885569v1                             95\n",
      "GCA_014730275.1_ASM1473027v1                             93\n",
      "GCA_049671145.1_ASM4967114v1                             88\n",
      "GCA_019057865.1_ASM1905786v1                             88\n",
      "GCA_019058165.1_ASM1905816v1                             88\n",
      "GCA_013388835.1_ASM1338883v1                             86\n",
      "GCA_048885925.1_ASM4888592v1                             85\n",
      "GCA_048882045.1_ASM4888204v1                             84\n",
      "GCA_048939265.1_ASM4893926v1                             84\n",
      "GCA_048941465.1_ASM4894146v1                             84\n",
      "GCA_014730165.1_ASM1473016v1                             84\n",
      "GCA_029856435.1_ASM2985643v1                             84\n",
      "GCA_018335335.1_ASM1833533v1                             84\n",
      "GCA_030612005.1_ASM3061200v1                             82\n",
      "GCA_019057575.1_ASM1905757v1                             80\n",
      "GCA_048938025.1_ASM4893802v1                             79\n",
      "GCA_016839585.1_ASM1683958v1                             77\n",
      "GCA_014730125.1_ASM1473012v1                             77\n",
      "GCA_965612995.1_SRR6823440_concoct_113                   77\n",
      "GCA_019057955.1_ASM1905795v1                             77\n",
      "GCA_015523565.1_ASM1552356v1                             76\n",
      "GCA_016926735.1_ASM1692673v1                             76\n",
      "GCA_029856505.1_ASM2985650v1                             75\n",
      "GCA_048885565.1_ASM4888556v1                             74\n",
      "GCA_048885535.1_ASM4888553v1                             74\n",
      "GCA_048885525.1_ASM4888552v1                             73\n",
      "GCA_030667705.1_ASM3066770v1                             71\n",
      "GCA_005223125.1_ASM522312v1                              71\n",
      "GCA_019894715.1_ASM1989471v1                             70\n",
      "GCA_048941715.1_ASM4894171v1                             70\n",
      "GCA_048941605.1_ASM4894160v1                             70\n",
      "GCA_019058095.1_ASM1905809v1                             69\n",
      "GCA_023705985.1_ASM2370598v1                             68\n",
      "GCA_046497275.1_ASM4649727v1                             68\n",
      "GCA_038131005.1_139573_S174.scaffolds.fasta              68\n",
      "GCA_018238345.1_ASM1823834v1                             67\n",
      "GCA_019058015.1_ASM1905801v1                             66\n",
      "GCA_048938295.1_ASM4893829v1                             66\n",
      "GCA_048938825.1_ASM4893882v1                             64\n",
      "GCA_029210805.1_ASM2921080v1                             64\n",
      "GCA_004376705.1_ASM437670v1                              63\n",
      "GCA_048941285.1_ASM4894128v1                             60\n",
      "GCA_048938985.1_ASM4893898v1                             60\n",
      "GCA_048940445.1_ASM4894044v1                             60\n",
      "GCA_051746095.1_ASM5174609v1                             58\n",
      "GCA_004524535.1_ASM452453v1                              58\n",
      "GCA_030668875.1_ASM3066887v1                             57\n",
      "GCA_011364925.1_ASM1136492v1                             56\n",
      "GCA_019058035.1_ASM1905803v1                             56\n",
      "GCA_027031765.1_ASM2703176v1                             54\n",
      "GCA_048882285.1_ASM4888228v1                             54\n",
      "GCA_038858525.1_ASM3885852v1                             53\n",
      "GCA_048886755.1_ASM4888675v1                             53\n",
      "GCA_029882335.1_ASM2988233v1                             51\n",
      "GCA_048885705.1_ASM4888570v1                             51\n",
      "GCA_048885325.1_ASM4888532v1                             50\n",
      "GCA_026993975.1_ASM2699397v1                             48\n",
      "GCA_048939465.1_ASM4893946v1                             46\n",
      "GCA_937877725.1_SRR6823441_bin.193_CONCOCT_v1.1_MAG      46\n",
      "GCA_029856545.1_ASM2985654v1                             46\n",
      "GCA_019058135.1_ASM1905813v1                             44\n",
      "GCA_030641055.1_ASM3064105v1                             40\n",
      "GCA_023705685.1_ASM2370568v1                             34\n",
      "GCA_048885575.1_ASM4888557v1                             33\n",
      "GCA_016840425.1_ASM1684042v1                             32\n",
      "GCA_016840465.1_ASM1684046v1                             30\n",
      "GCA_029856525.1_ASM2985652v1                             27\n",
      "GCA_019058055.1_ASM1905805v1                             26\n",
      "GCA_029856875.1_ASM2985687v1                             24\n",
      "GCA_038826135.1_ASM3882613v1                             24\n",
      "2026-01-16 20:12:04,530 [INFO] \n",
      "Saving neighborhood data...\n",
      "2026-01-16 20:12:04,615 [INFO] Saved: /home/anirudh/synteny/hmms/ESCRT_synteny_pipeline_m3v2_family_2026-01-16-20-11-34/[STEP:7]escrt_neighborhoods_2026-01-16-20-11-34.csv\n",
      "2026-01-16 20:12:04,616 [INFO] \n",
      "======================================================================\n",
      "2026-01-16 20:12:04,616 [INFO] ADDING TAXONOMY INFORMATION\n",
      "2026-01-16 20:12:04,617 [INFO] ======================================================================\n",
      "2026-01-16 20:12:04,617 [INFO] ======================================================================\n",
      "2026-01-16 20:12:04,617 [INFO] STARTING TAXONOMY MERGE\n",
      "2026-01-16 20:12:04,618 [INFO] ======================================================================\n",
      "2026-01-16 20:12:04,618 [INFO] Loading neighborhood data from: /home/anirudh/synteny/hmms/ESCRT_synteny_pipeline_m3v2_family_2026-01-16-20-11-34/[STEP:7]escrt_neighborhoods_2026-01-16-20-11-34.csv\n",
      "2026-01-16 20:12:04,658 [INFO] Neighborhood data: 28145 rows, 128 unique genomes\n",
      "2026-01-16 20:12:04,659 [INFO] Loading GTDB taxonomy from: /home/anirudh/synteny/ar53_taxonomy_r226.tsv\n",
      "2026-01-16 20:12:04,678 [INFO] Loaded taxonomy for 17245 genomes\n",
      "2026-01-16 20:12:04,679 [INFO] Sample taxonomy entry: d__Archaea;p__Methanobacteriota;c__Methanobacteria;o__Methanobacteriales;f__Methanobacteriaceae;g__Methanocatella;s__Methanocatella smithii\n",
      "2026-01-16 20:12:04,692 [INFO] Taxonomy split into 7 columns\n",
      "2026-01-16 20:12:04,697 [INFO] Extracted domain: 17245 non-null values\n",
      "2026-01-16 20:12:04,703 [INFO] Extracted phylum: 17245 non-null values\n",
      "2026-01-16 20:12:04,708 [INFO] Extracted class: 17245 non-null values\n",
      "2026-01-16 20:12:04,713 [INFO] Extracted order: 17245 non-null values\n",
      "2026-01-16 20:12:04,719 [INFO] Extracted family: 17245 non-null values\n",
      "2026-01-16 20:12:04,724 [INFO] Extracted genus: 17245 non-null values\n",
      "2026-01-16 20:12:04,729 [INFO] Extracted species: 17245 non-null values\n",
      "2026-01-16 20:12:04,731 [INFO] Sample parsed taxonomy:\n",
      "2026-01-16 20:12:04,734 [INFO]             genome_id   domain             phylum            class\n",
      "0  RS_GCF_945873965.1  Archaea  Methanobacteriota  Methanobacteria\n",
      "1  GB_GCA_963642515.1  Archaea  Methanobacteriota  Methanobacteria\n",
      "2  GB_GCA_023453475.1  Archaea  Methanobacteriota  Methanobacteria\n",
      "3  RS_GCF_000189895.1  Archaea  Methanobacteriota  Methanobacteria\n",
      "4  RS_GCF_000189795.1  Archaea  Methanobacteriota  Methanobacteria\n",
      "2026-01-16 20:12:04,738 [INFO] Genome ID overlap check:\n",
      "2026-01-16 20:12:04,739 [INFO]   Genomes in neighborhood data: 128\n",
      "2026-01-16 20:12:04,739 [INFO]   Genomes in taxonomy data: 17245\n",
      "2026-01-16 20:12:04,740 [INFO]   Overlapping genomes: 87\n",
      "2026-01-16 20:12:04,740 [WARNING] 41 genomes from neighborhood data not found in taxonomy!\n",
      "2026-01-16 20:12:04,740 [INFO] Performing left join on genome_id...\n",
      "2026-01-16 20:12:04,752 [INFO] Merge complete: 28145 rows\n",
      "2026-01-16 20:12:04,753 [INFO]   domain: 24301 non-null (86.3%)\n",
      "2026-01-16 20:12:04,754 [INFO]   phylum: 24301 non-null (86.3%)\n",
      "2026-01-16 20:12:04,756 [INFO]   class: 24301 non-null (86.3%)\n",
      "2026-01-16 20:12:04,757 [INFO]   order: 24301 non-null (86.3%)\n",
      "2026-01-16 20:12:04,757 [INFO]   family: 24301 non-null (86.3%)\n",
      "2026-01-16 20:12:04,758 [INFO]   genus: 24301 non-null (86.3%)\n",
      "2026-01-16 20:12:04,759 [INFO]   species: 24301 non-null (86.3%)\n",
      "2026-01-16 20:12:04,760 [INFO] Saving merged data to: /home/anirudh/synteny/hmms/ESCRT_synteny_pipeline_m3v2_family_2026-01-16-20-11-34/[STEP:8]escrt_neighborhoods_with_taxonomy_2026-01-16-20-11-34.csv\n",
      "2026-01-16 20:12:04,876 [INFO] Sample of merged data with taxonomy:\n",
      "2026-01-16 20:12:04,879 [INFO]        protein_id   domain           phylum         class           order\n",
      "0  NDAMOAGK_00136  Archaea  Asgardarchaeota  Odinarchaeia  Odinarchaeales\n",
      "1  NDAMOAGK_00137  Archaea  Asgardarchaeota  Odinarchaeia  Odinarchaeales\n",
      "2  NDAMOAGK_00138  Archaea  Asgardarchaeota  Odinarchaeia  Odinarchaeales\n",
      "3  NDAMOAGK_00139  Archaea  Asgardarchaeota  Odinarchaeia  Odinarchaeales\n",
      "4  NDAMOAGK_00140  Archaea  Asgardarchaeota  Odinarchaeia  Odinarchaeales\n",
      "5  NDAMOAGK_00141  Archaea  Asgardarchaeota  Odinarchaeia  Odinarchaeales\n",
      "6  NDAMOAGK_00142  Archaea  Asgardarchaeota  Odinarchaeia  Odinarchaeales\n",
      "7  NDAMOAGK_00143  Archaea  Asgardarchaeota  Odinarchaeia  Odinarchaeales\n",
      "8  NDAMOAGK_00144  Archaea  Asgardarchaeota  Odinarchaeia  Odinarchaeales\n",
      "9  NDAMOAGK_00145  Archaea  Asgardarchaeota  Odinarchaeia  Odinarchaeales\n",
      "2026-01-16 20:12:04,880 [INFO] ======================================================================\n",
      "2026-01-16 20:12:04,880 [INFO] TAXONOMY MERGE COMPLETE\n",
      "2026-01-16 20:12:04,880 [INFO] ======================================================================\n",
      "2026-01-16 20:12:04,885 [INFO] Saved taxonomy-annotated neighborhoods: /home/anirudh/synteny/hmms/ESCRT_synteny_pipeline_m3v2_family_2026-01-16-20-11-34/[STEP:8]escrt_neighborhoods_with_taxonomy_2026-01-16-20-11-34.csv\n",
      "2026-01-16 20:12:04,885 [INFO] \n",
      "Taxonomic diversity in results:\n",
      "2026-01-16 20:12:04,886 [INFO] Unique phyla: 1\n",
      "2026-01-16 20:12:04,887 [INFO] Unique classes: 8\n",
      "2026-01-16 20:12:04,888 [INFO] Unique orders: 11\n",
      "2026-01-16 20:12:04,888 [INFO] \n",
      "Sample genomes with taxonomy:\n",
      "2026-01-16 20:12:04,895 [INFO]        genome_id_base           phylum             class               order\n",
      "0     GCA_001940665.2  Asgardarchaeota      Odinarchaeia      Odinarchaeales\n",
      "969   GCA_003144275.1  Asgardarchaeota  Heimdallarchaeia       Hodarchaeales\n",
      "1158  GCA_003345555.1  Asgardarchaeota      Thorarchaeia      Thorarchaeales\n",
      "1263  GCA_004376705.1  Asgardarchaeota      Lokiarchaeia     Sigynarchaeales\n",
      "1326  GCA_004524535.1  Asgardarchaeota      Lokiarchaeia     Sigynarchaeales\n",
      "1384  GCA_005191425.1  Asgardarchaeota      Lokiarchaeia       Helarchaeales\n",
      "1489  GCA_005223125.1  Asgardarchaeota      Lokiarchaeia     Sigynarchaeales\n",
      "1560  GCA_008080745.1  Asgardarchaeota      Thorarchaeia      Thorarchaeales\n",
      "2507  GCA_011364925.1  Asgardarchaeota      Lokiarchaeia     Sigynarchaeales\n",
      "2563  GCA_013166775.1  Asgardarchaeota  Heimdallarchaeia  Heimdallarchaeales\n",
      "2026-01-16 20:12:04,900 [INFO] \n",
      "Contig architecture summary (first 10):\n",
      "2026-01-16 20:12:04,901 [INFO]                    genome_id_x          contig                                                                                                                                                                                                    neighborhood_architecture_compressed\n",
      "0  GCA_001940665.2_ASM194066v2      CP091871.1  other,other,other,other,other,E2-VPS23,other[30],Vps4,other[42],MPN,other[126],Vps4,other[309],E1,other[111],Vps4,other[323],E1,E2,Ub-like,other,other,E2-VPS23,Vps28,EAP30,VPS25,Vps4,ESCRTIII,ESCRTIII,other,other,other,other,other\n",
      "1  GCA_003144275.1_ASM314427v1  NJBF01000001.1                                                                                                            Ub-like,E1,other,E3-dom,other,EAP30,other[101],Vps4,other,other,other,other,MPN,other[20],Vps4,other,other,other,other,other\n",
      "2  GCA_003144275.1_ASM314427v1  NJBF01000006.1                                                                                                                                                                       other,other,other,other,other,EAP30,other,other,other,other,other\n",
      "3  GCA_003144275.1_ASM314427v1  NJBF01000007.1                                                                                                                                                                                    other,other,other,other,other,Vps4,other,other,other\n",
      "4  GCA_003144275.1_ASM314427v1  NJBF01000010.1                                                                                                                other,Ub-like,E1,other,other,E2-VPS23,Vps28,other,other,EAP30,ESCRTIII,ESCRTIII,Vps4,VPS25,other,other,other,other,other\n",
      "5  GCA_003144275.1_ASM314427v1  NJBF01000024.1                                                                                                                                                                        other,other,other,other,other,Vps4,other,other,other,other,other\n",
      "6  GCA_003345555.1_ASM334555v1  PRDJ01000006.1                                                                                                                                                                         other,other,other,other,other,BAR-VPS28,other,other,other,other\n",
      "7  GCA_003345555.1_ASM334555v1  PRDJ01000010.1                                                                                                                                                                        other,other,other,other,other,Vps4,other,other,other,other,other\n",
      "8  GCA_003345555.1_ASM334555v1  PRDJ01000039.1                                                                                                                                                    other,other,other,other,other,ESCRTIII,other[35],EAP30,other,other,other,other,other\n",
      "9  GCA_003345555.1_ASM334555v1  PRDJ01000042.1                                                                                                                                          other,other,other,other,other,EAP30,VPS25,Vps4,ESCRTIII,CC-VPS23,other,other,other,other,other\n",
      "2026-01-16 20:12:04,903 [INFO] \n",
      "======================================================================\n",
      "2026-01-16 20:12:04,904 [INFO] ALL PROCESSING COMPLETE\n",
      "2026-01-16 20:12:04,904 [INFO] ======================================================================\n"
     ]
    }
   ],
   "source": [
    "from pathlib import Path\n",
    "import os\n",
    "import sys\n",
    "import logging\n",
    "\n",
    "# ------------------------------------------------------------\n",
    "# SYNTENY PIPELINE (NOTEBOOK / LINEAR EXECUTION)\n",
    "# ------------------------------------------------------------\n",
    "\n",
    "logging.info(\"=\" * 70)\n",
    "logging.info(\"STARTING SYNTENY PIPELINE\")\n",
    "logging.info(\"=\" * 70)\n",
    "\n",
    "logging.info(f\"Hits DataFrame: {df_with_arch.shape}\")\n",
    "logging.info(f\"GFF directory: /home/anirudh/genomes/selected_genomes/prokka_results\")\n",
    "logging.info(f\"Window size: {WINDOW} genes\")\n",
    "\n",
    "gff_dir = Path(\"/home/anirudh/genomes/selected_genomes/prokka_results\")\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# STEP 1: Load GFFs\n",
    "# --------------------------------------------------------\n",
    "logging.info(\"\\n[STEP 1] Loading GFF files...\")\n",
    "\n",
    "gff_df = load_gffs_from_hits(df_with_arch, gff_dir)\n",
    "\n",
    "\n",
    "gff_df_file = os.path.join(\n",
    "    MAIN_OUTDIR,\n",
    "    f\"[STEP:{STEP}.5]gff_dataframe_{rightnow}.csv\"\n",
    ")\n",
    "gff_df.to_csv(gff_df_file, index=False)\n",
    "logging.info(\"Saved GFF dataframe: %s\", gff_df_file)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# STEP 2: Map anchor hits to gene order\n",
    "# --------------------------------------------------------\n",
    "logging.info(\"\\n[STEP 2] Mapping hits to gene order...\")\n",
    "\n",
    "anchor_df = df_with_arch.merge(\n",
    "    gff_df,\n",
    "    left_on=\"target\",\n",
    "    right_on=\"protein_id\",\n",
    "    how=\"inner\"\n",
    ")\n",
    "\n",
    "if anchor_df.empty:\n",
    "    logging.error(\"No hits could be mapped to GFFs (protein ID mismatch?)\")\n",
    "    sys.exit(1)\n",
    "\n",
    "logging.info(\"Mapped %d anchor hits to GFF entries\", len(anchor_df))\n",
    "\n",
    "logging.info(\"Saving anchor hits...\")\n",
    "STEP += 1\n",
    "\n",
    "anchor_hits_file = os.path.join(\n",
    "    MAIN_OUTDIR,\n",
    "    f\"[STEP:{STEP}]escrt_anchor_hits_{rightnow}.csv\"\n",
    ")\n",
    "anchor_df.to_csv(anchor_hits_file, index=False)\n",
    "logging.info(\"Saved: %s\", anchor_hits_file)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# STEP 3: Extract contig architectures / neighborhoods\n",
    "# --------------------------------------------------------\n",
    "logging.info(\"\\n[STEP 3] Extracting contig architectures...\")\n",
    "\n",
    "neigh_df = extract_neighborhoods(\n",
    "    anchor_df=anchor_df,\n",
    "    gff_df=gff_df,\n",
    "    window=WINDOW\n",
    ")\n",
    "\n",
    "logging.info(\"Extracted %d neighborhood gene rows\", len(neigh_df))\n",
    "\n",
    "logging.info(\"=\" * 70)\n",
    "logging.info(\"SYNTENY PIPELINE COMPLETED SUCCESSFULLY\")\n",
    "logging.info(\"=\" * 70)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# SANITY CHECKS\n",
    "# --------------------------------------------------------\n",
    "logging.info(\"\\n\" + \"=\" * 70)\n",
    "logging.info(\"PIPELINE OUTPUT SUMMARY\")\n",
    "logging.info(\"=\" * 70)\n",
    "\n",
    "logging.info(\"\\nAnchor gene architecture distribution:\")\n",
    "logging.info(anchor_df[\"architecture\"].value_counts().to_string())\n",
    "\n",
    "logging.info(\"\\nFirst few neighborhood entries:\")\n",
    "logging.info(neigh_df.head().to_string())\n",
    "\n",
    "logging.info(\"\\nGenome distribution:\")\n",
    "logging.info(neigh_df[\"genome_id\"].value_counts().to_string())\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# SAVE INTERMEDIATE RESULTS\n",
    "# --------------------------------------------------------\n",
    "logging.info(\"\\nSaving neighborhood data...\")\n",
    "STEP += 1\n",
    "\n",
    "escrt_neighborhoods_file = os.path.join(\n",
    "    MAIN_OUTDIR,\n",
    "    f\"[STEP:{STEP}]escrt_neighborhoods_{rightnow}.csv\"\n",
    ")\n",
    "neigh_df.to_csv(escrt_neighborhoods_file, index=False)\n",
    "logging.info(\"Saved: %s\", escrt_neighborhoods_file)\n",
    "\n",
    "\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# STEP 4: Merge with taxonomy\n",
    "# --------------------------------------------------------\n",
    "logging.info(\"\\n\" + \"=\" * 70)\n",
    "logging.info(\"ADDING TAXONOMY INFORMATION\")\n",
    "logging.info(\"=\" * 70)\n",
    "\n",
    "STEP += 1\n",
    "out_csv = os.path.join(\n",
    "    MAIN_OUTDIR,\n",
    "    f\"[STEP:{STEP}]escrt_neighborhoods_with_taxonomy_{rightnow}.csv\"\n",
    ")\n",
    "\n",
    "df = merge_taxonomy(\n",
    "    escrt_neighborhoods_file,\n",
    "    \"/home/anirudh/synteny/ar53_taxonomy_r226.tsv\",\n",
    "    out_csv\n",
    ")\n",
    "\n",
    "logging.info(\"Saved taxonomy-annotated neighborhoods: %s\", out_csv)\n",
    "\n",
    "# --------------------------------------------------------\n",
    "# TAXONOMIC SUMMARY\n",
    "# --------------------------------------------------------\n",
    "logging.info(\"\\nTaxonomic diversity in results:\")\n",
    "logging.info(\"Unique phyla: %d\", df[\"phylum\"].nunique())\n",
    "logging.info(\"Unique classes: %d\", df[\"class\"].nunique())\n",
    "logging.info(\"Unique orders: %d\", df[\"order\"].nunique())\n",
    "\n",
    "logging.info(\"\\nSample genomes with taxonomy:\")\n",
    "logging.info(\n",
    "    df[\n",
    "        [\"genome_id_base\", \"phylum\", \"class\", \"order\"]\n",
    "    ]\n",
    "    .drop_duplicates()\n",
    "    .head(10)\n",
    "    .to_string()\n",
    ")\n",
    "\n",
    "\n",
    "contigs = (\n",
    "    df\n",
    "    .groupby([\"genome_id_x\", \"contig\"], as_index=False)\n",
    "    .agg(\n",
    "        neighborhood_architecture_compressed=(\n",
    "            \"neighborhood_architecture_compressed\", \"first\"\n",
    "        )\n",
    "    )\n",
    ")\n",
    "\n",
    "logging.info(\"\\nContig architecture summary (first 10):\")\n",
    "logging.info(contigs.head(10).to_string())\n",
    "\n",
    "contigs.to_csv(os.path.join(MAIN_OUTDIR, f\"[STEP:{STEP}.5]contig_architecture_summary_{rightnow}.csv\"), index=False)\n",
    "\n",
    "logging.info(\"\\n\" + \"=\" * 70)\n",
    "logging.info(\"ALL PROCESSING COMPLETE\")\n",
    "logging.info(\"=\" * 70)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 153,
   "id": "bc91f487",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2026-01-16 20:12:04,913 [INFO] \n",
      "======================================================================\n",
      "2026-01-16 20:12:04,914 [INFO] ARCHITECTURE FILTERING DIAGNOSTIC\n",
      "2026-01-16 20:12:04,914 [INFO] ======================================================================\n",
      "2026-01-16 20:12:04,915 [INFO] Total proteins with valid HMM hits (i_evalue  1e-5, coverage  0.65): 3325\n",
      "2026-01-16 20:12:04,915 [INFO] Proteins with architectures matching keywords: 1470\n",
      "2026-01-16 20:12:04,915 [INFO] Proteins with architectures NOT matching keywords: 1855\n",
      "2026-01-16 20:12:04,916 [WARNING] \n",
      "  WARNING: 1855 proteins will be EXCLUDED from neighborhoods:\n",
      "2026-01-16 20:12:04,919 [WARNING]             target architecture\n",
      "0   AAEOKEPF_00166           E2\n",
      "3   AAEOKEPF_00273       E3-UFM\n",
      "6   AAEOKEPF_00322           E1\n",
      "8   AAEOKEPF_00337           E1\n",
      "10  AAEOKEPF_00338          MPN\n",
      "12  AAEOKEPF_00690          MPN\n",
      "13  AAEOKEPF_00878          MPN\n",
      "15  AAEOKEPF_00882      Ub-like\n",
      "19  AAEOKEPF_01109          MPN\n",
      "20  AAEOKEPF_01385          MPN\n",
      "29  AAEOKEPF_01448          MPN\n",
      "38  AAEOKEPF_02039          MPN\n",
      "39  AAEOKEPF_02040           E1\n",
      "42  AAEOKEPF_02593           E1\n",
      "49  AAEOKEPF_02795           E1\n",
      "52  AAEOKEPF_02806      E3-HEAT\n",
      "58  AEGAMCKF_00140          MPN\n",
      "62  AEGAMCKF_00191          MPN\n",
      "67  AEGAMCKF_00193           E2\n",
      "71  AEGAMCKF_00268          MPN\n",
      "2026-01-16 20:12:04,919 [WARNING] \n",
      "Distribution of excluded architectures:\n",
      "2026-01-16 20:12:04,920 [WARNING] architecture\n",
      "MPN           662\n",
      "E1            415\n",
      "Ub-like       282\n",
      "E2            212\n",
      "BRO1           70\n",
      "E3-HEAT        47\n",
      "E3-UFM         42\n",
      "E2-E3          37\n",
      "URM1           36\n",
      "E3             20\n",
      "E3-dom         18\n",
      "Zn-PH          10\n",
      "Ub-dom          2\n",
      "MPN-E3-dom      1\n",
      "E3-PCI          1\n",
      "2026-01-16 20:12:04,924 [WARNING] \n",
      "Saved excluded proteins to: /home/anirudh/synteny/hmms/ESCRT_synteny_pipeline_m3v2_family_2026-01-16-20-11-34/[DIAGNOSTIC]excluded_proteins_by_keyword_filter_2026-01-16-20-11-34.csv\n",
      "2026-01-16 20:12:04,924 [INFO] ======================================================================\n"
     ]
    }
   ],
   "source": [
    "# ------------------------------------------------------------------\n",
    "# DIAGNOSTIC: Check which architectures would be filtered by keywords\n",
    "# ------------------------------------------------------------------\n",
    "\n",
    "keyword_pattern = \"|\".join(CORE_TARGETS)\n",
    "\n",
    "# Create a boolean mask for architectures matching keywords\n",
    "matches_keywords = (\n",
    "    df_with_arch[\"architecture\"]\n",
    "    .astype(str)\n",
    "    .str.lower()\n",
    "    .str.contains(keyword_pattern)\n",
    ")\n",
    "\n",
    "# Count what would be kept vs discarded\n",
    "kept_count = matches_keywords.sum()\n",
    "discarded_count = (~matches_keywords).sum()\n",
    "\n",
    "logging.info(\"\\n\" + \"=\" * 70)\n",
    "logging.info(\"ARCHITECTURE FILTERING DIAGNOSTIC\")\n",
    "logging.info(\"=\" * 70)\n",
    "logging.info(f\"Total proteins with valid HMM hits (i_evalue  1e-5, coverage  0.65): {len(df_with_arch)}\")\n",
    "logging.info(f\"Proteins with architectures matching keywords: {kept_count}\")\n",
    "logging.info(f\"Proteins with architectures NOT matching keywords: {discarded_count}\")\n",
    "\n",
    "if discarded_count > 0:\n",
    "    logging.warning(f\"\\n  WARNING: {discarded_count} proteins will be EXCLUDED from neighborhoods:\")\n",
    "    excluded = df_with_arch[~matches_keywords][[\"target\", \"architecture\"]].drop_duplicates()\n",
    "    logging.warning(excluded.head(20).to_string())\n",
    "    \n",
    "    # Count distribution of excluded architectures\n",
    "    logging.warning(\"\\nDistribution of excluded architectures:\")\n",
    "    arch_counts = df_with_arch[~matches_keywords][\"architecture\"].value_counts()\n",
    "    logging.warning(arch_counts.head(20).to_string())\n",
    "    \n",
    "    # Save for review\n",
    "    excluded_file = os.path.join(MAIN_OUTDIR, f\"[DIAGNOSTIC]excluded_proteins_by_keyword_filter_{rightnow}.csv\")\n",
    "    df_with_arch[~matches_keywords][[\"target\", \"architecture\", \"architecture_method\"]].drop_duplicates().sort_values(by = ['architecture'], ascending=False).to_csv(excluded_file, index=False)\n",
    "    logging.warning(f\"\\nSaved excluded proteins to: {excluded_file}\")\n",
    "else:\n",
    "    logging.info(\" All proteins with valid hits match keyword filter - no data loss\")\n",
    "\n",
    "logging.info(\"=\" * 70)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Reg",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
